2025-02-24 10:50:49 ceph-node-1 mds[5552]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:13 ceph-node-3 client[2867]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:37 ceph-node-2 mgr[6899]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:50:59 ceph-node-1 radosgw[1630]: CRITICAL: Authentication failure in RADOS Gateway
2025-02-24 10:51:00 ceph-node-2 radosgw[8269]: NOTICE: MGR module loaded successfully
2025-02-24 10:51:30 ceph-node-1 osd[8747]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:18 ceph-node-4 client[4287]: INFO: MGR module loaded successfully
2025-02-24 10:50:34 ceph-node-5 mgr[2910]: INFO: OSD disk space utilization: 70%
2025-02-24 10:51:30 ceph-node-5 osd[7792]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:23 ceph-node-1 radosgw[9875]: NOTICE: Monitor map has been updated
2025-02-24 10:50:59 ceph-node-5 client[2516]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:51:09 ceph-node-2 mds[8825]: NOTICE: OSD rebalancing completed
2025-02-24 10:50:53 ceph-node-2 radosgw[9831]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:51:12 ceph-node-2 client[4078]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:51:11 ceph-node-4 mds[3666]: INFO: Data replication completed for object pool
2025-02-24 10:51:02 ceph-node-5 mds[6638]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:27 ceph-node-1 radosgw[7245]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:50:49 ceph-node-1 mds[6076]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:50:32 ceph-node-1 radosgw[4881]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:21 ceph-node-4 mon[7852]: WARNING: OSD 3.1 marked down due to heartbeat failure
2025-02-24 10:51:00 ceph-node-1 mgr[4175]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:50:47 ceph-node-2 osd[8943]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:50:55 ceph-node-1 mgr[2509]: INFO: OSD rebalancing completed
2025-02-24 10:51:26 ceph-node-1 client[2167]: CRITICAL: MDS failure detected: metadata inconsistency
2025-02-24 10:50:34 ceph-node-2 mgr[8974]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:51:25 ceph-node-3 mgr[2040]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:50:56 ceph-node-4 osd[3473]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:23 ceph-node-4 mds[2484]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:22 ceph-node-1 mds[6248]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:46 ceph-node-3 mon[3685]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:50:51 ceph-node-1 client[7464]: CRITICAL: Disk failure reported on ceph-node-4
2025-02-24 10:50:37 ceph-node-2 mon[1348]: NOTICE: Data replication completed for object pool
2025-02-24 10:51:01 ceph-node-5 radosgw[3266]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:57 ceph-node-1 mon[7300]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:21 ceph-node-1 osd[9356]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:50:40 ceph-node-2 mon[1853]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:24 ceph-node-1 mon[8934]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:50:33 ceph-node-1 radosgw[6698]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:50:59 ceph-node-1 radosgw[9557]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:17 ceph-node-4 radosgw[3404]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:30 ceph-node-5 radosgw[9943]: DEBUG: MGR module loaded successfully
2025-02-24 10:50:51 ceph-node-2 client[5567]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:03 ceph-node-3 mds[8693]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:51:26 ceph-node-1 mds[1549]: DEBUG: MGR module loaded successfully
2025-02-24 10:50:42 ceph-node-4 mgr[7225]: NOTICE: Data replication completed for object pool
2025-02-24 10:50:47 ceph-node-5 mgr[5914]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:27 ceph-node-3 client[4740]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:50:47 ceph-node-4 mon[1795]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:50:42 ceph-node-5 mds[4603]: INFO: OSD disk space utilization: 70%
2025-02-24 10:50:53 ceph-node-1 mon[3368]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:33 ceph-node-5 client[3591]: INFO: OSD disk space utilization: 70%
2025-02-24 10:50:35 ceph-node-3 mon[2021]: INFO: OSD rebalancing completed
2025-02-24 10:51:29 ceph-node-3 mon[2280]: ERROR: Monitor mon1 failed to reach quorum
2025-02-24 10:51:12 ceph-node-3 mgr[9413]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:51:16 ceph-node-2 mds[4601]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:32 ceph-node-2 radosgw[2144]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:31 ceph-node-2 osd[8287]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:50:55 ceph-node-3 mgr[9945]: DEBUG: Monitor map has been updated
2025-02-24 10:50:44 ceph-node-3 mon[9515]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:28 ceph-node-1 osd[9014]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:59 ceph-node-3 mon[4912]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:35 ceph-node-2 osd[5122]: ERROR: Client connection timeout detected
2025-02-24 10:50:35 ceph-node-4 client[5970]: CRITICAL: Slow request detected on OSD 4.3
2025-02-24 10:51:11 ceph-node-1 mgr[5275]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:31 ceph-node-3 mds[1216]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:51:31 ceph-node-1 mgr[8493]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:50:32 ceph-node-5 mds[5454]: NOTICE: Monitor map has been updated
2025-02-24 10:50:47 ceph-node-1 osd[2909]: INFO: MGR module loaded successfully
2025-02-24 10:50:55 ceph-node-3 mon[9802]: INFO: MGR module loaded successfully
2025-02-24 10:51:29 ceph-node-3 radosgw[4924]: INFO: PG 1.2 replicated successfully
2025-02-24 10:50:32 ceph-node-5 client[6243]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:01 ceph-node-4 mon[1274]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:50:34 ceph-node-3 mgr[9952]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:54 ceph-node-1 mds[8470]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:51:25 ceph-node-2 mds[3276]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:43 ceph-node-1 client[3953]: DEBUG: OSD rebalancing completed
2025-02-24 10:51:28 ceph-node-3 client[6188]: NOTICE: OSD rebalancing completed
2025-02-24 10:51:07 ceph-node-2 client[9689]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:00 ceph-node-2 radosgw[8170]: INFO: Data replication completed for object pool
2025-02-24 10:51:18 ceph-node-5 client[5171]: NOTICE: Monitor map has been updated
2025-02-24 10:51:29 ceph-node-2 radosgw[1516]: WARNING: Cluster status: HEALTH_WARN
2025-02-24 10:51:13 ceph-node-5 mds[4605]: ERROR: CRITICAL: Data loss detected on pool 1
2025-02-24 10:51:12 ceph-node-1 client[7444]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:29 ceph-node-5 osd[7961]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:51:31 ceph-node-2 mgr[1634]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:27 ceph-node-1 radosgw[2828]: DEBUG: Monitor map has been updated
2025-02-24 10:50:34 ceph-node-3 mon[8453]: NOTICE: Data replication completed for object pool
2025-02-24 10:50:47 ceph-node-1 radosgw[7255]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:51:12 ceph-node-5 mgr[3708]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:50 ceph-node-1 client[2665]: INFO: MGR module loaded successfully
2025-02-24 10:51:06 ceph-node-4 mds[1276]: DEBUG: OSD rebalancing completed
2025-02-24 10:51:27 ceph-node-2 mds[3369]: INFO: OSD disk space utilization: 70%
2025-02-24 10:51:31 ceph-node-2 radosgw[6993]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:28 ceph-node-2 client[1802]: NOTICE: OSD rebalancing completed
2025-02-24 10:50:41 ceph-node-1 osd[2374]: CRITICAL: MDS failure detected: metadata inconsistency
2025-02-24 10:51:02 ceph-node-1 mds[6815]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:51:17 ceph-node-5 mgr[5884]: NOTICE: MGR module loaded successfully
2025-02-24 10:51:31 ceph-node-4 mon[8812]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:51:06 ceph-node-3 mds[3893]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:51:10 ceph-node-2 osd[7684]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:50:42 ceph-node-3 mgr[7864]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:50:43 ceph-node-1 mon[6338]: CRITICAL: CRITICAL: Data loss detected on pool 1
2025-02-24 10:50:37 ceph-node-4 mon[7235]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:59 ceph-node-5 osd[7183]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:05 ceph-node-2 client[2513]: INFO: Monitor map has been updated
2025-02-24 10:51:04 ceph-node-3 mon[8355]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:50:57 ceph-node-1 client[8174]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:31 ceph-node-4 radosgw[8404]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:28 ceph-node-5 radosgw[1083]: INFO: Data replication completed for object pool
2025-02-24 10:50:49 ceph-node-4 osd[7563]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:50:33 ceph-node-5 mds[8953]: WARNING: MDS failure detected: metadata inconsistency
2025-02-24 10:50:32 ceph-node-5 mgr[5118]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:50:37 ceph-node-4 radosgw[9309]: CRITICAL: Uncorrectable ECC error detected in MDS cache
2025-02-24 10:51:03 ceph-node-2 client[1351]: INFO: MGR module loaded successfully
2025-02-24 10:50:41 ceph-node-5 mds[5370]: NOTICE: Monitor map has been updated
2025-02-24 10:50:59 ceph-node-5 radosgw[8335]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:51:06 ceph-node-5 osd[9688]: CRITICAL: OSD disk space utilization exceeded 90%
2025-02-24 10:51:00 ceph-node-2 mgr[3700]: DEBUG: OSD rebalancing completed
2025-02-24 10:51:15 ceph-node-1 radosgw[5853]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:51:23 ceph-node-5 mds[8402]: INFO: PG 1.2 replicated successfully
2025-02-24 10:50:31 ceph-node-1 mgr[6046]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:51:04 ceph-node-5 mds[4290]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:51:27 ceph-node-5 client[8612]: INFO: MGR module loaded successfully
2025-02-24 10:51:05 ceph-node-1 client[1494]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:50:38 ceph-node-5 osd[6864]: NOTICE: Data replication completed for object pool
2025-02-24 10:50:51 ceph-node-4 mds[5823]: NOTICE: OSD rebalancing completed
2025-02-24 10:50:59 ceph-node-4 mgr[8981]: ERROR: Cluster status: HEALTH_WARN
2025-02-24 10:51:25 ceph-node-2 mds[9606]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:50:42 ceph-node-4 mon[5886]: ERROR: OSD 3.1 marked down due to heartbeat failure
2025-02-24 10:51:14 ceph-node-3 mon[1643]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:50:51 ceph-node-2 mds[5761]: CRITICAL: CRITICAL: PG 2.1 stuck in inconsistent state
2025-02-24 10:50:33 ceph-node-4 mon[2995]: CRITICAL: Slow request detected on OSD 4.3
2025-02-24 10:51:01 ceph-node-1 mon[3310]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:50:39 ceph-node-2 mgr[7859]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:06 ceph-node-2 radosgw[9084]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:58 ceph-node-3 mds[9147]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:19 ceph-node-2 mds[9821]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:50:51 ceph-node-3 mon[7915]: NOTICE: OSD rebalancing completed
2025-02-24 10:51:14 ceph-node-2 radosgw[9866]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:50:58 ceph-node-4 osd[5174]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:51:22 ceph-node-2 radosgw[4131]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:50:52 ceph-node-2 mgr[3696]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:51:15 ceph-node-5 mgr[8731]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:50:32 ceph-node-3 client[3876]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:50:55 ceph-node-2 mon[3144]: NOTICE: MGR module loaded successfully
2025-02-24 10:50:31 ceph-node-1 radosgw[5755]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:50:48 ceph-node-3 osd[6933]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:15 ceph-node-1 radosgw[3103]: INFO: PG 1.2 replicated successfully
2025-02-24 10:50:53 ceph-node-5 mgr[6349]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:50:54 ceph-node-3 radosgw[2375]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:03 ceph-node-3 radosgw[1178]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:01 ceph-node-3 mds[1519]: NOTICE: OSD rebalancing completed
2025-02-24 10:51:00 ceph-node-4 client[1280]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:45 ceph-node-2 mgr[6905]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:50:54 ceph-node-2 mon[2707]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:51:07 ceph-node-4 mgr[1547]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:18 ceph-node-3 osd[2655]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:50:56 ceph-node-5 client[7551]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:50:59 ceph-node-3 client[8349]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:27 ceph-node-3 mgr[1712]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:51:05 ceph-node-2 mon[9310]: INFO: Monitor map has been updated
2025-02-24 10:50:35 ceph-node-1 client[7534]: DEBUG: Data replication completed for object pool
2025-02-24 10:50:34 ceph-node-5 mds[3665]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:51:22 ceph-node-3 client[7260]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:51:24 ceph-node-1 osd[7451]: NOTICE: Data replication completed for object pool
2025-02-24 10:51:01 ceph-node-2 mds[1049]: ERROR: MDS failure detected: metadata inconsistency
2025-02-24 10:51:05 ceph-node-3 mon[7131]: INFO: PG 1.2 replicated successfully
2025-02-24 10:50:51 ceph-node-3 mon[6719]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:16 ceph-node-5 mon[3670]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:31 ceph-node-4 mgr[1293]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:51:05 ceph-node-1 osd[5559]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:50:40 ceph-node-2 osd[6841]: INFO: OSD disk space utilization: 70%
2025-02-24 10:50:58 ceph-node-1 mgr[4496]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:04 ceph-node-2 mon[1082]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:51:14 ceph-node-4 client[9547]: WARNING: MDS failure detected: metadata inconsistency
2025-02-24 10:50:36 ceph-node-4 osd[9433]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:18 ceph-node-1 osd[7768]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:41 ceph-node-5 mds[9471]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:51:20 ceph-node-4 client[3174]: ERROR: Authentication failure in RADOS Gateway
2025-02-24 10:51:31 ceph-node-5 radosgw[5091]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:29 ceph-node-2 mgr[8203]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:51:17 ceph-node-4 mds[1994]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:50:45 ceph-node-2 mon[3718]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:52 ceph-node-2 mon[7532]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:50:45 ceph-node-4 mon[3909]: ERROR: Uncorrectable ECC error detected in MDS cache
2025-02-24 10:50:50 ceph-node-1 mgr[8517]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:12 ceph-node-3 client[2188]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:57 ceph-node-2 osd[4223]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:13 ceph-node-1 mds[5999]: CRITICAL: High I/O latency detected in radosgw service
2025-02-24 10:51:04 ceph-node-5 radosgw[2073]: INFO: Monitor map has been updated
2025-02-24 10:51:07 ceph-node-4 radosgw[6574]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:39 ceph-node-2 client[5152]: ERROR: Disk failure reported on ceph-node-4
2025-02-24 10:50:38 ceph-node-2 mds[6658]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:09 ceph-node-1 mds[5619]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:50:45 ceph-node-5 mon[5416]: INFO: OSD rebalancing completed
2025-02-24 10:51:25 ceph-node-2 osd[8078]: NOTICE: OSD rebalancing completed
2025-02-24 10:50:55 ceph-node-3 osd[4257]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:51:05 ceph-node-1 osd[1618]: NOTICE: Data replication completed for object pool
2025-02-24 10:50:47 ceph-node-1 osd[3215]: ERROR: Disk failure reported on ceph-node-4
2025-02-24 10:51:02 ceph-node-1 client[6364]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:50:55 ceph-node-1 mds[7772]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:34 ceph-node-4 mon[6743]: NOTICE: MGR module loaded successfully
2025-02-24 10:50:35 ceph-node-5 client[7446]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:50:34 ceph-node-2 mgr[9102]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:40 ceph-node-5 osd[6743]: CRITICAL: CRITICAL: PG 2.1 stuck in inconsistent state
2025-02-24 10:50:54 ceph-node-1 mon[6511]: NOTICE: Monitor map has been updated
2025-02-24 10:51:17 ceph-node-1 mgr[1265]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:17 ceph-node-4 mgr[2710]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:44 ceph-node-2 mds[5598]: INFO: OSD rebalancing completed
2025-02-24 10:51:11 ceph-node-2 radosgw[3067]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:18 ceph-node-4 mgr[5806]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:51:02 ceph-node-4 mds[2309]: DEBUG: Data replication completed for object pool
2025-02-24 10:50:48 ceph-node-1 mon[4138]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:31 ceph-node-5 mgr[9634]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:51:26 ceph-node-4 mon[1964]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:51:15 ceph-node-5 mon[4729]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:17 ceph-node-4 mgr[3672]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:12 ceph-node-4 radosgw[9664]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:50:45 ceph-node-2 client[8460]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:50:33 ceph-node-3 mgr[8609]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:04 ceph-node-1 mgr[4393]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:44 ceph-node-3 mds[2411]: INFO: Monitor map has been updated
2025-02-24 10:51:20 ceph-node-4 mds[4830]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:06 ceph-node-4 mds[9159]: INFO: OSD rebalancing completed
2025-02-24 10:51:19 ceph-node-3 radosgw[9418]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:07 ceph-node-5 radosgw[4680]: ERROR: Client connection timeout detected
2025-02-24 10:50:47 ceph-node-5 client[8003]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:10 ceph-node-3 osd[9008]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:50:38 ceph-node-2 mon[3497]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:50:36 ceph-node-4 mgr[6720]: WARNING: Slow request detected on OSD 4.3
2025-02-24 10:50:42 ceph-node-3 mon[6001]: DEBUG: OSD rebalancing completed
2025-02-24 10:51:01 ceph-node-3 mds[8088]: INFO: PG 1.2 replicated successfully
2025-02-24 10:50:40 ceph-node-1 radosgw[3677]: INFO: PG 1.2 replicated successfully
2025-02-24 10:51:09 ceph-node-3 mgr[4154]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:51:26 ceph-node-3 mgr[5095]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:08 ceph-node-2 mgr[6570]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:18 ceph-node-3 mon[6675]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:50:32 ceph-node-4 mon[3754]: NOTICE: Data replication completed for object pool
2025-02-24 10:51:10 ceph-node-2 mds[9542]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:50:58 ceph-node-4 mds[9556]: NOTICE: MGR module loaded successfully
2025-02-24 10:50:41 ceph-node-5 radosgw[5900]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:51:13 ceph-node-4 radosgw[3829]: NOTICE: Monitor map has been updated
2025-02-24 10:51:11 ceph-node-3 mon[3994]: DEBUG: Data replication completed for object pool
2025-02-24 10:50:53 ceph-node-2 client[9851]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:15 ceph-node-5 mgr[2332]: INFO: OSD disk space utilization: 70%
2025-02-24 10:50:46 ceph-node-5 mds[9156]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:49 ceph-node-2 osd[4096]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:19 ceph-node-4 mgr[6922]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:51:25 ceph-node-4 osd[2716]: NOTICE: MGR module loaded successfully
2025-02-24 10:50:38 ceph-node-1 client[8014]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:06 ceph-node-3 mgr[6477]: INFO: PG 1.2 replicated successfully
2025-02-24 10:51:17 ceph-node-4 mon[1102]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:47 ceph-node-5 osd[4351]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:20 ceph-node-2 mgr[3505]: DEBUG: Data replication completed for object pool
2025-02-24 10:50:52 ceph-node-2 mgr[3548]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:51:06 ceph-node-4 client[9903]: CRITICAL: Cluster status: HEALTH_WARN
2025-02-24 10:51:12 ceph-node-3 radosgw[9644]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:51:18 ceph-node-5 mgr[1858]: INFO: Monitor map has been updated
2025-02-24 10:50:53 ceph-node-3 mds[5288]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:13 ceph-node-2 mgr[7293]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:50:47 ceph-node-2 mgr[1832]: DEBUG: Monitor map has been updated
2025-02-24 10:50:46 ceph-node-4 mon[4901]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:33 ceph-node-2 osd[8592]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:23 ceph-node-4 mgr[9760]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:08 ceph-node-5 mds[3896]: NOTICE: Monitor map has been updated
2025-02-24 10:50:51 ceph-node-4 mon[6985]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:50:36 ceph-node-2 mds[5186]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:51:22 ceph-node-4 mon[8820]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:50:39 ceph-node-5 mon[4926]: INFO: PG 1.2 replicated successfully
2025-02-24 10:50:44 ceph-node-5 osd[8865]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:50:54 ceph-node-2 radosgw[6096]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:50:40 ceph-node-5 mds[1407]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:51:11 ceph-node-2 osd[9361]: DEBUG: MGR module loaded successfully
2025-02-24 10:50:41 ceph-node-2 radosgw[2858]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:50:45 ceph-node-3 mds[2614]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:50:52 ceph-node-5 osd[5108]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:27 ceph-node-2 osd[6153]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:50:43 ceph-node-1 mgr[4261]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:54 ceph-node-1 mds[7426]: NOTICE: Data replication completed for object pool
2025-02-24 10:51:19 ceph-node-3 mon[5418]: ERROR: Client connection timeout detected
2025-02-24 10:51:21 ceph-node-1 mds[3640]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:50 ceph-node-1 client[1189]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:50:50 ceph-node-3 radosgw[3084]: ERROR: OSD disk space utilization exceeded 90%
2025-02-24 10:51:19 ceph-node-4 client[6290]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:26 ceph-node-3 osd[7006]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:03 ceph-node-2 radosgw[7937]: CRITICAL: CRITICAL: Data loss detected on pool 1
2025-02-24 10:51:02 ceph-node-4 client[9859]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:50 ceph-node-1 mon[5492]: ERROR: Cluster status: HEALTH_WARN
2025-02-24 10:50:37 ceph-node-4 radosgw[8218]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:38 ceph-node-3 mds[9493]: CRITICAL: Disk failure reported on ceph-node-4
2025-02-24 10:50:57 ceph-node-5 mon[5066]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:51:09 ceph-node-2 radosgw[5396]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:50:47 ceph-node-5 mon[2234]: DEBUG: OSD rebalancing completed
2025-02-24 10:50:33 ceph-node-2 client[7423]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:51:27 ceph-node-4 mon[3619]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:13 ceph-node-4 mgr[8042]: NOTICE: Data replication completed for object pool
2025-02-24 10:51:15 ceph-node-1 radosgw[6768]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:50:46 ceph-node-2 client[2868]: NOTICE: OSD rebalancing completed
2025-02-24 10:51:24 ceph-node-4 radosgw[6622]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:08 ceph-node-2 mon[5106]: INFO: Data replication completed for object pool
2025-02-24 10:50:49 ceph-node-1 mon[3845]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:50:49 ceph-node-5 mds[1831]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:13 ceph-node-4 mds[1042]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:29 ceph-node-4 mon[9200]: DEBUG: Data replication completed for object pool
2025-02-24 10:50:39 ceph-node-5 mgr[8400]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:51:03 ceph-node-3 mgr[8923]: INFO: Monitor map has been updated
2025-02-24 10:51:26 ceph-node-5 mgr[4020]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:50:33 ceph-node-3 mgr[3694]: DEBUG: OSD rebalancing completed
2025-02-24 10:50:51 ceph-node-2 osd[3678]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:51:02 ceph-node-1 client[8978]: CRITICAL: High I/O latency detected in radosgw service
2025-02-24 10:50:46 ceph-node-2 radosgw[4749]: INFO: Data replication completed for object pool
2025-02-24 10:50:59 ceph-node-4 mon[8165]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:40 ceph-node-2 mon[9058]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:11 ceph-node-3 osd[5549]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:51:25 ceph-node-4 mon[6725]: INFO: OSD disk space utilization: 70%
2025-02-24 10:50:53 ceph-node-4 osd[9330]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:50:47 ceph-node-4 client[4784]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:50:51 ceph-node-5 radosgw[8899]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:54 ceph-node-4 mds[6138]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:39 ceph-node-3 radosgw[4296]: INFO: Monitor map has been updated
2025-02-24 10:51:26 ceph-node-4 osd[6432]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:40 ceph-node-3 mon[5448]: NOTICE: OSD rebalancing completed
2025-02-24 10:50:52 ceph-node-5 osd[6564]: CRITICAL: Uncorrectable ECC error detected in MDS cache
2025-02-24 10:50:37 ceph-node-5 osd[2150]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:51:26 ceph-node-2 radosgw[9773]: INFO: Monitor map has been updated
2025-02-24 10:51:00 ceph-node-4 mgr[4189]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:50:46 ceph-node-4 mds[2920]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:26 ceph-node-1 mgr[6974]: NOTICE: Data replication completed for object pool
2025-02-24 10:50:49 ceph-node-4 osd[9781]: INFO: MGR module loaded successfully
2025-02-24 10:51:23 ceph-node-5 radosgw[9771]: NOTICE: Monitor map has been updated
2025-02-24 10:51:28 ceph-node-1 client[4166]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:31 ceph-node-5 mon[8651]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:03 ceph-node-5 mgr[7427]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:10 ceph-node-5 client[8619]: INFO: PG 1.2 replicated successfully
2025-02-24 10:50:59 ceph-node-3 client[2625]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:51:30 ceph-node-4 mon[8326]: INFO: Data replication completed for object pool
2025-02-24 10:51:12 ceph-node-2 mgr[3692]: CRITICAL: MDS failure detected: metadata inconsistency
2025-02-24 10:50:33 ceph-node-2 mds[8248]: WARNING: OSD 3.1 marked down due to heartbeat failure
2025-02-24 10:50:35 ceph-node-1 client[5825]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:51:14 ceph-node-5 mgr[7601]: ERROR: High I/O latency detected in radosgw service
2025-02-24 10:50:37 ceph-node-5 client[3832]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:01 ceph-node-5 mgr[1417]: DEBUG: OSD rebalancing completed
2025-02-24 10:51:21 ceph-node-2 radosgw[4589]: DEBUG: Data replication completed for object pool
2025-02-24 10:50:50 ceph-node-1 radosgw[4601]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:20 ceph-node-1 osd[9532]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:07 ceph-node-2 radosgw[9566]: NOTICE: Monitor map has been updated
2025-02-24 10:50:44 ceph-node-2 mds[7889]: ERROR: Uncorrectable ECC error detected in MDS cache
2025-02-24 10:50:52 ceph-node-4 mgr[7996]: INFO: PG 1.2 replicated successfully
2025-02-24 10:51:19 ceph-node-2 mds[7043]: CRITICAL: Network partition detected between OSD nodes
2025-02-24 10:50:43 ceph-node-2 mgr[7086]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:51:25 ceph-node-5 client[7927]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:50:59 ceph-node-3 radosgw[6898]: DEBUG: MGR module loaded successfully
2025-02-24 10:50:39 ceph-node-3 mds[7795]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:22 ceph-node-3 client[3500]: INFO: MGR module loaded successfully
2025-02-24 10:50:40 ceph-node-5 mon[7723]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:58 ceph-node-2 client[3945]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:50:56 ceph-node-1 osd[5390]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:50:54 ceph-node-2 mgr[9125]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:50:39 ceph-node-4 client[7210]: NOTICE: OSD rebalancing completed
2025-02-24 10:50:41 ceph-node-5 osd[2650]: INFO: Data replication completed for object pool
2025-02-24 10:50:36 ceph-node-4 mon[6332]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:50:56 ceph-node-4 radosgw[8339]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:38 ceph-node-4 mon[1023]: DEBUG: Monitor map has been updated
2025-02-24 10:51:28 ceph-node-5 client[7500]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:44 ceph-node-5 mon[8424]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:50:38 ceph-node-5 mds[4902]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:50:54 ceph-node-4 mds[7347]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:50:44 ceph-node-2 client[5295]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:29 ceph-node-2 mon[6920]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:50:54 ceph-node-5 radosgw[3319]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:49 ceph-node-4 mgr[5010]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:51:08 ceph-node-4 client[4264]: INFO: MGR module loaded successfully
2025-02-24 10:50:36 ceph-node-5 osd[6751]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:26 ceph-node-4 client[4175]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:13 ceph-node-1 osd[9193]: INFO: Monitor map has been updated
2025-02-24 10:50:43 ceph-node-4 mds[6906]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:59 ceph-node-3 osd[5480]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:57 ceph-node-1 mds[6898]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:43 ceph-node-3 client[6053]: INFO: OSD disk space utilization: 70%
2025-02-24 10:50:49 ceph-node-4 mds[1016]: INFO: OSD disk space utilization: 70%
2025-02-24 10:50:37 ceph-node-5 radosgw[3577]: DEBUG: Monitor map has been updated
2025-02-24 10:50:56 ceph-node-1 client[5790]: INFO: Monitor map has been updated
2025-02-24 10:50:58 ceph-node-4 client[2742]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:50:39 ceph-node-5 mgr[2683]: INFO: OSD rebalancing completed
2025-02-24 10:50:59 ceph-node-1 mon[5638]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:05 ceph-node-5 mon[7012]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:50:43 ceph-node-3 mon[7412]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:04 ceph-node-2 osd[2248]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:51:08 ceph-node-5 osd[3930]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:23 ceph-node-5 mgr[7090]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:50:54 ceph-node-2 mon[7266]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:30 ceph-node-3 mon[1276]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:51:15 ceph-node-2 radosgw[4898]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:54 ceph-node-4 mon[1074]: DEBUG: Data replication completed for object pool
2025-02-24 10:50:40 ceph-node-3 mds[5988]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:51:14 ceph-node-5 radosgw[9156]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:51:15 ceph-node-5 osd[3163]: DEBUG: Monitor map has been updated
2025-02-24 10:50:44 ceph-node-1 client[2553]: NOTICE: Data replication completed for object pool
2025-02-24 10:51:05 ceph-node-2 mgr[7238]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:19 ceph-node-2 mgr[2778]: NOTICE: OSD rebalancing completed
2025-02-24 10:50:58 ceph-node-4 mds[9804]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:29 ceph-node-1 mon[3209]: NOTICE: Monitor map has been updated
2025-02-24 10:51:05 ceph-node-1 osd[5238]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:24 ceph-node-5 osd[7578]: CRITICAL: Cluster status: HEALTH_WARN
2025-02-24 10:51:15 ceph-node-1 radosgw[5939]: INFO: MGR module loaded successfully
2025-02-24 10:51:04 ceph-node-4 osd[3770]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:51 ceph-node-4 osd[4800]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:56 ceph-node-4 radosgw[6284]: INFO: PG 1.2 replicated successfully
2025-02-24 10:51:22 ceph-node-2 client[7459]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:15 ceph-node-2 client[2625]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:50:46 ceph-node-1 mgr[1187]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:37 ceph-node-1 client[4517]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:51:31 ceph-node-5 client[3620]: INFO: MGR module loaded successfully
2025-02-24 10:50:36 ceph-node-2 mgr[6873]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:50:34 ceph-node-4 mds[3810]: DEBUG: Data replication completed for object pool
2025-02-24 10:50:34 ceph-node-5 mds[3887]: INFO: Data replication completed for object pool
2025-02-24 10:51:21 ceph-node-1 mds[4136]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:29 ceph-node-4 mon[8985]: DEBUG: Monitor map has been updated
2025-02-24 10:50:52 ceph-node-5 mon[3385]: ERROR: OSD 3.1 marked down due to heartbeat failure
2025-02-24 10:50:51 ceph-node-2 mon[3157]: WARNING: Disk failure reported on ceph-node-4
2025-02-24 10:50:37 ceph-node-3 mgr[3419]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:50:41 ceph-node-4 osd[6130]: INFO: OSD rebalancing completed
2025-02-24 10:51:20 ceph-node-4 mgr[1185]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:50:37 ceph-node-3 radosgw[2598]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:17 ceph-node-1 osd[4938]: INFO: MGR module loaded successfully
2025-02-24 10:51:24 ceph-node-5 osd[3508]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:50:52 ceph-node-3 mds[3270]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:51:22 ceph-node-3 mon[8602]: INFO: PG 1.2 replicated successfully
2025-02-24 10:51:09 ceph-node-4 radosgw[6143]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:51:07 ceph-node-5 mon[8886]: CRITICAL: High I/O latency detected in radosgw service
2025-02-24 10:50:43 ceph-node-4 mds[2200]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:50:39 ceph-node-4 mgr[8712]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:24 ceph-node-2 radosgw[5912]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:00 ceph-node-4 client[4554]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:50:36 ceph-node-4 mgr[9505]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:50:47 ceph-node-4 mds[2978]: NOTICE: MGR module loaded successfully
2025-02-24 10:50:36 ceph-node-3 client[8214]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:51:11 ceph-node-3 mds[9207]: INFO: MGR module loaded successfully
2025-02-24 10:50:57 ceph-node-3 radosgw[2074]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:50:56 ceph-node-4 radosgw[2151]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:04 ceph-node-2 mds[5995]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:50:32 ceph-node-4 osd[1350]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:15 ceph-node-1 mgr[1149]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:25 ceph-node-3 mon[4202]: CRITICAL: Monitor mon1 failed to reach quorum
2025-02-24 10:51:20 ceph-node-3 mds[4244]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:03 ceph-node-5 osd[9002]: INFO: Monitor map has been updated
2025-02-24 10:51:21 ceph-node-1 mds[3375]: INFO: MGR module loaded successfully
2025-02-24 10:51:04 ceph-node-2 radosgw[4101]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:50:59 ceph-node-1 mgr[3418]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:50:45 ceph-node-1 mgr[4546]: NOTICE: Monitor map has been updated
2025-02-24 10:51:06 ceph-node-3 client[4513]: DEBUG: MGR module loaded successfully
2025-02-24 10:50:39 ceph-node-1 mgr[2455]: ERROR: CRITICAL: PG 2.1 stuck in inconsistent state
2025-02-24 10:51:18 ceph-node-2 mon[5098]: INFO: OSD rebalancing completed
2025-02-24 10:51:15 ceph-node-5 mds[9304]: NOTICE: MGR module loaded successfully
2025-02-24 10:50:47 ceph-node-2 mds[2885]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:51:30 ceph-node-3 mgr[5129]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:50:40 ceph-node-1 client[3880]: INFO: OSD disk space utilization: 70%
2025-02-24 10:50:31 ceph-node-5 osd[5377]: CRITICAL: Slow request detected on OSD 4.3
2025-02-24 10:51:01 ceph-node-5 radosgw[9595]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:50:46 ceph-node-5 mon[7064]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:26 ceph-node-4 osd[8955]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:51:14 ceph-node-1 osd[4026]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:47 ceph-node-5 osd[5436]: ERROR: CRITICAL: Data loss detected on pool 1
2025-02-24 10:50:33 ceph-node-1 radosgw[3881]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:50:56 ceph-node-5 mon[8415]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:11 ceph-node-2 mon[4210]: WARNING: Journal write failure detected on OSD node
2025-02-24 10:51:27 ceph-node-1 radosgw[6811]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:51:28 ceph-node-2 radosgw[3845]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:08 ceph-node-5 mon[6697]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:50:57 ceph-node-2 osd[8902]: DEBUG: Monitor map has been updated
2025-02-24 10:50:32 ceph-node-4 radosgw[5912]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:17 ceph-node-2 osd[7630]: INFO: Data replication completed for object pool
2025-02-24 10:51:08 ceph-node-5 mds[4152]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:22 ceph-node-2 radosgw[1128]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:12 ceph-node-4 mds[7191]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:21 ceph-node-2 mgr[8985]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:50:52 ceph-node-3 mds[6485]: ERROR: Disk failure reported on ceph-node-4
2025-02-24 10:51:22 ceph-node-2 mgr[4706]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:58 ceph-node-1 osd[3234]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:13 ceph-node-3 client[2109]: CRITICAL: Monitor down, unable to connect to quorum
2025-02-24 10:51:03 ceph-node-4 osd[8523]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:50:52 ceph-node-5 osd[9283]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:50:32 ceph-node-5 radosgw[3065]: NOTICE: Data replication completed for object pool
2025-02-24 10:51:13 ceph-node-5 mon[1677]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:35 ceph-node-4 osd[3051]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:25 ceph-node-2 mgr[4881]: INFO: Monitor map has been updated
2025-02-24 10:50:53 ceph-node-4 osd[3555]: INFO: PG 1.2 replicated successfully
2025-02-24 10:50:43 ceph-node-3 radosgw[3108]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:29 ceph-node-1 mgr[8902]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:50:31 ceph-node-3 osd[5057]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:50:33 ceph-node-4 client[9396]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:13 ceph-node-4 client[7092]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:08 ceph-node-1 osd[3706]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:08 ceph-node-2 mds[5810]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:51:09 ceph-node-2 mon[2860]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:50:46 ceph-node-3 mgr[6289]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:50:34 ceph-node-3 osd[4687]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:51:24 ceph-node-5 mon[2192]: NOTICE: OSD rebalancing completed
2025-02-24 10:51:05 ceph-node-1 osd[7714]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:19 ceph-node-3 mgr[2907]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:27 ceph-node-5 mon[3433]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:50:37 ceph-node-2 client[3522]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:45 ceph-node-3 mds[4778]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:50:48 ceph-node-2 osd[8220]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:50:39 ceph-node-2 mgr[3152]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:51:31 ceph-node-5 osd[4351]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:50:48 ceph-node-3 radosgw[2160]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:50 ceph-node-4 radosgw[8059]: NOTICE: Monitor map has been updated
2025-02-24 10:51:07 ceph-node-1 mgr[4677]: ERROR: Network partition detected between OSD nodes
2025-02-24 10:50:41 ceph-node-2 mon[9618]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:50:32 ceph-node-5 mon[8815]: CRITICAL: Monitor mon1 failed to reach quorum
2025-02-24 10:51:14 ceph-node-3 client[3434]: WARNING: Cluster status: HEALTH_WARN
2025-02-24 10:51:17 ceph-node-5 mgr[5442]: CRITICAL: Uncorrectable ECC error detected in MDS cache
2025-02-24 10:51:07 ceph-node-1 radosgw[3289]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:50:57 ceph-node-2 osd[5198]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:29 ceph-node-2 radosgw[1113]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:33 ceph-node-2 mds[4357]: DEBUG: OSD rebalancing completed
2025-02-24 10:50:31 ceph-node-1 mon[2164]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:49 ceph-node-5 client[7815]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:27 ceph-node-2 osd[2364]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:50:32 ceph-node-4 mgr[7608]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:01 ceph-node-5 mon[2579]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:50:56 ceph-node-5 mds[7784]: ERROR: CRITICAL: PG 2.1 stuck in inconsistent state
2025-02-24 10:51:13 ceph-node-2 client[8554]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:08 ceph-node-2 radosgw[2814]: INFO: MGR module loaded successfully
2025-02-24 10:51:27 ceph-node-3 osd[3257]: INFO: MGR module loaded successfully
2025-02-24 10:51:13 ceph-node-2 osd[9225]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:50:31 ceph-node-2 mds[6244]: INFO: OSD disk space utilization: 70%
2025-02-24 10:50:52 ceph-node-5 mon[5780]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:31 ceph-node-3 mds[5832]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:40 ceph-node-5 mon[8052]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:50:44 ceph-node-3 mds[7329]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:50:48 ceph-node-5 mgr[1563]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:29 ceph-node-5 osd[3702]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:11 ceph-node-3 osd[9747]: INFO: OSD disk space utilization: 70%
2025-02-24 10:50:51 ceph-node-4 mds[9748]: ERROR: Monitor mon1 failed to reach quorum
2025-02-24 10:51:26 ceph-node-5 radosgw[1311]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:57 ceph-node-5 mds[8802]: INFO: Data replication completed for object pool
2025-02-24 10:51:20 ceph-node-3 mgr[1118]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:26 ceph-node-2 mgr[4238]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:22 ceph-node-2 mon[3061]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:11 ceph-node-4 mgr[5237]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:51:27 ceph-node-5 mds[7880]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:03 ceph-node-2 mgr[3554]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:51:11 ceph-node-5 mds[3432]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:55 ceph-node-4 radosgw[1571]: INFO: Monitor map has been updated
2025-02-24 10:51:17 ceph-node-1 mgr[8358]: DEBUG: OSD rebalancing completed
2025-02-24 10:50:41 ceph-node-4 mon[1000]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:50:41 ceph-node-4 mds[4277]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:51:14 ceph-node-2 osd[5348]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:32 ceph-node-4 radosgw[4502]: ERROR: Monitor down, unable to connect to quorum
2025-02-24 10:51:30 ceph-node-1 radosgw[1516]: WARNING: MDS failure detected: metadata inconsistency
2025-02-24 10:51:22 ceph-node-4 radosgw[4796]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:50:47 ceph-node-2 mgr[9121]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:50:38 ceph-node-1 mon[8381]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:50:51 ceph-node-2 radosgw[5263]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:50:45 ceph-node-2 osd[2329]: ERROR: Cluster status: HEALTH_WARN
2025-02-24 10:50:34 ceph-node-4 osd[6080]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:08 ceph-node-5 osd[9018]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:29 ceph-node-3 client[8583]: CRITICAL: Disk failure reported on ceph-node-4
2025-02-24 10:51:14 ceph-node-5 osd[9498]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:51:07 ceph-node-2 radosgw[5621]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:00 ceph-node-3 mon[8238]: INFO: OSD rebalancing completed
2025-02-24 10:51:11 ceph-node-3 osd[1726]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:50:37 ceph-node-4 mgr[9269]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:50:51 ceph-node-1 client[1950]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:50:57 ceph-node-2 mgr[6370]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:51:10 ceph-node-5 mon[7667]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:15 ceph-node-5 mds[7531]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:15 ceph-node-1 mon[7906]: DEBUG: MGR module loaded successfully
2025-02-24 10:50:40 ceph-node-4 mgr[3666]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:03 ceph-node-3 osd[8391]: INFO: OSD disk space utilization: 70%
2025-02-24 10:51:26 ceph-node-1 mgr[5572]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:21 ceph-node-4 radosgw[5342]: NOTICE: OSD rebalancing completed
2025-02-24 10:51:08 ceph-node-1 mon[1175]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:00 ceph-node-5 client[8840]: DEBUG: Monitor map has been updated
2025-02-24 10:50:45 ceph-node-3 osd[7728]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:29 ceph-node-2 client[5942]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:50:45 ceph-node-4 client[3152]: DEBUG: Data replication completed for object pool
2025-02-24 10:50:44 ceph-node-1 radosgw[8447]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:50:55 ceph-node-4 radosgw[2801]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:51:24 ceph-node-5 radosgw[9038]: INFO: OSD disk space utilization: 70%
2025-02-24 10:51:04 ceph-node-5 mon[7084]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:03 ceph-node-1 client[8643]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:18 ceph-node-5 radosgw[2794]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:51:24 ceph-node-1 client[7035]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:11 ceph-node-4 mds[3464]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:04 ceph-node-3 osd[3096]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:50:46 ceph-node-3 mgr[5899]: ERROR: Network partition detected between OSD nodes
2025-02-24 10:50:44 ceph-node-4 osd[3310]: CRITICAL: Data corruption detected in object pool
2025-02-24 10:51:04 ceph-node-2 mon[1173]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:50:40 ceph-node-4 mgr[4425]: CRITICAL: Authentication failure in RADOS Gateway
2025-02-24 10:50:55 ceph-node-2 radosgw[6109]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:17 ceph-node-1 osd[7423]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:50:40 ceph-node-3 radosgw[4650]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:28 ceph-node-5 radosgw[1576]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:49 ceph-node-3 radosgw[2445]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:08 ceph-node-4 osd[7197]: DEBUG: Data replication completed for object pool
2025-02-24 10:50:34 ceph-node-1 client[6327]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:51:15 ceph-node-3 client[3458]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:42 ceph-node-1 mgr[1631]: ERROR: Authentication failure in RADOS Gateway
2025-02-24 10:51:01 ceph-node-3 mds[7429]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:40 ceph-node-5 client[3533]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:22 ceph-node-4 osd[6757]: NOTICE: OSD rebalancing completed
2025-02-24 10:51:07 ceph-node-2 osd[1604]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:50:50 ceph-node-4 client[4600]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:52 ceph-node-5 radosgw[1311]: INFO: Monitor map has been updated
2025-02-24 10:50:38 ceph-node-1 client[6565]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:09 ceph-node-3 radosgw[1166]: INFO: PG 1.2 replicated successfully
2025-02-24 10:50:44 ceph-node-2 mgr[8936]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:50:56 ceph-node-3 mgr[8623]: INFO: Monitor map has been updated
2025-02-24 10:51:13 ceph-node-2 mon[6425]: INFO: Data replication completed for object pool
2025-02-24 10:50:35 ceph-node-4 osd[8128]: NOTICE: Data replication completed for object pool
2025-02-24 10:50:53 ceph-node-1 mgr[3578]: CRITICAL: Disk failure reported on ceph-node-4
2025-02-24 10:50:47 ceph-node-3 client[1183]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:41 ceph-node-5 mon[8544]: DEBUG: Data replication completed for object pool
2025-02-24 10:50:49 ceph-node-1 client[1052]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:39 ceph-node-5 mon[6851]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:03 ceph-node-1 client[5054]: CRITICAL: Monitor down, unable to connect to quorum
2025-02-24 10:51:22 ceph-node-2 mds[1687]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:51:18 ceph-node-3 mgr[7144]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:39 ceph-node-5 radosgw[3159]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:51:13 ceph-node-3 mon[2944]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:18 ceph-node-1 osd[7987]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:50:34 ceph-node-4 osd[1583]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:23 ceph-node-1 mds[2751]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:51:02 ceph-node-5 mon[7779]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:22 ceph-node-3 mgr[6331]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:50:34 ceph-node-4 radosgw[9509]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:17 ceph-node-1 osd[2065]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:02 ceph-node-1 mon[8265]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:51:14 ceph-node-4 radosgw[7017]: ERROR: Monitor mon1 failed to reach quorum
2025-02-24 10:51:21 ceph-node-5 mds[7753]: DEBUG: MGR module loaded successfully
2025-02-24 10:50:59 ceph-node-2 mgr[9544]: WARNING: Data corruption detected in object pool
2025-02-24 10:51:15 ceph-node-5 client[2190]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:50:49 ceph-node-2 osd[3273]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:22 ceph-node-5 osd[4229]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:50:37 ceph-node-2 radosgw[2931]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:51:29 ceph-node-4 mgr[9553]: DEBUG: OSD rebalancing completed
2025-02-24 10:50:53 ceph-node-2 client[7136]: WARNING: Monitor down, unable to connect to quorum
2025-02-24 10:51:08 ceph-node-5 radosgw[3417]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:50:41 ceph-node-3 mgr[9615]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:11 ceph-node-3 client[2219]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:51:18 ceph-node-2 osd[5166]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:13 ceph-node-2 osd[8026]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:27 ceph-node-2 mds[3478]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:06 ceph-node-1 client[5114]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:50:38 ceph-node-5 client[4934]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:50:53 ceph-node-1 osd[4494]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:28 ceph-node-2 mgr[9102]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:02 ceph-node-3 client[9709]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:38 ceph-node-1 osd[9632]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:50:34 ceph-node-2 osd[7070]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:29 ceph-node-3 client[6478]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:54 ceph-node-5 client[9875]: INFO: Monitor map has been updated
2025-02-24 10:50:53 ceph-node-5 client[8896]: ERROR: CRITICAL: PG 2.1 stuck in inconsistent state
2025-02-24 10:51:13 ceph-node-2 client[6293]: CRITICAL: CRITICAL: PG 2.1 stuck in inconsistent state
2025-02-24 10:50:42 ceph-node-4 radosgw[7907]: ERROR: Data corruption detected in object pool
2025-02-24 10:50:42 ceph-node-1 mds[1240]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:51:06 ceph-node-5 client[5147]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:27 ceph-node-4 mds[4707]: NOTICE: Monitor map has been updated
2025-02-24 10:50:49 ceph-node-1 osd[7243]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:23 ceph-node-3 client[6058]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:50:54 ceph-node-2 mon[3360]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:28 ceph-node-5 radosgw[3221]: CRITICAL: Cluster status: HEALTH_WARN
2025-02-24 10:50:36 ceph-node-1 mon[8711]: INFO: OSD rebalancing completed
2025-02-24 10:50:31 ceph-node-5 osd[9304]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:51:10 ceph-node-3 client[5600]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:51:22 ceph-node-3 client[3349]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:46 ceph-node-5 radosgw[2344]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:10 ceph-node-4 client[6728]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:50 ceph-node-5 mgr[7608]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:50:35 ceph-node-4 osd[4258]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:43 ceph-node-4 mgr[2468]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:50:37 ceph-node-1 radosgw[5125]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:50:43 ceph-node-1 client[7169]: INFO: PG 1.2 replicated successfully
2025-02-24 10:50:48 ceph-node-5 radosgw[2368]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:10 ceph-node-4 client[8066]: INFO: Data replication completed for object pool
2025-02-24 10:50:34 ceph-node-4 client[5534]: DEBUG: Monitor map has been updated
2025-02-24 10:51:07 ceph-node-3 client[2810]: INFO: MGR module loaded successfully
2025-02-24 10:51:01 ceph-node-4 osd[3684]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:50:52 ceph-node-3 mon[4556]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:10 ceph-node-3 mon[6227]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:16 ceph-node-1 mon[9252]: INFO: OSD rebalancing completed
2025-02-24 10:50:55 ceph-node-2 osd[8291]: DEBUG: Data replication completed for object pool
2025-02-24 10:50:57 ceph-node-1 radosgw[2967]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:50:39 ceph-node-1 radosgw[5990]: CRITICAL: Network partition detected between OSD nodes
2025-02-24 10:51:14 ceph-node-5 client[3683]: INFO: MGR module loaded successfully
2025-02-24 10:50:59 ceph-node-4 mgr[6435]: NOTICE: MGR module loaded successfully
2025-02-24 10:51:24 ceph-node-1 mon[7597]: ERROR: Authentication failure in RADOS Gateway
2025-02-24 10:51:21 ceph-node-4 osd[8841]: ERROR: Authentication failure in RADOS Gateway
2025-02-24 10:50:32 ceph-node-4 osd[1054]: NOTICE: MGR module loaded successfully
2025-02-24 10:51:08 ceph-node-1 mds[1158]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:14 ceph-node-4 client[4973]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:51:08 ceph-node-2 mon[1979]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:06 ceph-node-2 radosgw[6909]: INFO: OSD rebalancing completed
2025-02-24 10:50:34 ceph-node-1 mgr[1216]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:50:57 ceph-node-4 osd[1312]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:08 ceph-node-4 mds[9285]: NOTICE: MGR module loaded successfully
2025-02-24 10:50:45 ceph-node-5 osd[6484]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:19 ceph-node-2 client[1708]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:50:50 ceph-node-3 mds[6099]: CRITICAL: Monitor down, unable to connect to quorum
2025-02-24 10:50:57 ceph-node-4 mon[1198]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:13 ceph-node-2 radosgw[8567]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:51:14 ceph-node-3 mon[5699]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:50:40 ceph-node-1 mgr[7598]: INFO: Monitor map has been updated
2025-02-24 10:50:40 ceph-node-5 mds[1457]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:05 ceph-node-5 client[4696]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:50:37 ceph-node-3 client[4319]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:51:17 ceph-node-4 mon[4907]: WARNING: Slow request detected on OSD 4.3
2025-02-24 10:50:55 ceph-node-1 osd[8379]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:50:44 ceph-node-1 mgr[8125]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:57 ceph-node-3 osd[6940]: INFO: OSD rebalancing completed
2025-02-24 10:50:45 ceph-node-3 radosgw[6275]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:26 ceph-node-2 client[4227]: DEBUG: OSD rebalancing completed
2025-02-24 10:51:00 ceph-node-3 client[6368]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:27 ceph-node-5 radosgw[8199]: DEBUG: MGR module loaded successfully
2025-02-24 10:50:58 ceph-node-2 mon[5646]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:21 ceph-node-3 mgr[1978]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:50:55 ceph-node-4 radosgw[9922]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:27 ceph-node-5 osd[1038]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:09 ceph-node-2 client[5948]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:38 ceph-node-5 osd[3772]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:50:35 ceph-node-5 client[6054]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:51:02 ceph-node-1 radosgw[1003]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:01 ceph-node-2 mon[4157]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:06 ceph-node-4 osd[9320]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:31 ceph-node-5 mds[8275]: WARNING: Disk failure reported on ceph-node-4
2025-02-24 10:51:20 ceph-node-4 mon[3353]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:50:37 ceph-node-5 client[7821]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:59 ceph-node-1 mgr[2499]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:14 ceph-node-1 osd[5679]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:55 ceph-node-3 mon[1425]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:50:38 ceph-node-5 radosgw[1859]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:27 ceph-node-3 osd[8639]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:20 ceph-node-4 mgr[8717]: NOTICE: OSD rebalancing completed
2025-02-24 10:50:58 ceph-node-2 osd[1039]: NOTICE: Monitor map has been updated
2025-02-24 10:50:42 ceph-node-3 mon[8772]: INFO: Monitor map has been updated
2025-02-24 10:50:59 ceph-node-3 client[8935]: DEBUG: OSD rebalancing completed
2025-02-24 10:51:21 ceph-node-5 mgr[8236]: DEBUG: Monitor map has been updated
2025-02-24 10:50:49 ceph-node-2 mgr[2484]: NOTICE: OSD rebalancing completed
2025-02-24 10:50:55 ceph-node-5 client[4065]: INFO: Data replication completed for object pool
2025-02-24 10:50:57 ceph-node-1 mds[4367]: WARNING: Client connection timeout detected
2025-02-24 10:50:51 ceph-node-5 mgr[6720]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:50:55 ceph-node-2 mon[9248]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:50:56 ceph-node-4 client[9256]: INFO: PG 1.2 replicated successfully
2025-02-24 10:51:18 ceph-node-3 osd[1454]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:50:53 ceph-node-5 client[5045]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:32 ceph-node-3 mon[3778]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:50:56 ceph-node-1 mds[3246]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:49 ceph-node-5 client[1836]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:53 ceph-node-5 mgr[9979]: NOTICE: Monitor map has been updated
2025-02-24 10:50:37 ceph-node-5 mgr[3755]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:51:17 ceph-node-2 mon[5690]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:25 ceph-node-1 mgr[5624]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:52 ceph-node-5 mgr[7464]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:15 ceph-node-3 mgr[6297]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:51:02 ceph-node-1 radosgw[9912]: NOTICE: Data replication completed for object pool
2025-02-24 10:51:01 ceph-node-4 mds[1323]: DEBUG: Monitor map has been updated
2025-02-24 10:51:31 ceph-node-5 mgr[7177]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:04 ceph-node-3 mon[9943]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:40 ceph-node-5 radosgw[4107]: ERROR: Authentication failure in RADOS Gateway
2025-02-24 10:50:57 ceph-node-2 mds[6056]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:54 ceph-node-4 osd[5766]: WARNING: Monitor mon1 failed to reach quorum
2025-02-24 10:50:36 ceph-node-3 osd[3224]: DEBUG: OSD rebalancing completed
2025-02-24 10:50:37 ceph-node-5 mds[9858]: WARNING: OSD 3.1 marked down due to heartbeat failure
2025-02-24 10:50:58 ceph-node-1 mgr[3735]: DEBUG: OSD rebalancing completed
2025-02-24 10:50:35 ceph-node-1 client[3423]: INFO: OSD disk space utilization: 70%
2025-02-24 10:51:23 ceph-node-1 mgr[4166]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:50:51 ceph-node-4 client[8442]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:50:49 ceph-node-3 mon[2636]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:06 ceph-node-3 radosgw[6040]: INFO: Monitor map has been updated
2025-02-24 10:50:44 ceph-node-2 mgr[8992]: ERROR: OSD disk space utilization exceeded 90%
2025-02-24 10:50:48 ceph-node-3 mgr[9327]: INFO: PG 1.2 replicated successfully
2025-02-24 10:51:16 ceph-node-5 osd[8096]: NOTICE: OSD rebalancing completed
2025-02-24 10:51:07 ceph-node-1 osd[9809]: DEBUG: OSD rebalancing completed
2025-02-24 10:50:59 ceph-node-3 mon[2771]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:03 ceph-node-3 client[3049]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:50:39 ceph-node-2 client[5158]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:51:10 ceph-node-3 osd[4092]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:51:21 ceph-node-4 mds[5422]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:06 ceph-node-4 mon[7862]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:16 ceph-node-3 mon[6721]: CRITICAL: Authentication failure in RADOS Gateway
2025-02-24 10:50:47 ceph-node-1 radosgw[4522]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:02 ceph-node-3 mgr[3112]: ERROR: Client connection timeout detected
2025-02-24 10:50:59 ceph-node-5 mds[7802]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:15 ceph-node-2 mon[1701]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:51:02 ceph-node-2 mon[7233]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:50:40 ceph-node-1 radosgw[1766]: NOTICE: Data replication completed for object pool
2025-02-24 10:50:47 ceph-node-3 mds[1203]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:17 ceph-node-2 radosgw[6496]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:50:55 ceph-node-5 mon[2408]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:52 ceph-node-1 client[6080]: INFO: Data replication completed for object pool
2025-02-24 10:50:38 ceph-node-2 client[6944]: NOTICE: Monitor map has been updated
2025-02-24 10:51:06 ceph-node-1 mon[2835]: INFO: MGR module loaded successfully
2025-02-24 10:50:49 ceph-node-2 radosgw[1902]: INFO: PG 1.2 replicated successfully
2025-02-24 10:51:05 ceph-node-5 mgr[8902]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:51:16 ceph-node-4 client[3814]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:51:24 ceph-node-4 mgr[1022]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:53 ceph-node-3 mds[1908]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:48 ceph-node-1 mgr[2854]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:26 ceph-node-4 mds[5215]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:22 ceph-node-3 client[3389]: WARNING: CRITICAL: PG 2.1 stuck in inconsistent state
2025-02-24 10:51:28 ceph-node-4 osd[5617]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:50:47 ceph-node-1 client[8154]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:13 ceph-node-4 osd[8154]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:51:15 ceph-node-4 osd[7082]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:51:05 ceph-node-4 mon[3277]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:51:20 ceph-node-3 osd[8828]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:51:06 ceph-node-5 client[4566]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:51:04 ceph-node-4 mgr[8508]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:08 ceph-node-4 mds[3669]: INFO: Data replication completed for object pool
2025-02-24 10:50:57 ceph-node-4 mgr[2746]: INFO: MGR module loaded successfully
2025-02-24 10:50:46 ceph-node-5 osd[3661]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:51:16 ceph-node-3 osd[9118]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:50:39 ceph-node-2 mgr[8839]: INFO: PG 1.2 replicated successfully
2025-02-24 10:50:36 ceph-node-4 mgr[6642]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:55 ceph-node-3 mgr[9889]: ERROR: Journal write failure detected on OSD node
2025-02-24 10:50:35 ceph-node-3 radosgw[7746]: DEBUG: MGR module loaded successfully
2025-02-24 10:50:40 ceph-node-2 mds[4018]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:50:37 ceph-node-4 mgr[4603]: CRITICAL: CRITICAL: PG 2.1 stuck in inconsistent state
2025-02-24 10:50:59 ceph-node-5 radosgw[2074]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:25 ceph-node-1 client[5192]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:10 ceph-node-5 mon[6652]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:08 ceph-node-3 mon[7433]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:08 ceph-node-1 mds[3131]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:51:01 ceph-node-2 mon[5544]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:51:21 ceph-node-4 radosgw[3229]: ERROR: OSD 3.1 marked down due to heartbeat failure
2025-02-24 10:50:40 ceph-node-3 radosgw[4076]: INFO: OSD rebalancing completed
2025-02-24 10:51:24 ceph-node-4 client[5928]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:50:31 ceph-node-2 mon[2248]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:20 ceph-node-2 mon[9899]: DEBUG: Monitor map has been updated
2025-02-24 10:51:03 ceph-node-5 osd[8485]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:50:55 ceph-node-3 client[1209]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:50:39 ceph-node-4 client[2329]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:31 ceph-node-3 mds[3268]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:30 ceph-node-3 mds[9233]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:50:39 ceph-node-3 mds[8782]: INFO: Data replication completed for object pool
2025-02-24 10:50:55 ceph-node-4 client[4543]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:39 ceph-node-4 client[5633]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:50:31 ceph-node-1 client[6693]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:51:27 ceph-node-2 mon[4848]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:50:31 ceph-node-2 mon[7509]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:50:46 ceph-node-5 mgr[5661]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:38 ceph-node-1 client[2164]: CRITICAL: CRITICAL: Data loss detected on pool 1
2025-02-24 10:51:29 ceph-node-4 mds[6226]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:51:17 ceph-node-3 mon[9587]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:52 ceph-node-1 osd[6498]: DEBUG: Monitor map has been updated
2025-02-24 10:50:55 ceph-node-1 osd[4513]: DEBUG: Monitor map has been updated
2025-02-24 10:50:39 ceph-node-2 osd[7372]: NOTICE: MGR module loaded successfully
2025-02-24 10:50:37 ceph-node-5 client[1066]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:08 ceph-node-2 radosgw[2045]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:54 ceph-node-5 mgr[6018]: NOTICE: MGR module loaded successfully
2025-02-24 10:50:51 ceph-node-4 osd[8718]: ERROR: CRITICAL: Data loss detected on pool 1
2025-02-24 10:51:02 ceph-node-4 radosgw[2940]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:50:55 ceph-node-5 mgr[6252]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:21 ceph-node-3 osd[8532]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:50:39 ceph-node-5 mon[5817]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:50:46 ceph-node-1 osd[6845]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:31 ceph-node-1 mgr[7217]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:11 ceph-node-2 radosgw[8672]: INFO: OSD disk space utilization: 70%
2025-02-24 10:50:46 ceph-node-3 mon[9634]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:50:38 ceph-node-1 mgr[3652]: DEBUG: Monitor map has been updated
2025-02-24 10:50:41 ceph-node-4 mds[8269]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:50:39 ceph-node-5 mgr[5632]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:51:03 ceph-node-1 radosgw[8747]: WARNING: Monitor mon1 failed to reach quorum
2025-02-24 10:51:14 ceph-node-4 mon[2831]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:50:56 ceph-node-5 client[1758]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:51:05 ceph-node-2 mgr[1288]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:51:30 ceph-node-5 radosgw[2186]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:50:42 ceph-node-4 mgr[7990]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:28 ceph-node-3 mon[5999]: INFO: OSD rebalancing completed
2025-02-24 10:50:44 ceph-node-3 osd[3038]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:49 ceph-node-2 osd[8183]: WARNING: Monitor mon1 failed to reach quorum
2025-02-24 10:51:10 ceph-node-4 mon[6549]: DEBUG: OSD rebalancing completed
2025-02-24 10:51:08 ceph-node-1 mgr[3402]: DEBUG: Data replication completed for object pool
2025-02-24 10:50:52 ceph-node-2 mds[7915]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:51:17 ceph-node-3 osd[8448]: DEBUG: MGR module loaded successfully
2025-02-24 10:50:51 ceph-node-3 mon[1775]: DEBUG: Monitor map has been updated
2025-02-24 10:50:59 ceph-node-5 client[3785]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:14 ceph-node-5 mds[5331]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:02 ceph-node-2 mon[9581]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:33 ceph-node-5 mgr[4390]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:05 ceph-node-2 mgr[3842]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:50:38 ceph-node-2 osd[6969]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:35 ceph-node-1 client[6675]: DEBUG: MGR module loaded successfully
2025-02-24 10:50:37 ceph-node-2 client[6751]: CRITICAL: Monitor mon1 failed to reach quorum
2025-02-24 10:50:42 ceph-node-1 mgr[7685]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:50:39 ceph-node-5 mon[1338]: DEBUG: Monitor map has been updated
2025-02-24 10:51:10 ceph-node-1 mgr[1694]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:10 ceph-node-4 mon[7861]: CRITICAL: Disk failure reported on ceph-node-4
2025-02-24 10:50:42 ceph-node-2 mgr[3139]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:51:25 ceph-node-1 mgr[7245]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:50:44 ceph-node-5 radosgw[3416]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:51:02 ceph-node-1 mon[9466]: INFO: Data replication completed for object pool
2025-02-24 10:51:08 ceph-node-3 radosgw[3247]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:59 ceph-node-3 mds[8416]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:51:12 ceph-node-3 client[2972]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:51:06 ceph-node-3 osd[5678]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:10 ceph-node-4 mds[4296]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:51 ceph-node-1 radosgw[4837]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:24 ceph-node-1 radosgw[7502]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:51:00 ceph-node-1 radosgw[8840]: NOTICE: OSD rebalancing completed
2025-02-24 10:50:36 ceph-node-4 mds[7893]: DEBUG: Monitor map has been updated
2025-02-24 10:51:18 ceph-node-5 mon[4125]: INFO: Monitor map has been updated
2025-02-24 10:50:58 ceph-node-2 mon[4606]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:02 ceph-node-4 radosgw[5350]: INFO: Data replication completed for object pool
2025-02-24 10:50:38 ceph-node-2 client[2135]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:51:21 ceph-node-5 client[5179]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:00 ceph-node-2 osd[4739]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:51:31 ceph-node-1 mds[5224]: NOTICE: Monitor map has been updated
2025-02-24 10:50:51 ceph-node-3 client[3117]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:24 ceph-node-5 osd[9931]: CRITICAL: Data corruption detected in object pool
2025-02-24 10:51:12 ceph-node-1 mon[3132]: WARNING: Cluster status: HEALTH_WARN
2025-02-24 10:51:00 ceph-node-3 mon[8103]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:50:50 ceph-node-4 osd[9101]: DEBUG: OSD rebalancing completed
2025-02-24 10:50:50 ceph-node-3 client[5560]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:51:09 ceph-node-5 radosgw[1829]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:06 ceph-node-2 mon[3893]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:50:52 ceph-node-5 client[3469]: NOTICE: MGR module loaded successfully
2025-02-24 10:51:24 ceph-node-3 mds[1469]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:50:46 ceph-node-2 mon[6790]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:50:42 ceph-node-2 mds[1780]: NOTICE: MGR module loaded successfully
2025-02-24 10:50:42 ceph-node-4 client[8148]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:51:11 ceph-node-4 osd[8994]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:51:14 ceph-node-2 mgr[6961]: NOTICE: Data replication completed for object pool
2025-02-24 10:51:18 ceph-node-1 client[6447]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:51:08 ceph-node-1 client[1107]: DEBUG: OSD rebalancing completed
2025-02-24 10:50:37 ceph-node-3 mon[9171]: NOTICE: OSD rebalancing completed
2025-02-24 10:51:06 ceph-node-3 client[4297]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:50:54 ceph-node-5 client[4210]: CRITICAL: CRITICAL: PG 2.1 stuck in inconsistent state
2025-02-24 10:51:05 ceph-node-1 osd[2002]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:10 ceph-node-1 mgr[6135]: CRITICAL: Cluster status: HEALTH_WARN
2025-02-24 10:50:58 ceph-node-3 mgr[8859]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:51:19 ceph-node-1 osd[1496]: CRITICAL: OSD 3.1 marked down due to heartbeat failure
2025-02-24 10:51:26 ceph-node-4 mon[4800]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:50:46 ceph-node-5 osd[8670]: NOTICE: MGR module loaded successfully
2025-02-24 10:50:54 ceph-node-2 mon[4281]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:13 ceph-node-2 radosgw[5922]: INFO: MGR module loaded successfully
2025-02-24 10:50:36 ceph-node-1 client[1841]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:51:00 ceph-node-5 osd[8376]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:02 ceph-node-2 mgr[4944]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:51:11 ceph-node-1 client[6814]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:17 ceph-node-1 mgr[7595]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:50:38 ceph-node-2 mds[3691]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:08 ceph-node-2 mon[3347]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:20 ceph-node-1 mgr[3385]: ERROR: Data corruption detected in object pool
2025-02-24 10:51:20 ceph-node-5 mgr[3906]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:39 ceph-node-1 mon[7603]: CRITICAL: Network partition detected between OSD nodes
2025-02-24 10:51:06 ceph-node-2 client[8416]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:50:42 ceph-node-1 osd[7764]: DEBUG: OSD rebalancing completed
2025-02-24 10:51:03 ceph-node-4 osd[9752]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:50:49 ceph-node-1 mds[8822]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:51:24 ceph-node-5 mon[5442]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:51:29 ceph-node-2 client[7204]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:50:49 ceph-node-3 client[1296]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:51:26 ceph-node-5 mds[9335]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:37 ceph-node-1 mon[9609]: INFO: OSD rebalancing completed
2025-02-24 10:50:31 ceph-node-3 osd[8715]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:19 ceph-node-1 client[2498]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:38 ceph-node-5 osd[5990]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:50:45 ceph-node-4 mgr[5244]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:51:14 ceph-node-4 osd[1425]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:29 ceph-node-5 mgr[4824]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:59 ceph-node-2 mgr[6737]: DEBUG: Data replication completed for object pool
2025-02-24 10:50:49 ceph-node-3 mon[4715]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:01 ceph-node-2 radosgw[4691]: NOTICE: OSD rebalancing completed
2025-02-24 10:51:17 ceph-node-3 radosgw[9795]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:46 ceph-node-5 mds[3392]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:14 ceph-node-3 mgr[1945]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:50:53 ceph-node-5 osd[2661]: INFO: PG 1.2 replicated successfully
2025-02-24 10:50:44 ceph-node-4 mds[9046]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:50:44 ceph-node-5 osd[1567]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:20 ceph-node-1 mon[4411]: INFO: PG 1.2 replicated successfully
2025-02-24 10:51:09 ceph-node-5 mgr[7733]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:50:39 ceph-node-4 mds[1202]: INFO: MGR module loaded successfully
2025-02-24 10:51:00 ceph-node-4 radosgw[1349]: WARNING: High I/O latency detected in radosgw service
2025-02-24 10:51:28 ceph-node-1 mgr[1660]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:00 ceph-node-3 osd[8203]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:53 ceph-node-4 mds[8344]: INFO: PG 1.2 replicated successfully
2025-02-24 10:50:47 ceph-node-1 mds[5243]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:15 ceph-node-5 mgr[2718]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:51:04 ceph-node-4 client[1019]: CRITICAL: OSD disk space utilization exceeded 90%
2025-02-24 10:51:21 ceph-node-4 mgr[4030]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:31 ceph-node-1 osd[8543]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:50:53 ceph-node-4 radosgw[2793]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:06 ceph-node-2 mon[4506]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:28 ceph-node-2 mon[2896]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:50:46 ceph-node-1 mon[7161]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:00 ceph-node-4 client[5884]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:51:14 ceph-node-4 mgr[6825]: DEBUG: OSD rebalancing completed
2025-02-24 10:50:37 ceph-node-2 mon[2272]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:00 ceph-node-5 osd[6244]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:50:56 ceph-node-2 mon[9806]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:51:26 ceph-node-5 radosgw[8398]: CRITICAL: Network partition detected between OSD nodes
2025-02-24 10:50:32 ceph-node-4 osd[8126]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:50:56 ceph-node-4 osd[4200]: INFO: PG 1.2 replicated successfully
2025-02-24 10:51:04 ceph-node-2 radosgw[3301]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:14 ceph-node-4 client[4211]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:11 ceph-node-2 osd[2450]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:02 ceph-node-1 radosgw[8676]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:26 ceph-node-2 client[1053]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:31 ceph-node-5 mgr[1732]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:50:59 ceph-node-3 mds[7578]: NOTICE: Monitor map has been updated
2025-02-24 10:50:46 ceph-node-5 radosgw[3935]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:10 ceph-node-3 osd[7687]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:44 ceph-node-4 mds[6530]: DEBUG: OSD rebalancing completed
2025-02-24 10:51:08 ceph-node-4 mds[8368]: WARNING: Disk failure reported on ceph-node-4
2025-02-24 10:51:04 ceph-node-3 client[9819]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:50:34 ceph-node-2 osd[8485]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:50:53 ceph-node-1 client[3556]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:24 ceph-node-4 osd[5134]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:46 ceph-node-3 client[6298]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:10 ceph-node-5 mgr[9895]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:08 ceph-node-2 radosgw[2354]: INFO: Monitor map has been updated
2025-02-24 10:51:21 ceph-node-5 radosgw[1812]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:51:22 ceph-node-4 radosgw[2020]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:08 ceph-node-3 mon[3798]: INFO: OSD rebalancing completed
2025-02-24 10:51:05 ceph-node-3 client[1678]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:50:46 ceph-node-1 mgr[1841]: INFO: OSD rebalancing completed
2025-02-24 10:50:48 ceph-node-3 mds[9891]: ERROR: Journal write failure detected on OSD node
2025-02-24 10:50:41 ceph-node-2 radosgw[4735]: ERROR: Cluster status: HEALTH_WARN
2025-02-24 10:50:32 ceph-node-1 mon[5179]: DEBUG: MGR module loaded successfully
2025-02-24 10:50:53 ceph-node-1 osd[8734]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:47 ceph-node-5 mon[3072]: CRITICAL: Monitor down, unable to connect to quorum
2025-02-24 10:51:04 ceph-node-3 client[9380]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:31 ceph-node-5 osd[2741]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:50:59 ceph-node-1 osd[2269]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:50:55 ceph-node-2 mds[3898]: INFO: OSD rebalancing completed
2025-02-24 10:51:01 ceph-node-5 mgr[4939]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:42 ceph-node-3 mon[1692]: DEBUG: Data replication completed for object pool
2025-02-24 10:50:34 ceph-node-2 mds[9351]: DEBUG: OSD rebalancing completed
2025-02-24 10:50:52 ceph-node-3 osd[7785]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:50:40 ceph-node-4 mds[5405]: WARNING: OSD 3.1 marked down due to heartbeat failure
2025-02-24 10:50:32 ceph-node-4 mds[9136]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:51:17 ceph-node-1 mon[7484]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:19 ceph-node-3 radosgw[5078]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:28 ceph-node-4 mgr[5362]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:51:28 ceph-node-4 radosgw[6192]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:50:35 ceph-node-4 mds[3309]: DEBUG: Monitor map has been updated
2025-02-24 10:51:00 ceph-node-2 mgr[4752]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:50:56 ceph-node-2 client[9293]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:50:47 ceph-node-4 client[8086]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:51:27 ceph-node-3 osd[6575]: INFO: Data replication completed for object pool
2025-02-24 10:51:19 ceph-node-5 mds[9503]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:39 ceph-node-4 client[9776]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:50:49 ceph-node-3 mon[4931]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:50:45 ceph-node-1 mon[1704]: DEBUG: MGR module loaded successfully
2025-02-24 10:50:55 ceph-node-4 osd[6455]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:51:21 ceph-node-2 osd[4344]: INFO: Data replication completed for object pool
2025-02-24 10:50:32 ceph-node-3 mgr[2335]: NOTICE: Data replication completed for object pool
2025-02-24 10:51:02 ceph-node-1 osd[7315]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:57 ceph-node-2 mgr[4328]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:30 ceph-node-5 mds[8614]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:38 ceph-node-1 mon[2647]: INFO: Monitor map has been updated
2025-02-24 10:50:56 ceph-node-1 osd[4297]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:39 ceph-node-1 radosgw[5982]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:50:43 ceph-node-3 mgr[2159]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:50:50 ceph-node-4 osd[5793]: INFO: Monitor map has been updated
2025-02-24 10:51:14 ceph-node-1 osd[2741]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:05 ceph-node-4 mds[4879]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:50:49 ceph-node-2 osd[1261]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:57 ceph-node-5 mon[8116]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:31 ceph-node-4 client[6491]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:23 ceph-node-3 client[8823]: WARNING: Monitor down, unable to connect to quorum
2025-02-24 10:51:09 ceph-node-3 client[2185]: NOTICE: Data replication completed for object pool
2025-02-24 10:51:09 ceph-node-2 client[5761]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:50 ceph-node-3 osd[9647]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:50:34 ceph-node-4 radosgw[6059]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:50:36 ceph-node-4 client[6900]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:06 ceph-node-4 client[6190]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:50:35 ceph-node-1 client[1948]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:35 ceph-node-2 radosgw[5304]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:10 ceph-node-1 osd[4946]: CRITICAL: Authentication failure in RADOS Gateway
2025-02-24 10:50:49 ceph-node-2 osd[7054]: DEBUG: Data replication completed for object pool
2025-02-24 10:50:31 ceph-node-3 client[5374]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:50:33 ceph-node-2 osd[6416]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:50:41 ceph-node-4 radosgw[2515]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:37 ceph-node-3 client[7131]: NOTICE: Monitor map has been updated
2025-02-24 10:51:22 ceph-node-3 client[2702]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:38 ceph-node-4 mon[2252]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:50:32 ceph-node-4 client[4679]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:38 ceph-node-3 radosgw[2394]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:42 ceph-node-1 mds[9413]: INFO: OSD disk space utilization: 70%
2025-02-24 10:51:12 ceph-node-3 mds[2397]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:29 ceph-node-1 osd[2460]: INFO: MGR module loaded successfully
2025-02-24 10:50:44 ceph-node-2 mon[5833]: NOTICE: MGR module loaded successfully
2025-02-24 10:51:19 ceph-node-1 mgr[6864]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:51:27 ceph-node-4 radosgw[9532]: NOTICE: Monitor map has been updated
2025-02-24 10:50:33 ceph-node-4 radosgw[6458]: CRITICAL: Journal write failure detected on OSD node
2025-02-24 10:51:09 ceph-node-5 client[1550]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:50:41 ceph-node-1 mgr[1462]: INFO: OSD disk space utilization: 70%
2025-02-24 10:50:40 ceph-node-4 mds[7471]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:50:47 ceph-node-3 osd[5106]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:50:48 ceph-node-2 radosgw[3531]: DEBUG: OSD rebalancing completed
2025-02-24 10:50:44 ceph-node-3 radosgw[2357]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:01 ceph-node-4 mgr[1973]: INFO: OSD rebalancing completed
2025-02-24 10:51:23 ceph-node-3 osd[4222]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:51:13 ceph-node-5 mgr[4855]: INFO: OSD disk space utilization: 70%
2025-02-24 10:50:39 ceph-node-5 mon[4605]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:50:33 ceph-node-4 mon[2076]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:50:44 ceph-node-4 mgr[9243]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:13 ceph-node-3 radosgw[2638]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:40 ceph-node-2 radosgw[7257]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:51:02 ceph-node-1 osd[9473]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:45 ceph-node-1 mon[1558]: INFO: OSD disk space utilization: 70%
2025-02-24 10:50:33 ceph-node-4 mon[7655]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:58 ceph-node-1 mds[2783]: NOTICE: OSD rebalancing completed
2025-02-24 10:51:00 ceph-node-2 mgr[1107]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:51:29 ceph-node-4 client[9950]: NOTICE: OSD rebalancing completed
2025-02-24 10:50:57 ceph-node-5 osd[2039]: DEBUG: MGR module loaded successfully
2025-02-24 10:50:38 ceph-node-4 client[2900]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:51 ceph-node-4 mds[2148]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:18 ceph-node-5 mds[9522]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:50:54 ceph-node-1 client[9636]: CRITICAL: Cluster status: HEALTH_WARN
2025-02-24 10:50:39 ceph-node-3 radosgw[5367]: INFO: OSD disk space utilization: 70%
2025-02-24 10:50:36 ceph-node-4 osd[6885]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:50:33 ceph-node-2 radosgw[6767]: CRITICAL: High I/O latency detected in radosgw service
2025-02-24 10:50:59 ceph-node-5 mgr[5931]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:51:06 ceph-node-1 client[8066]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:50:31 ceph-node-4 mgr[5874]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:50:56 ceph-node-1 mgr[7249]: INFO: OSD rebalancing completed
2025-02-24 10:51:10 ceph-node-5 client[3896]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:23 ceph-node-1 client[4361]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:22 ceph-node-5 mds[4461]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:51:04 ceph-node-2 osd[8966]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:51:25 ceph-node-2 client[6240]: CRITICAL: CRITICAL: Data loss detected on pool 1
2025-02-24 10:51:03 ceph-node-2 mon[6886]: INFO: Monitor map has been updated
2025-02-24 10:50:32 ceph-node-4 radosgw[1277]: DEBUG: OSD rebalancing completed
2025-02-24 10:50:58 ceph-node-2 mon[8751]: INFO: OSD rebalancing completed
2025-02-24 10:50:50 ceph-node-1 mon[4470]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:51:11 ceph-node-4 mgr[8838]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:51:12 ceph-node-1 mds[1420]: ERROR: Uncorrectable ECC error detected in MDS cache
2025-02-24 10:50:34 ceph-node-4 radosgw[6700]: INFO: OSD disk space utilization: 70%
2025-02-24 10:50:42 ceph-node-1 client[1027]: DEBUG: MGR module loaded successfully
2025-02-24 10:50:46 ceph-node-1 mon[5530]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:51:29 ceph-node-5 mgr[8698]: NOTICE: Monitor map has been updated
2025-02-24 10:51:05 ceph-node-1 osd[2223]: CRITICAL: Monitor mon1 failed to reach quorum
2025-02-24 10:50:55 ceph-node-4 mds[2170]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:06 ceph-node-3 osd[6778]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:52 ceph-node-5 osd[4161]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:50:58 ceph-node-2 mds[6770]: ERROR: Authentication failure in RADOS Gateway
2025-02-24 10:51:30 ceph-node-2 mgr[4156]: ERROR: Client connection timeout detected
2025-02-24 10:51:26 ceph-node-1 mon[1591]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:50:39 ceph-node-1 mon[5266]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:30 ceph-node-3 mon[5815]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:26 ceph-node-1 osd[6530]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:50:56 ceph-node-1 mon[4442]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:51 ceph-node-5 mds[2629]: INFO: OSD disk space utilization: 70%
2025-02-24 10:51:10 ceph-node-5 mon[4382]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:02 ceph-node-4 radosgw[9929]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:19 ceph-node-5 mds[2163]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:51:03 ceph-node-2 mds[4659]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:17 ceph-node-4 mgr[8530]: INFO: Data replication completed for object pool
2025-02-24 10:51:21 ceph-node-4 osd[8853]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:50:42 ceph-node-1 client[4679]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:34 ceph-node-3 client[6100]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:51:23 ceph-node-5 osd[5184]: DEBUG: OSD rebalancing completed
2025-02-24 10:50:39 ceph-node-3 radosgw[1032]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:50:37 ceph-node-4 radosgw[8936]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:50:53 ceph-node-2 client[1686]: INFO: MGR module loaded successfully
2025-02-24 10:51:05 ceph-node-2 mon[2274]: INFO: OSD disk space utilization: 70%
2025-02-24 10:50:44 ceph-node-3 mds[8558]: DEBUG: OSD rebalancing completed
2025-02-24 10:50:40 ceph-node-4 mds[1047]: ERROR: Monitor mon1 failed to reach quorum
2025-02-24 10:51:23 ceph-node-3 mds[8642]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:25 ceph-node-5 mds[9547]: NOTICE: OSD rebalancing completed
2025-02-24 10:50:46 ceph-node-5 mgr[3238]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:21 ceph-node-1 mds[4410]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:47 ceph-node-1 client[3330]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:50:44 ceph-node-3 mon[1762]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:31 ceph-node-4 mon[8574]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:29 ceph-node-4 radosgw[5107]: DEBUG: Data replication completed for object pool
2025-02-24 10:50:37 ceph-node-1 osd[3542]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:50:34 ceph-node-5 mon[3714]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:31 ceph-node-4 radosgw[5837]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:50:54 ceph-node-4 client[8453]: INFO: PG 1.2 replicated successfully
2025-02-24 10:50:50 ceph-node-5 mds[9689]: NOTICE: OSD rebalancing completed
2025-02-24 10:51:23 ceph-node-2 mon[5916]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:33 ceph-node-2 osd[2496]: INFO: OSD rebalancing completed
2025-02-24 10:50:54 ceph-node-1 mgr[6376]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:08 ceph-node-2 osd[6653]: DEBUG: Monitor map has been updated
2025-02-24 10:51:04 ceph-node-2 client[4153]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:50:56 ceph-node-1 mon[5468]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:50:45 ceph-node-5 mgr[7277]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:51:05 ceph-node-3 osd[4723]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:22 ceph-node-4 mgr[1311]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:50:57 ceph-node-2 mon[1001]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:28 ceph-node-5 osd[7637]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:21 ceph-node-1 osd[9931]: NOTICE: OSD rebalancing completed
2025-02-24 10:50:52 ceph-node-5 radosgw[2328]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:29 ceph-node-2 radosgw[1281]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:50:40 ceph-node-5 radosgw[6778]: INFO: MGR module loaded successfully
2025-02-24 10:51:22 ceph-node-3 client[9680]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:50:49 ceph-node-2 radosgw[3032]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:04 ceph-node-5 osd[7614]: INFO: Monitor map has been updated
2025-02-24 10:51:30 ceph-node-4 radosgw[4567]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:33 ceph-node-4 mds[4583]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:50:39 ceph-node-3 client[3383]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:50:39 ceph-node-2 mon[4100]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:50 ceph-node-4 mds[4473]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:51:10 ceph-node-5 mon[9800]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:16 ceph-node-5 mds[1940]: WARNING: Client connection timeout detected
2025-02-24 10:51:19 ceph-node-4 client[6896]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:46 ceph-node-1 mgr[1720]: INFO: Monitor map has been updated
2025-02-24 10:51:23 ceph-node-5 radosgw[3633]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:51:29 ceph-node-2 radosgw[5581]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:26 ceph-node-3 radosgw[3756]: WARNING: Disk failure reported on ceph-node-4
2025-02-24 10:50:57 ceph-node-2 mds[1871]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:42 ceph-node-4 mon[6672]: NOTICE: Monitor map has been updated
2025-02-24 10:51:28 ceph-node-3 mds[9394]: WARNING: Data corruption detected in object pool
2025-02-24 10:50:57 ceph-node-4 osd[1826]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:16 ceph-node-5 radosgw[4517]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:50:54 ceph-node-2 client[2935]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:00 ceph-node-2 client[7309]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:04 ceph-node-3 mon[8477]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:50 ceph-node-4 mon[6130]: WARNING: Monitor mon1 failed to reach quorum
2025-02-24 10:51:21 ceph-node-1 mgr[7247]: NOTICE: Monitor map has been updated
2025-02-24 10:51:14 ceph-node-2 client[8999]: NOTICE: Monitor map has been updated
2025-02-24 10:51:07 ceph-node-4 mon[9387]: INFO: Monitor map has been updated
2025-02-24 10:50:45 ceph-node-1 radosgw[7362]: NOTICE: Monitor map has been updated
2025-02-24 10:50:44 ceph-node-2 mon[7741]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:38 ceph-node-5 radosgw[4539]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:50:47 ceph-node-5 osd[3495]: DEBUG: MGR module loaded successfully
2025-02-24 10:50:40 ceph-node-2 mds[2656]: NOTICE: MGR module loaded successfully
2025-02-24 10:51:25 ceph-node-3 mds[4935]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:52 ceph-node-4 radosgw[6333]: NOTICE: Data replication completed for object pool
2025-02-24 10:50:35 ceph-node-4 radosgw[5155]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:51:19 ceph-node-2 radosgw[7163]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:51:17 ceph-node-3 osd[2633]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:50:59 ceph-node-2 client[1309]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:51:20 ceph-node-2 mon[1510]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:50:43 ceph-node-3 radosgw[6503]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:20 ceph-node-1 mds[4527]: NOTICE: OSD rebalancing completed
2025-02-24 10:50:56 ceph-node-4 mds[4340]: NOTICE: OSD rebalancing completed
2025-02-24 10:50:35 ceph-node-2 osd[5284]: NOTICE: MGR module loaded successfully
2025-02-24 10:50:34 ceph-node-3 client[2968]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:50:44 ceph-node-1 client[7657]: DEBUG: OSD rebalancing completed
2025-02-24 10:50:57 ceph-node-5 radosgw[8654]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:51:04 ceph-node-3 mon[3997]: NOTICE: Data replication completed for object pool
2025-02-24 10:51:26 ceph-node-2 mds[3975]: INFO: OSD disk space utilization: 70%
2025-02-24 10:50:50 ceph-node-1 mgr[5593]: NOTICE: Data replication completed for object pool
2025-02-24 10:50:55 ceph-node-5 radosgw[9381]: INFO: PG 1.2 replicated successfully
2025-02-24 10:50:59 ceph-node-2 mds[8694]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:50:41 ceph-node-4 client[1865]: INFO: Monitor map has been updated
2025-02-24 10:51:25 ceph-node-1 radosgw[4760]: DEBUG: Data replication completed for object pool
2025-02-24 10:50:57 ceph-node-1 radosgw[7647]: CRITICAL: Uncorrectable ECC error detected in MDS cache
2025-02-24 10:51:17 ceph-node-4 mgr[7017]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:19 ceph-node-2 radosgw[4439]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:50:58 ceph-node-5 mds[5951]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:50:51 ceph-node-2 radosgw[4501]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:49 ceph-node-3 mgr[3390]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:51:19 ceph-node-5 radosgw[4389]: CRITICAL: OSD disk space utilization exceeded 90%
2025-02-24 10:50:45 ceph-node-4 client[9352]: INFO: Monitor map has been updated
2025-02-24 10:50:50 ceph-node-2 mds[8483]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:50:47 ceph-node-2 mgr[1789]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:13 ceph-node-5 client[2583]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:50:55 ceph-node-1 osd[2946]: NOTICE: Monitor map has been updated
2025-02-24 10:51:28 ceph-node-3 mds[7554]: INFO: OSD rebalancing completed
2025-02-24 10:50:37 ceph-node-5 mds[3530]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:36 ceph-node-4 osd[6991]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:13 ceph-node-4 radosgw[5762]: INFO: OSD rebalancing completed
2025-02-24 10:51:24 ceph-node-3 client[8360]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:20 ceph-node-5 mon[2685]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:51:10 ceph-node-5 mgr[1436]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:50:32 ceph-node-2 mds[5768]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:50:53 ceph-node-3 mds[5860]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:50:44 ceph-node-1 client[2329]: INFO: PG 1.2 replicated successfully
2025-02-24 10:50:45 ceph-node-2 mds[6896]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:50:43 ceph-node-4 mds[8324]: NOTICE: Data replication completed for object pool
2025-02-24 10:50:32 ceph-node-2 client[9003]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:50:46 ceph-node-4 mgr[7239]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:35 ceph-node-5 radosgw[8615]: WARNING: Authentication failure in RADOS Gateway
2025-02-24 10:51:12 ceph-node-3 radosgw[1291]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:29 ceph-node-5 mgr[3785]: INFO: Monitor map has been updated
2025-02-24 10:50:48 ceph-node-3 osd[4474]: DEBUG: OSD rebalancing completed
2025-02-24 10:50:37 ceph-node-1 mgr[6983]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:50:49 ceph-node-4 mgr[1723]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:50:42 ceph-node-3 osd[3980]: DEBUG: OSD rebalancing completed
2025-02-24 10:50:32 ceph-node-3 radosgw[4770]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:29 ceph-node-2 osd[5937]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:15 ceph-node-4 mds[3570]: NOTICE: MGR module loaded successfully
2025-02-24 10:50:42 ceph-node-5 mds[4087]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:53 ceph-node-5 osd[8581]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:19 ceph-node-3 mds[6540]: INFO: OSD disk space utilization: 70%
2025-02-24 10:51:31 ceph-node-4 mds[3731]: ERROR: Disk failure reported on ceph-node-4
2025-02-24 10:51:09 ceph-node-1 mon[9432]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:04 ceph-node-4 mgr[8550]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:50:39 ceph-node-2 osd[4413]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:51:30 ceph-node-5 mgr[7915]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:50:38 ceph-node-4 osd[5347]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:25 ceph-node-2 radosgw[7130]: ERROR: High I/O latency detected in radosgw service
2025-02-24 10:51:30 ceph-node-2 mds[5132]: INFO: OSD rebalancing completed
2025-02-24 10:50:44 ceph-node-5 osd[4688]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:50:31 ceph-node-5 mgr[7082]: INFO: PG 1.2 replicated successfully
2025-02-24 10:51:03 ceph-node-3 client[2904]: NOTICE: Data replication completed for object pool
2025-02-24 10:51:31 ceph-node-1 osd[3915]: WARNING: CRITICAL: PG 2.1 stuck in inconsistent state
2025-02-24 10:50:58 ceph-node-3 osd[9310]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:51:15 ceph-node-5 client[5291]: NOTICE: OSD rebalancing completed
2025-02-24 10:51:19 ceph-node-2 mon[2544]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:40 ceph-node-5 osd[2708]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:57 ceph-node-1 mgr[8621]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:50:37 ceph-node-2 mon[3346]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:51:14 ceph-node-2 client[2632]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:03 ceph-node-1 client[7253]: CRITICAL: Authentication failure in RADOS Gateway
2025-02-24 10:51:21 ceph-node-5 mon[8630]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:50:34 ceph-node-5 osd[9674]: NOTICE: OSD rebalancing completed
2025-02-24 10:51:12 ceph-node-5 mgr[8609]: INFO: PG 1.2 replicated successfully
2025-02-24 10:50:57 ceph-node-5 client[7558]: DEBUG: MGR module loaded successfully
2025-02-24 10:50:51 ceph-node-3 osd[6048]: INFO: OSD rebalancing completed
2025-02-24 10:50:56 ceph-node-4 client[3809]: INFO: Data replication completed for object pool
2025-02-24 10:51:12 ceph-node-1 osd[6638]: NOTICE: Data replication completed for object pool
2025-02-24 10:50:56 ceph-node-1 mgr[1897]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:29 ceph-node-5 client[5893]: WARNING: Monitor down, unable to connect to quorum
2025-02-24 10:51:19 ceph-node-3 mds[8326]: NOTICE: MGR module loaded successfully
2025-02-24 10:51:26 ceph-node-5 mds[2044]: WARNING: OSD disk space utilization exceeded 90%
2025-02-24 10:50:31 ceph-node-4 mds[7628]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:03 ceph-node-2 radosgw[8886]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:50:43 ceph-node-3 radosgw[1199]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:11 ceph-node-1 mgr[7735]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:02 ceph-node-1 osd[6405]: WARNING: MDS failure detected: metadata inconsistency
2025-02-24 10:51:21 ceph-node-2 mon[7291]: NOTICE: Monitor map has been updated
2025-02-24 10:51:24 ceph-node-1 mgr[4476]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:48 ceph-node-3 client[1812]: DEBUG: MGR module loaded successfully
2025-02-24 10:50:48 ceph-node-2 client[7733]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:52 ceph-node-2 mon[9852]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:00 ceph-node-4 mgr[7041]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:51:06 ceph-node-3 client[6384]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:51:17 ceph-node-1 client[3315]: INFO: PG 1.2 replicated successfully
2025-02-24 10:51:20 ceph-node-2 mgr[1103]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:07 ceph-node-2 radosgw[9812]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:31 ceph-node-5 radosgw[8907]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:50:43 ceph-node-1 osd[4187]: ERROR: CRITICAL: PG 2.1 stuck in inconsistent state
2025-02-24 10:50:56 ceph-node-2 mds[5244]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:10 ceph-node-1 client[9099]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:51:28 ceph-node-3 client[7265]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:00 ceph-node-2 osd[2234]: CRITICAL: Disk failure reported on ceph-node-4
2025-02-24 10:50:32 ceph-node-5 mon[1852]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:25 ceph-node-2 mgr[1355]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:29 ceph-node-1 client[6502]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:50:41 ceph-node-4 mds[9827]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:51:15 ceph-node-5 mds[7179]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:50:31 ceph-node-4 mgr[9868]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:50:44 ceph-node-4 osd[1854]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:51:13 ceph-node-1 radosgw[7326]: INFO: OSD disk space utilization: 70%
2025-02-24 10:51:08 ceph-node-2 mgr[8433]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:50 ceph-node-4 mds[9978]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:01 ceph-node-4 mgr[1258]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:51:09 ceph-node-1 mds[5365]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:51:01 ceph-node-2 mds[9963]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:51:14 ceph-node-1 client[8705]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:51:04 ceph-node-2 client[9024]: INFO: OSD rebalancing completed
2025-02-24 10:51:07 ceph-node-5 mds[4208]: INFO: OSD disk space utilization: 70%
2025-02-24 10:51:13 ceph-node-2 mgr[4455]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:52 ceph-node-5 radosgw[2374]: INFO: PG 1.2 replicated successfully
2025-02-24 10:51:00 ceph-node-2 client[5335]: NOTICE: Monitor map has been updated
2025-02-24 10:51:11 ceph-node-5 mgr[3410]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:17 ceph-node-5 mon[2776]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:51:13 ceph-node-4 mds[7851]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:50:37 ceph-node-2 mgr[1805]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:51:03 ceph-node-1 client[2811]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:51:10 ceph-node-1 mgr[1055]: ERROR: Monitor down, unable to connect to quorum
2025-02-24 10:51:29 ceph-node-2 mon[1426]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:55 ceph-node-2 osd[1319]: WARNING: MDS failure detected: metadata inconsistency
2025-02-24 10:51:17 ceph-node-2 osd[5070]: ERROR: Monitor down, unable to connect to quorum
2025-02-24 10:51:13 ceph-node-2 radosgw[8939]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:26 ceph-node-5 mon[9803]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:50:53 ceph-node-3 mgr[1256]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:31 ceph-node-4 mon[7766]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:50:47 ceph-node-5 mgr[1694]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:30 ceph-node-3 osd[5418]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:31 ceph-node-3 radosgw[1632]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:50:47 ceph-node-4 mon[5875]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:50:46 ceph-node-1 client[7280]: NOTICE: Monitor map has been updated
2025-02-24 10:50:50 ceph-node-3 mgr[8426]: INFO: Monitor map has been updated
2025-02-24 10:50:57 ceph-node-4 mgr[2634]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:00 ceph-node-3 client[8798]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:42 ceph-node-1 client[7478]: ERROR: CRITICAL: PG 2.1 stuck in inconsistent state
2025-02-24 10:50:53 ceph-node-4 mon[9582]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:36 ceph-node-2 mds[4343]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:58 ceph-node-3 mon[5135]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:50:54 ceph-node-2 client[5805]: INFO: Monitor map has been updated
2025-02-24 10:50:54 ceph-node-5 radosgw[2502]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:51:23 ceph-node-1 client[9840]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:16 ceph-node-4 osd[9252]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:51 ceph-node-3 mon[2428]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:51:26 ceph-node-1 client[1104]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:51:02 ceph-node-5 mds[1903]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:10 ceph-node-4 osd[9955]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:13 ceph-node-1 radosgw[1962]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:46 ceph-node-1 mon[7361]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:51:12 ceph-node-1 client[2836]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:21 ceph-node-4 client[7881]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:51:09 ceph-node-5 osd[7046]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:51:21 ceph-node-3 radosgw[4273]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:24 ceph-node-1 mon[6793]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:50:33 ceph-node-3 mds[3977]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:51:20 ceph-node-4 client[9234]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:50:34 ceph-node-1 osd[3247]: ERROR: Monitor down, unable to connect to quorum
2025-02-24 10:50:55 ceph-node-2 mon[4887]: CRITICAL: High I/O latency detected in radosgw service
2025-02-24 10:51:08 ceph-node-5 mds[5266]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:18 ceph-node-2 mon[4665]: DEBUG: Monitor map has been updated
2025-02-24 10:50:36 ceph-node-4 radosgw[6592]: INFO: OSD disk space utilization: 70%
2025-02-24 10:50:36 ceph-node-3 mgr[9297]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:07 ceph-node-5 mds[9877]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:24 ceph-node-1 osd[4252]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:50:50 ceph-node-3 mgr[6318]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:50:48 ceph-node-1 radosgw[7392]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:50:39 ceph-node-2 osd[6883]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:51:07 ceph-node-2 mgr[5071]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:50:59 ceph-node-3 mon[3975]: DEBUG: OSD rebalancing completed
2025-02-24 10:51:30 ceph-node-4 mgr[3122]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:13 ceph-node-4 osd[5170]: DEBUG: MGR module loaded successfully
2025-02-24 10:50:55 ceph-node-1 mgr[2158]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:51:27 ceph-node-1 mgr[8872]: NOTICE: Monitor map has been updated
2025-02-24 10:51:18 ceph-node-5 mgr[4684]: INFO: OSD rebalancing completed
2025-02-24 10:51:03 ceph-node-3 mon[9387]: DEBUG: MGR module loaded successfully
2025-02-24 10:50:35 ceph-node-1 mon[6079]: ERROR: CRITICAL: PG 2.1 stuck in inconsistent state
2025-02-24 10:50:51 ceph-node-3 radosgw[2867]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:05 ceph-node-5 mon[4499]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:23 ceph-node-1 radosgw[7765]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:50:56 ceph-node-5 client[3997]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:27 ceph-node-5 client[8661]: INFO: Monitor map has been updated
2025-02-24 10:51:21 ceph-node-1 radosgw[7143]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:39 ceph-node-4 osd[4245]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:19 ceph-node-2 radosgw[3363]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:47 ceph-node-5 mds[5112]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:53 ceph-node-2 osd[2956]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:50:34 ceph-node-2 mds[6137]: NOTICE: Data replication completed for object pool
2025-02-24 10:50:45 ceph-node-4 osd[5948]: DEBUG: OSD rebalancing completed
2025-02-24 10:51:02 ceph-node-3 osd[5460]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:51:31 ceph-node-2 mgr[9686]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:50:41 ceph-node-2 osd[1344]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:49 ceph-node-2 osd[6531]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:35 ceph-node-5 mgr[2279]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:50:48 ceph-node-4 mds[2672]: CRITICAL: Monitor mon1 failed to reach quorum
2025-02-24 10:51:15 ceph-node-3 client[8966]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:51:24 ceph-node-1 radosgw[8168]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:37 ceph-node-1 mgr[4816]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:50:55 ceph-node-1 mon[8199]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:50:54 ceph-node-4 osd[8305]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:06 ceph-node-5 osd[1458]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:28 ceph-node-5 radosgw[8045]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:03 ceph-node-3 radosgw[9023]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:47 ceph-node-5 mgr[9387]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:45 ceph-node-5 osd[9344]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:58 ceph-node-2 osd[6412]: INFO: Data replication completed for object pool
2025-02-24 10:50:47 ceph-node-4 client[8167]: NOTICE: OSD rebalancing completed
2025-02-24 10:51:13 ceph-node-1 client[8659]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:03 ceph-node-4 mon[4730]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:51:04 ceph-node-5 client[6569]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:34 ceph-node-3 osd[4592]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:19 ceph-node-5 mgr[4121]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:36 ceph-node-1 client[2163]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:10 ceph-node-1 osd[1962]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:50:41 ceph-node-2 mgr[8401]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:02 ceph-node-1 mon[3926]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:39 ceph-node-4 mds[2623]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:17 ceph-node-2 mon[6032]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:14 ceph-node-5 client[9482]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:50:50 ceph-node-4 mds[2139]: CRITICAL: Cluster status: HEALTH_WARN
2025-02-24 10:51:27 ceph-node-5 mon[6673]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:50:49 ceph-node-1 osd[4038]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:23 ceph-node-5 radosgw[1522]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:01 ceph-node-1 osd[5528]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:50:56 ceph-node-4 mds[4455]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:50:57 ceph-node-4 mgr[3128]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:03 ceph-node-1 osd[2673]: NOTICE: Data replication completed for object pool
2025-02-24 10:51:14 ceph-node-3 mon[5645]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:50:53 ceph-node-1 mon[1875]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:50:43 ceph-node-4 mds[7895]: DEBUG: Monitor map has been updated
2025-02-24 10:50:51 ceph-node-3 mon[2354]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:50:39 ceph-node-4 osd[1270]: WARNING: Client connection timeout detected
2025-02-24 10:51:04 ceph-node-3 mds[1932]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:51:09 ceph-node-4 mgr[6277]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:51:04 ceph-node-2 mon[7656]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:45 ceph-node-1 mgr[4868]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:35 ceph-node-3 mgr[4921]: INFO: Monitor map has been updated
2025-02-24 10:51:11 ceph-node-2 mds[1471]: CRITICAL: Disk failure reported on ceph-node-4
2025-02-24 10:50:51 ceph-node-2 osd[6315]: INFO: Monitor map has been updated
2025-02-24 10:51:26 ceph-node-3 osd[2475]: DEBUG: OSD rebalancing completed
2025-02-24 10:51:24 ceph-node-2 mgr[1315]: DEBUG: Monitor map has been updated
2025-02-24 10:51:26 ceph-node-5 mds[5276]: INFO: OSD disk space utilization: 70%
2025-02-24 10:50:38 ceph-node-3 mon[5772]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:50:45 ceph-node-4 osd[1794]: ERROR: Monitor down, unable to connect to quorum
2025-02-24 10:50:44 ceph-node-3 mgr[8512]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:51:00 ceph-node-5 mds[6030]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:51:21 ceph-node-5 mon[6941]: NOTICE: MGR module loaded successfully
2025-02-24 10:51:22 ceph-node-3 mon[9341]: CRITICAL: Monitor down, unable to connect to quorum
2025-02-24 10:50:54 ceph-node-1 mds[1112]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:50:52 ceph-node-4 mgr[5404]: WARNING: Disk failure reported on ceph-node-4
2025-02-24 10:50:34 ceph-node-3 mds[3136]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:50:49 ceph-node-5 radosgw[4314]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:50:51 ceph-node-1 mgr[1097]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:50:35 ceph-node-2 osd[7557]: NOTICE: OSD rebalancing completed
2025-02-24 10:51:28 ceph-node-4 mds[1964]: DEBUG: Monitor map has been updated
2025-02-24 10:51:11 ceph-node-4 radosgw[9221]: DEBUG: Monitor map has been updated
2025-02-24 10:50:40 ceph-node-1 mon[1148]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:22 ceph-node-2 mds[2028]: NOTICE: Monitor map has been updated
2025-02-24 10:50:37 ceph-node-1 osd[7993]: INFO: OSD rebalancing completed
2025-02-24 10:50:50 ceph-node-1 client[9728]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:57 ceph-node-2 mgr[6624]: WARNING: Network partition detected between OSD nodes
2025-02-24 10:51:29 ceph-node-3 mon[8883]: INFO: PG 1.2 replicated successfully
2025-02-24 10:50:46 ceph-node-2 client[8495]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:25 ceph-node-4 osd[9552]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:34 ceph-node-4 osd[8131]: DEBUG: MGR module loaded successfully
2025-02-24 10:50:39 ceph-node-2 mgr[1229]: CRITICAL: OSD 3.1 marked down due to heartbeat failure
2025-02-24 10:51:21 ceph-node-5 mds[6149]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:30 ceph-node-1 mon[9434]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:09 ceph-node-2 mon[6211]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:41 ceph-node-5 mgr[4358]: INFO: Monitor map has been updated
2025-02-24 10:51:01 ceph-node-5 radosgw[8066]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:08 ceph-node-5 mgr[9382]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:22 ceph-node-4 osd[8546]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:51:15 ceph-node-3 osd[2759]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:08 ceph-node-4 mon[3341]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:42 ceph-node-3 client[8314]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:09 ceph-node-5 mgr[5982]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:50:43 ceph-node-3 client[1558]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:16 ceph-node-4 mgr[5037]: INFO: OSD rebalancing completed
2025-02-24 10:51:20 ceph-node-3 client[3167]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:11 ceph-node-5 mds[5178]: NOTICE: OSD rebalancing completed
2025-02-24 10:50:34 ceph-node-2 client[4829]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:20 ceph-node-4 osd[5341]: WARNING: Authentication failure in RADOS Gateway
2025-02-24 10:51:29 ceph-node-2 osd[1600]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:01 ceph-node-4 mds[5192]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:42 ceph-node-5 mds[4717]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:51:30 ceph-node-3 mgr[3784]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:51:28 ceph-node-5 mds[4276]: NOTICE: MGR module loaded successfully
2025-02-24 10:50:39 ceph-node-1 mds[9408]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:30 ceph-node-5 osd[2339]: INFO: OSD rebalancing completed
2025-02-24 10:51:16 ceph-node-3 mgr[1535]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:51:28 ceph-node-5 mgr[3068]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:04 ceph-node-4 client[2653]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:50:38 ceph-node-4 radosgw[2736]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:55 ceph-node-5 mon[6309]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:28 ceph-node-5 osd[3323]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:03 ceph-node-1 osd[1862]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:06 ceph-node-3 mds[3542]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:50:47 ceph-node-3 osd[7202]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:36 ceph-node-5 mds[7008]: DEBUG: MGR module loaded successfully
2025-02-24 10:50:50 ceph-node-5 client[8081]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:50:55 ceph-node-2 mon[6642]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:03 ceph-node-4 osd[7288]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:09 ceph-node-3 client[9950]: INFO: OSD rebalancing completed
2025-02-24 10:51:25 ceph-node-3 mgr[9604]: WARNING: CRITICAL: Data loss detected on pool 1
2025-02-24 10:51:11 ceph-node-5 radosgw[8925]: NOTICE: Data replication completed for object pool
2025-02-24 10:51:29 ceph-node-4 osd[6246]: INFO: OSD disk space utilization: 70%
2025-02-24 10:50:37 ceph-node-3 radosgw[1299]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:50:58 ceph-node-3 mds[6247]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:05 ceph-node-3 mgr[2391]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:51:10 ceph-node-2 mds[5266]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:51:24 ceph-node-5 mgr[8246]: WARNING: Network partition detected between OSD nodes
2025-02-24 10:51:22 ceph-node-3 mon[4477]: ERROR: Slow request detected on OSD 4.3
2025-02-24 10:51:06 ceph-node-3 client[6457]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:50:31 ceph-node-4 radosgw[6955]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:51:05 ceph-node-1 mon[8104]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:50:31 ceph-node-4 osd[9518]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:00 ceph-node-5 mon[5170]: NOTICE: Monitor map has been updated
2025-02-24 10:51:24 ceph-node-3 radosgw[3079]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:50:55 ceph-node-4 mon[5073]: INFO: PG 1.2 replicated successfully
2025-02-24 10:51:11 ceph-node-5 mgr[2075]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:51 ceph-node-5 client[5982]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:01 ceph-node-1 mgr[7701]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:20 ceph-node-4 mon[4631]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:08 ceph-node-2 mgr[9145]: NOTICE: OSD rebalancing completed
2025-02-24 10:51:18 ceph-node-4 radosgw[3313]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:50:45 ceph-node-5 mon[6162]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:42 ceph-node-3 mds[5852]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:51:19 ceph-node-4 client[3522]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:20 ceph-node-1 client[1217]: NOTICE: OSD rebalancing completed
2025-02-24 10:51:07 ceph-node-5 osd[3485]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:58 ceph-node-2 client[8096]: DEBUG: Data replication completed for object pool
2025-02-24 10:50:39 ceph-node-4 mds[1117]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:29 ceph-node-5 osd[7873]: WARNING: Uncorrectable ECC error detected in MDS cache
2025-02-24 10:51:11 ceph-node-3 mon[7271]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:51:17 ceph-node-3 mgr[5628]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:39 ceph-node-2 mgr[2359]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:12 ceph-node-4 osd[5310]: DEBUG: MGR module loaded successfully
2025-02-24 10:50:56 ceph-node-2 radosgw[6575]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:07 ceph-node-4 radosgw[1435]: ERROR: High I/O latency detected in radosgw service
2025-02-24 10:51:24 ceph-node-1 mgr[3050]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:10 ceph-node-4 client[7593]: NOTICE: Monitor map has been updated
2025-02-24 10:51:06 ceph-node-2 mgr[2782]: DEBUG: Monitor map has been updated
2025-02-24 10:50:39 ceph-node-5 mgr[4856]: INFO: Data replication completed for object pool
2025-02-24 10:51:13 ceph-node-1 osd[6260]: INFO: Data replication completed for object pool
2025-02-24 10:50:41 ceph-node-2 client[8081]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:31 ceph-node-3 client[6791]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:51:20 ceph-node-3 mgr[9459]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:51:26 ceph-node-2 radosgw[2091]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:13 ceph-node-1 radosgw[8545]: NOTICE: Data replication completed for object pool
2025-02-24 10:51:00 ceph-node-3 mgr[3397]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:51:23 ceph-node-2 client[5664]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:51:27 ceph-node-3 radosgw[8565]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:33 ceph-node-2 osd[7643]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:31 ceph-node-2 mds[3283]: CRITICAL: Client connection timeout detected
2025-02-24 10:50:51 ceph-node-1 mon[1215]: WARNING: Uncorrectable ECC error detected in MDS cache
2025-02-24 10:50:38 ceph-node-5 mds[5399]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:42 ceph-node-1 mon[9367]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:50:48 ceph-node-4 mon[1271]: DEBUG: OSD rebalancing completed
2025-02-24 10:50:36 ceph-node-4 osd[3868]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:22 ceph-node-2 osd[4520]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:51:13 ceph-node-5 osd[7537]: INFO: OSD rebalancing completed
2025-02-24 10:51:21 ceph-node-2 mgr[2760]: INFO: OSD rebalancing completed
2025-02-24 10:51:00 ceph-node-3 radosgw[7805]: WARNING: Client connection timeout detected
2025-02-24 10:51:14 ceph-node-4 radosgw[5088]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:51:15 ceph-node-4 radosgw[2227]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:50:56 ceph-node-4 mgr[1341]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:50:37 ceph-node-5 mon[1815]: NOTICE: MGR module loaded successfully
2025-02-24 10:51:17 ceph-node-4 mds[6101]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:56 ceph-node-4 mds[7074]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:50:36 ceph-node-2 mgr[6581]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:51:22 ceph-node-4 mon[3102]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:26 ceph-node-4 mgr[8405]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:51:31 ceph-node-3 mds[5435]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:40 ceph-node-1 mon[4870]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:56 ceph-node-2 mon[6852]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:07 ceph-node-1 mds[6080]: DEBUG: Monitor map has been updated
2025-02-24 10:50:36 ceph-node-3 mgr[8590]: INFO: MGR module loaded successfully
2025-02-24 10:50:32 ceph-node-3 client[3270]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:29 ceph-node-1 radosgw[2035]: INFO: MGR module loaded successfully
2025-02-24 10:50:40 ceph-node-1 mon[4632]: DEBUG: Monitor map has been updated
2025-02-24 10:51:13 ceph-node-2 mds[7302]: NOTICE: Data replication completed for object pool
2025-02-24 10:50:48 ceph-node-2 radosgw[2059]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:01 ceph-node-1 mds[3874]: NOTICE: OSD rebalancing completed
2025-02-24 10:51:28 ceph-node-1 mgr[7139]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:51:04 ceph-node-1 radosgw[5525]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:43 ceph-node-3 osd[9278]: DEBUG: Data replication completed for object pool
2025-02-24 10:50:53 ceph-node-5 mon[1488]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:13 ceph-node-2 mds[6639]: INFO: PG 1.2 replicated successfully
2025-02-24 10:51:13 ceph-node-4 mon[6837]: INFO: Data replication completed for object pool
2025-02-24 10:50:42 ceph-node-3 mds[5195]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:15 ceph-node-3 mon[7434]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:28 ceph-node-1 mgr[4690]: CRITICAL: Monitor mon1 failed to reach quorum
2025-02-24 10:50:34 ceph-node-5 client[3196]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:51:26 ceph-node-2 osd[5887]: DEBUG: OSD rebalancing completed
2025-02-24 10:50:48 ceph-node-4 osd[2577]: INFO: Data replication completed for object pool
2025-02-24 10:51:02 ceph-node-4 mon[2150]: CRITICAL: Monitor mon1 failed to reach quorum
2025-02-24 10:51:31 ceph-node-4 radosgw[4966]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:01 ceph-node-1 mon[4336]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:07 ceph-node-3 client[9342]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:31 ceph-node-2 osd[5970]: DEBUG: Data replication completed for object pool
2025-02-24 10:50:40 ceph-node-5 mgr[9765]: ERROR: CRITICAL: PG 2.1 stuck in inconsistent state
2025-02-24 10:51:02 ceph-node-2 radosgw[9039]: INFO: Data replication completed for object pool
2025-02-24 10:51:14 ceph-node-3 osd[4020]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:12 ceph-node-2 client[9150]: NOTICE: Monitor map has been updated
2025-02-24 10:50:47 ceph-node-4 radosgw[5900]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:50:48 ceph-node-5 mds[1498]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:11 ceph-node-5 mon[3597]: INFO: Data replication completed for object pool
2025-02-24 10:50:46 ceph-node-3 mds[3029]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:50:58 ceph-node-3 client[5652]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:05 ceph-node-4 radosgw[9615]: INFO: OSD disk space utilization: 70%
2025-02-24 10:51:26 ceph-node-3 mds[8695]: DEBUG: Data replication completed for object pool
2025-02-24 10:50:47 ceph-node-5 radosgw[9669]: NOTICE: OSD rebalancing completed
2025-02-24 10:51:12 ceph-node-3 mgr[7533]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:50:44 ceph-node-4 mds[5258]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:23 ceph-node-5 radosgw[1444]: NOTICE: Data replication completed for object pool
2025-02-24 10:50:43 ceph-node-4 mgr[5345]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:50:33 ceph-node-2 mgr[1284]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:51:09 ceph-node-5 mon[4898]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:30 ceph-node-3 client[6082]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:50:39 ceph-node-1 client[4864]: NOTICE: OSD rebalancing completed
2025-02-24 10:51:22 ceph-node-3 mon[5378]: CRITICAL: Client connection timeout detected
2025-02-24 10:50:40 ceph-node-4 client[7366]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:51:29 ceph-node-1 mon[9199]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:51:29 ceph-node-5 mon[4787]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:00 ceph-node-4 mon[7649]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:50:39 ceph-node-4 mds[8313]: INFO: Monitor map has been updated
2025-02-24 10:50:58 ceph-node-2 osd[9925]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:16 ceph-node-1 mgr[2353]: INFO: OSD rebalancing completed
2025-02-24 10:51:21 ceph-node-4 osd[5508]: ERROR: Client connection timeout detected
2025-02-24 10:51:00 ceph-node-4 radosgw[9359]: INFO: Monitor map has been updated
2025-02-24 10:51:20 ceph-node-2 mgr[9182]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:50:33 ceph-node-1 mon[6299]: INFO: MGR module loaded successfully
2025-02-24 10:50:46 ceph-node-2 mgr[2632]: NOTICE: Monitor map has been updated
2025-02-24 10:50:52 ceph-node-2 mgr[2664]: NOTICE: Monitor map has been updated
2025-02-24 10:50:54 ceph-node-4 mgr[9596]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:50:52 ceph-node-1 radosgw[1763]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:50:53 ceph-node-2 radosgw[9897]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:15 ceph-node-3 mon[5634]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:50:40 ceph-node-2 osd[2859]: INFO: MGR module loaded successfully
2025-02-24 10:51:13 ceph-node-3 mds[6631]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:50:46 ceph-node-4 mgr[7887]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:03 ceph-node-4 mds[2661]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:26 ceph-node-4 osd[8652]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:50:59 ceph-node-5 mds[6016]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:31 ceph-node-2 mds[9580]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:15 ceph-node-1 client[7914]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:00 ceph-node-2 mds[9739]: DEBUG: MGR module loaded successfully
2025-02-24 10:50:58 ceph-node-4 mgr[9991]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:30 ceph-node-2 mds[4272]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:08 ceph-node-2 mon[5700]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:51:20 ceph-node-2 client[4572]: INFO: Monitor map has been updated
2025-02-24 10:50:51 ceph-node-2 mds[2274]: NOTICE: Data replication completed for object pool
2025-02-24 10:50:52 ceph-node-5 mon[1367]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:51:31 ceph-node-3 radosgw[3330]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:57 ceph-node-3 client[9658]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:50:51 ceph-node-2 client[5000]: INFO: OSD rebalancing completed
2025-02-24 10:50:53 ceph-node-2 client[5362]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:00 ceph-node-4 mgr[3800]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:51:02 ceph-node-3 mds[6032]: INFO: OSD disk space utilization: 70%
2025-02-24 10:51:11 ceph-node-3 mds[3622]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:41 ceph-node-5 mgr[8591]: ERROR: Disk failure reported on ceph-node-4
2025-02-24 10:50:35 ceph-node-1 mds[9532]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:05 ceph-node-4 mds[8519]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:07 ceph-node-5 mds[7373]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:14 ceph-node-5 mds[8227]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:50:38 ceph-node-2 mon[9591]: DEBUG: MGR module loaded successfully
2025-02-24 10:50:38 ceph-node-5 mds[6927]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:15 ceph-node-1 osd[6426]: INFO: PG 1.2 replicated successfully
2025-02-24 10:51:24 ceph-node-2 osd[8349]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:12 ceph-node-2 mon[8609]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:38 ceph-node-3 mgr[2362]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:50:31 ceph-node-4 radosgw[6131]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:25 ceph-node-5 osd[8375]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:21 ceph-node-2 osd[5889]: NOTICE: Data replication completed for object pool
2025-02-24 10:50:36 ceph-node-2 mon[8783]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:50:57 ceph-node-3 mon[6120]: CRITICAL: Cluster status: HEALTH_WARN
2025-02-24 10:50:52 ceph-node-5 mon[1533]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:59 ceph-node-2 osd[3438]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:46 ceph-node-4 mds[5105]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:06 ceph-node-3 mgr[2526]: NOTICE: Monitor map has been updated
2025-02-24 10:51:04 ceph-node-2 mgr[9643]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:24 ceph-node-4 mds[1985]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:05 ceph-node-3 mgr[2102]: ERROR: Network partition detected between OSD nodes
2025-02-24 10:50:54 ceph-node-2 osd[8792]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:51:16 ceph-node-2 osd[2934]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:50:57 ceph-node-4 radosgw[1782]: INFO: OSD disk space utilization: 70%
2025-02-24 10:50:57 ceph-node-1 client[1008]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:51:07 ceph-node-3 radosgw[7855]: DEBUG: Data replication completed for object pool
2025-02-24 10:50:38 ceph-node-1 mgr[2925]: DEBUG: Monitor map has been updated
2025-02-24 10:50:53 ceph-node-4 mds[1271]: NOTICE: MGR module loaded successfully
2025-02-24 10:50:53 ceph-node-3 mon[7846]: WARNING: Journal write failure detected on OSD node
2025-02-24 10:51:05 ceph-node-4 radosgw[1416]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:50 ceph-node-5 client[4078]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:03 ceph-node-4 mon[7161]: INFO: Monitor map has been updated
2025-02-24 10:51:12 ceph-node-1 radosgw[9469]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:50:38 ceph-node-4 osd[8854]: NOTICE: Data replication completed for object pool
2025-02-24 10:51:27 ceph-node-1 mds[8271]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:02 ceph-node-5 mgr[2837]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:50:45 ceph-node-4 mgr[8387]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:06 ceph-node-1 mgr[5133]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:50:54 ceph-node-5 radosgw[8476]: WARNING: Monitor mon1 failed to reach quorum
2025-02-24 10:50:41 ceph-node-2 mds[7263]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:00 ceph-node-5 mgr[1176]: WARNING: CRITICAL: PG 2.1 stuck in inconsistent state
2025-02-24 10:51:23 ceph-node-4 osd[1795]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:03 ceph-node-1 osd[1645]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:07 ceph-node-1 radosgw[8075]: ERROR: Uncorrectable ECC error detected in MDS cache
2025-02-24 10:51:28 ceph-node-2 mon[2342]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:03 ceph-node-4 mon[5354]: WARNING: Network partition detected between OSD nodes
2025-02-24 10:50:48 ceph-node-4 osd[8501]: ERROR: Network partition detected between OSD nodes
2025-02-24 10:51:04 ceph-node-2 mon[8983]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:51:20 ceph-node-1 mds[1701]: ERROR: OSD disk space utilization exceeded 90%
2025-02-24 10:50:53 ceph-node-1 radosgw[9751]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:50:48 ceph-node-1 radosgw[4113]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:38 ceph-node-2 radosgw[9054]: ERROR: Cluster status: HEALTH_WARN
2025-02-24 10:50:36 ceph-node-3 mon[6970]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:43 ceph-node-3 client[6538]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:50:46 ceph-node-5 radosgw[6131]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:57 ceph-node-1 mon[5034]: DEBUG: Monitor map has been updated
2025-02-24 10:51:23 ceph-node-3 mon[7317]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:46 ceph-node-1 mgr[9263]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:51:06 ceph-node-2 osd[7949]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:50:47 ceph-node-2 mon[1405]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:50:40 ceph-node-2 mon[9336]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:51:03 ceph-node-3 mgr[9570]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:50:52 ceph-node-4 mgr[4530]: NOTICE: OSD rebalancing completed
2025-02-24 10:51:11 ceph-node-2 radosgw[3356]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:32 ceph-node-2 mds[2096]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:28 ceph-node-2 osd[4643]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:05 ceph-node-4 client[5900]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:50:33 ceph-node-3 mon[2327]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:23 ceph-node-3 mgr[5984]: WARNING: Monitor mon1 failed to reach quorum
2025-02-24 10:50:51 ceph-node-4 client[4953]: INFO: MGR module loaded successfully
2025-02-24 10:50:49 ceph-node-5 client[5217]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:34 ceph-node-5 osd[3174]: NOTICE: Monitor map has been updated
2025-02-24 10:50:37 ceph-node-3 mds[7557]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:09 ceph-node-2 mgr[6979]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:48 ceph-node-1 client[2779]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:51:00 ceph-node-1 mgr[8828]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:50:38 ceph-node-4 osd[9458]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:08 ceph-node-4 mon[8233]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:19 ceph-node-1 mon[5054]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:59 ceph-node-5 mds[3777]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:51:06 ceph-node-4 mds[4159]: ERROR: CRITICAL: PG 2.1 stuck in inconsistent state
2025-02-24 10:51:30 ceph-node-1 mon[2994]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:51:27 ceph-node-3 client[1619]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:50:49 ceph-node-1 mon[8295]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:06 ceph-node-2 mon[5940]: DEBUG: Monitor map has been updated
2025-02-24 10:51:27 ceph-node-4 mgr[4386]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:14 ceph-node-4 osd[1341]: INFO: MGR module loaded successfully
2025-02-24 10:50:38 ceph-node-1 radosgw[9864]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:50:49 ceph-node-1 mgr[5184]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:50:51 ceph-node-4 client[6403]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:54 ceph-node-3 osd[6914]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:51:06 ceph-node-1 mon[2535]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:11 ceph-node-3 mgr[1446]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:17 ceph-node-4 radosgw[4332]: ERROR: Cluster status: HEALTH_WARN
2025-02-24 10:51:04 ceph-node-3 mds[4295]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:50:47 ceph-node-1 radosgw[9700]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:51:10 ceph-node-3 client[2900]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:51:18 ceph-node-4 osd[3728]: NOTICE: Monitor map has been updated
2025-02-24 10:51:25 ceph-node-4 radosgw[2124]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:50:34 ceph-node-1 client[6427]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:02 ceph-node-5 osd[7418]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:11 ceph-node-1 radosgw[9404]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:50:35 ceph-node-1 radosgw[6071]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:20 ceph-node-5 osd[3979]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:50:37 ceph-node-2 mds[9307]: DEBUG: Monitor map has been updated
2025-02-24 10:50:40 ceph-node-4 osd[8623]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:45 ceph-node-1 mds[9045]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:50:35 ceph-node-4 mon[5793]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:51:23 ceph-node-1 osd[6808]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:01 ceph-node-3 osd[5596]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:47 ceph-node-2 radosgw[9716]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:51:23 ceph-node-3 mds[6995]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:28 ceph-node-1 mds[4108]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:30 ceph-node-5 osd[5041]: INFO: Data replication completed for object pool
2025-02-24 10:51:17 ceph-node-1 radosgw[2210]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:51:29 ceph-node-5 client[7902]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:08 ceph-node-2 radosgw[6379]: WARNING: Uncorrectable ECC error detected in MDS cache
2025-02-24 10:50:52 ceph-node-1 radosgw[4311]: DEBUG: Monitor map has been updated
2025-02-24 10:51:01 ceph-node-5 mgr[6663]: NOTICE: Monitor map has been updated
2025-02-24 10:50:52 ceph-node-5 radosgw[2072]: INFO: PG 1.2 replicated successfully
2025-02-24 10:50:45 ceph-node-3 mds[5914]: ERROR: Monitor mon1 failed to reach quorum
2025-02-24 10:50:45 ceph-node-2 osd[1527]: NOTICE: MGR module loaded successfully
2025-02-24 10:50:31 ceph-node-3 osd[6579]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:50:45 ceph-node-1 mds[8576]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:16 ceph-node-1 radosgw[1370]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:50:52 ceph-node-1 mds[8003]: NOTICE: OSD rebalancing completed
2025-02-24 10:50:53 ceph-node-2 client[1423]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:38 ceph-node-5 osd[1312]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:51:16 ceph-node-1 mds[3092]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:17 ceph-node-2 mds[8657]: DEBUG: OSD rebalancing completed
2025-02-24 10:51:12 ceph-node-1 radosgw[4178]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:18 ceph-node-3 radosgw[4694]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:12 ceph-node-1 mgr[2539]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:18 ceph-node-1 client[6067]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:51:26 ceph-node-3 mon[9078]: INFO: Data replication completed for object pool
2025-02-24 10:50:59 ceph-node-5 client[4536]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:22 ceph-node-5 mgr[3695]: INFO: MGR module loaded successfully
2025-02-24 10:51:04 ceph-node-1 mgr[1660]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:36 ceph-node-1 mds[3702]: WARNING: Client connection timeout detected
2025-02-24 10:51:05 ceph-node-5 mgr[7976]: INFO: Data replication completed for object pool
2025-02-24 10:50:32 ceph-node-3 radosgw[3397]: DEBUG: OSD rebalancing completed
2025-02-24 10:50:35 ceph-node-4 radosgw[2443]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:51:10 ceph-node-5 radosgw[9358]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:51:11 ceph-node-1 mds[7578]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:22 ceph-node-1 client[2910]: DEBUG: Monitor map has been updated
2025-02-24 10:50:47 ceph-node-3 client[5105]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:02 ceph-node-1 client[3301]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:50:31 ceph-node-3 mds[6440]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:43 ceph-node-2 radosgw[2863]: INFO: MGR module loaded successfully
2025-02-24 10:50:45 ceph-node-5 mgr[1809]: WARNING: High I/O latency detected in radosgw service
2025-02-24 10:50:53 ceph-node-4 mon[7459]: ERROR: Data corruption detected in object pool
2025-02-24 10:51:16 ceph-node-5 osd[7724]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:50:34 ceph-node-4 mds[9482]: ERROR: OSD 3.1 marked down due to heartbeat failure
2025-02-24 10:50:58 ceph-node-2 mgr[1465]: INFO: Data replication completed for object pool
2025-02-24 10:51:09 ceph-node-5 mgr[7244]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:08 ceph-node-3 osd[7487]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:24 ceph-node-1 client[9912]: NOTICE: MGR module loaded successfully
2025-02-24 10:51:20 ceph-node-3 mon[9607]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:59 ceph-node-1 osd[1475]: WARNING: Cluster status: HEALTH_WARN
2025-02-24 10:51:08 ceph-node-2 osd[7598]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:51:11 ceph-node-2 mon[3536]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:31 ceph-node-4 osd[9623]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:50:48 ceph-node-5 radosgw[6464]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:35 ceph-node-3 radosgw[6110]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:51:04 ceph-node-4 radosgw[2165]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:10 ceph-node-4 mds[7639]: DEBUG: OSD rebalancing completed
2025-02-24 10:51:30 ceph-node-5 mds[4552]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:15 ceph-node-1 mds[1916]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:01 ceph-node-5 mds[4865]: WARNING: OSD disk space utilization exceeded 90%
2025-02-24 10:51:27 ceph-node-4 client[7603]: DEBUG: Monitor map has been updated
2025-02-24 10:51:24 ceph-node-3 mgr[7848]: INFO: Monitor map has been updated
2025-02-24 10:51:01 ceph-node-5 radosgw[5954]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:58 ceph-node-5 mon[9688]: DEBUG: Monitor map has been updated
2025-02-24 10:51:01 ceph-node-2 radosgw[4260]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:50:43 ceph-node-5 client[3164]: INFO: Monitor map has been updated
2025-02-24 10:51:27 ceph-node-3 radosgw[4529]: NOTICE: OSD rebalancing completed
2025-02-24 10:51:04 ceph-node-3 mds[8233]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:50:46 ceph-node-1 mon[3801]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:21 ceph-node-5 client[8619]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:50:31 ceph-node-3 client[4795]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:50:40 ceph-node-2 mon[4788]: NOTICE: OSD rebalancing completed
2025-02-24 10:50:31 ceph-node-2 radosgw[8915]: INFO: OSD disk space utilization: 70%
2025-02-24 10:50:35 ceph-node-5 mon[8532]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:23 ceph-node-4 mds[1566]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:51:29 ceph-node-2 mds[8853]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:51:10 ceph-node-1 mds[4475]: INFO: Data replication completed for object pool
2025-02-24 10:50:41 ceph-node-5 radosgw[6413]: DEBUG: Data replication completed for object pool
2025-02-24 10:50:57 ceph-node-4 radosgw[9651]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:14 ceph-node-5 client[4121]: INFO: PG 1.2 replicated successfully
2025-02-24 10:51:14 ceph-node-4 radosgw[2482]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:30 ceph-node-5 mgr[7912]: NOTICE: Monitor map has been updated
2025-02-24 10:50:56 ceph-node-4 radosgw[2478]: INFO: PG 1.2 replicated successfully
2025-02-24 10:50:54 ceph-node-5 osd[2751]: WARNING: OSD 3.1 marked down due to heartbeat failure
2025-02-24 10:51:17 ceph-node-4 mds[1007]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:00 ceph-node-4 client[1754]: CRITICAL: Slow request detected on OSD 4.3
2025-02-24 10:51:23 ceph-node-3 mgr[7676]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:50:39 ceph-node-2 mds[5492]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:06 ceph-node-2 client[1506]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:39 ceph-node-5 osd[5192]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:50:59 ceph-node-3 osd[1287]: WARNING: OSD disk space utilization exceeded 90%
2025-02-24 10:50:34 ceph-node-4 radosgw[6476]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:51:16 ceph-node-3 client[6813]: WARNING: OSD 3.1 marked down due to heartbeat failure
2025-02-24 10:51:25 ceph-node-1 radosgw[4176]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:09 ceph-node-4 osd[8487]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:50:55 ceph-node-5 osd[7771]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:50:52 ceph-node-4 radosgw[4616]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:50:43 ceph-node-4 mon[7271]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:17 ceph-node-5 mds[9033]: NOTICE: MGR module loaded successfully
2025-02-24 10:51:28 ceph-node-4 mds[9865]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:50:49 ceph-node-4 radosgw[6009]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:40 ceph-node-3 radosgw[5304]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:51:15 ceph-node-5 client[4935]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:06 ceph-node-2 mgr[8325]: INFO: Data replication completed for object pool
2025-02-24 10:51:13 ceph-node-4 mgr[5675]: NOTICE: OSD rebalancing completed
2025-02-24 10:50:52 ceph-node-5 osd[9606]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:33 ceph-node-5 client[5480]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:03 ceph-node-4 mds[3244]: NOTICE: MGR module loaded successfully
2025-02-24 10:50:43 ceph-node-3 client[3476]: INFO: OSD disk space utilization: 70%
2025-02-24 10:50:43 ceph-node-4 osd[4182]: INFO: OSD disk space utilization: 70%
2025-02-24 10:51:20 ceph-node-2 radosgw[9496]: DEBUG: Monitor map has been updated
2025-02-24 10:50:35 ceph-node-1 radosgw[5274]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:16 ceph-node-2 osd[5290]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:50:56 ceph-node-2 client[4818]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:17 ceph-node-3 osd[8661]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:50:58 ceph-node-3 mgr[9801]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:50:58 ceph-node-3 mgr[5856]: WARNING: Uncorrectable ECC error detected in MDS cache
2025-02-24 10:50:33 ceph-node-4 mgr[1117]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:51:06 ceph-node-4 osd[9805]: WARNING: Network partition detected between OSD nodes
2025-02-24 10:50:56 ceph-node-5 mon[3445]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:22 ceph-node-5 mon[7632]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:16 ceph-node-3 client[4262]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:23 ceph-node-1 osd[5332]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:51:02 ceph-node-4 osd[3137]: WARNING: Authentication failure in RADOS Gateway
2025-02-24 10:51:25 ceph-node-3 mon[7478]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:51:08 ceph-node-1 mon[1272]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:25 ceph-node-2 mgr[5229]: INFO: MGR module loaded successfully
2025-02-24 10:51:17 ceph-node-5 radosgw[1552]: DEBUG: Data replication completed for object pool
2025-02-24 10:50:31 ceph-node-2 mon[8416]: INFO: PG 1.2 replicated successfully
2025-02-24 10:51:08 ceph-node-1 osd[2347]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:50:40 ceph-node-1 mgr[8905]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:30 ceph-node-5 mds[7775]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:31 ceph-node-5 mon[3913]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:50:41 ceph-node-4 mds[6395]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:09 ceph-node-4 osd[3011]: INFO: OSD rebalancing completed
2025-02-24 10:51:21 ceph-node-2 mds[6636]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:21 ceph-node-4 radosgw[6122]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:50:56 ceph-node-2 mon[1047]: NOTICE: Data replication completed for object pool
2025-02-24 10:50:59 ceph-node-3 mds[8096]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:12 ceph-node-2 radosgw[3900]: INFO: PG 1.2 replicated successfully
2025-02-24 10:50:46 ceph-node-2 mgr[7215]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:51:26 ceph-node-5 mgr[6313]: WARNING: OSD 3.1 marked down due to heartbeat failure
2025-02-24 10:51:16 ceph-node-5 osd[8984]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:51:04 ceph-node-2 client[8309]: CRITICAL: Network partition detected between OSD nodes
2025-02-24 10:51:02 ceph-node-3 mgr[5849]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:53 ceph-node-3 mds[6037]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:51:20 ceph-node-1 mon[1979]: INFO: OSD rebalancing completed
2025-02-24 10:51:06 ceph-node-2 mon[2500]: CRITICAL: Network partition detected between OSD nodes
2025-02-24 10:50:41 ceph-node-4 radosgw[2716]: WARNING: OSD 3.1 marked down due to heartbeat failure
2025-02-24 10:50:44 ceph-node-2 osd[2639]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:50:49 ceph-node-3 osd[9892]: DEBUG: OSD rebalancing completed
2025-02-24 10:51:15 ceph-node-1 mon[3348]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:51:08 ceph-node-1 mds[2873]: WARNING: Journal write failure detected on OSD node
2025-02-24 10:50:37 ceph-node-4 radosgw[2684]: ERROR: Cluster status: HEALTH_WARN
2025-02-24 10:51:30 ceph-node-2 client[5123]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:51:28 ceph-node-1 radosgw[5642]: CRITICAL: Monitor mon1 failed to reach quorum
2025-02-24 10:50:49 ceph-node-5 osd[5408]: DEBUG: OSD rebalancing completed
2025-02-24 10:51:13 ceph-node-4 osd[4365]: WARNING: MDS failure detected: metadata inconsistency
2025-02-24 10:51:08 ceph-node-1 client[2957]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:31 ceph-node-4 mon[2122]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:51:30 ceph-node-4 client[1237]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:31 ceph-node-2 radosgw[8716]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:10 ceph-node-5 radosgw[2518]: INFO: MGR module loaded successfully
2025-02-24 10:50:54 ceph-node-3 client[3853]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:26 ceph-node-4 mon[5637]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:54 ceph-node-4 client[9521]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:51:07 ceph-node-5 osd[3165]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:50:51 ceph-node-5 mgr[3080]: DEBUG: Monitor map has been updated
2025-02-24 10:50:33 ceph-node-4 mgr[8954]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:02 ceph-node-3 mon[1438]: DEBUG: Monitor map has been updated
2025-02-24 10:50:47 ceph-node-5 radosgw[1500]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:50:42 ceph-node-5 mon[5266]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:33 ceph-node-1 mgr[1555]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:00 ceph-node-1 mds[5440]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:50:41 ceph-node-4 mgr[1444]: NOTICE: Monitor map has been updated
2025-02-24 10:50:38 ceph-node-5 osd[6096]: CRITICAL: OSD 3.1 marked down due to heartbeat failure
2025-02-24 10:51:09 ceph-node-2 mds[9859]: INFO: PG 1.2 replicated successfully
2025-02-24 10:51:17 ceph-node-4 osd[1515]: INFO: PG 1.2 replicated successfully
2025-02-24 10:51:02 ceph-node-1 osd[5407]: INFO: MGR module loaded successfully
2025-02-24 10:51:11 ceph-node-2 mds[2818]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:20 ceph-node-5 mgr[4685]: INFO: MGR module loaded successfully
2025-02-24 10:51:00 ceph-node-1 client[1287]: INFO: OSD disk space utilization: 70%
2025-02-24 10:50:38 ceph-node-2 osd[7067]: WARNING: Journal write failure detected on OSD node
2025-02-24 10:51:24 ceph-node-3 osd[7982]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:12 ceph-node-1 mds[7476]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:50:46 ceph-node-3 mon[1848]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:40 ceph-node-4 mgr[3872]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:29 ceph-node-2 mds[4246]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:12 ceph-node-4 mgr[1184]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:51:27 ceph-node-5 mgr[5087]: NOTICE: MGR module loaded successfully
2025-02-24 10:51:12 ceph-node-4 client[4651]: CRITICAL: Journal write failure detected on OSD node
2025-02-24 10:50:34 ceph-node-5 client[8251]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:51:20 ceph-node-1 mon[2157]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:50:35 ceph-node-5 mon[9309]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:27 ceph-node-1 radosgw[6251]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:50:32 ceph-node-3 client[8516]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:50:36 ceph-node-1 radosgw[3005]: INFO: Data replication completed for object pool
2025-02-24 10:51:25 ceph-node-5 mon[8347]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:50:38 ceph-node-3 mon[2953]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:40 ceph-node-1 osd[6086]: WARNING: Slow request detected on OSD 4.3
2025-02-24 10:50:41 ceph-node-3 client[4749]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:22 ceph-node-2 mon[1111]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:30 ceph-node-4 osd[1081]: INFO: OSD rebalancing completed
2025-02-24 10:51:16 ceph-node-5 mon[2637]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:51:22 ceph-node-1 mgr[7040]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:17 ceph-node-5 mds[3520]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:21 ceph-node-1 mds[6540]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:50:50 ceph-node-2 radosgw[2840]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:50:49 ceph-node-5 mds[6559]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:13 ceph-node-4 osd[1806]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:28 ceph-node-5 mgr[8861]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:04 ceph-node-2 client[8339]: WARNING: OSD 3.1 marked down due to heartbeat failure
2025-02-24 10:51:20 ceph-node-4 radosgw[5475]: DEBUG: OSD rebalancing completed
2025-02-24 10:50:36 ceph-node-3 mds[6078]: CRITICAL: High I/O latency detected in radosgw service
2025-02-24 10:50:40 ceph-node-4 osd[4871]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:43 ceph-node-2 osd[1390]: INFO: OSD disk space utilization: 70%
2025-02-24 10:50:57 ceph-node-5 mon[2875]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:30 ceph-node-5 mds[9292]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:50:43 ceph-node-5 mds[3599]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:50:57 ceph-node-1 client[3053]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:51:01 ceph-node-2 radosgw[9690]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:57 ceph-node-4 mgr[7774]: INFO: OSD disk space utilization: 70%
2025-02-24 10:50:47 ceph-node-1 mgr[2902]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:11 ceph-node-4 osd[2908]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:51:22 ceph-node-5 radosgw[5610]: NOTICE: OSD rebalancing completed
2025-02-24 10:51:28 ceph-node-3 mon[2522]: DEBUG: OSD rebalancing completed
2025-02-24 10:50:46 ceph-node-2 client[9413]: NOTICE: OSD rebalancing completed
2025-02-24 10:50:37 ceph-node-4 mds[6885]: DEBUG: Monitor map has been updated
2025-02-24 10:51:27 ceph-node-1 radosgw[2493]: INFO: MGR module loaded successfully
2025-02-24 10:50:57 ceph-node-2 osd[1728]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:54 ceph-node-1 osd[4946]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:50:35 ceph-node-4 osd[3561]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:50:53 ceph-node-5 mgr[8221]: DEBUG: Monitor map has been updated
2025-02-24 10:50:51 ceph-node-3 mds[6840]: INFO: OSD disk space utilization: 70%
2025-02-24 10:51:30 ceph-node-2 client[2292]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:50:38 ceph-node-5 mgr[6740]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:39 ceph-node-5 osd[1302]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:50:40 ceph-node-5 mgr[4627]: WARNING: Network partition detected between OSD nodes
2025-02-24 10:51:17 ceph-node-4 mds[5513]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:50:52 ceph-node-4 mgr[8614]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:10 ceph-node-5 radosgw[2449]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:18 ceph-node-5 mon[9529]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:50:55 ceph-node-3 client[1373]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:50:59 ceph-node-4 radosgw[4155]: DEBUG: Monitor map has been updated
2025-02-24 10:50:37 ceph-node-3 client[8974]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:41 ceph-node-4 client[8912]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:22 ceph-node-2 client[5029]: NOTICE: Data replication completed for object pool
2025-02-24 10:50:43 ceph-node-4 mgr[9711]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:31 ceph-node-4 client[3548]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:06 ceph-node-3 mgr[9502]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:13 ceph-node-1 client[7697]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:50:50 ceph-node-4 mgr[4582]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:22 ceph-node-2 mon[8563]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:25 ceph-node-1 radosgw[8179]: WARNING: OSD disk space utilization exceeded 90%
2025-02-24 10:50:49 ceph-node-3 mon[6505]: NOTICE: MGR module loaded successfully
2025-02-24 10:51:28 ceph-node-4 mds[7979]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:51:20 ceph-node-2 mgr[5974]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:18 ceph-node-5 mgr[7301]: NOTICE: Monitor map has been updated
2025-02-24 10:50:39 ceph-node-1 mgr[3536]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:50:51 ceph-node-2 osd[6210]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:50:46 ceph-node-4 mds[7722]: ERROR: Data corruption detected in object pool
2025-02-24 10:50:56 ceph-node-2 mgr[1470]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:43 ceph-node-3 mgr[6477]: CRITICAL: CRITICAL: Data loss detected on pool 1
2025-02-24 10:50:49 ceph-node-5 radosgw[9811]: INFO: Monitor map has been updated
2025-02-24 10:51:25 ceph-node-1 mon[9021]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:50:38 ceph-node-2 mon[7709]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:50 ceph-node-1 mon[5588]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:59 ceph-node-5 mds[4004]: INFO: MGR module loaded successfully
2025-02-24 10:50:56 ceph-node-5 mgr[5287]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:51:09 ceph-node-2 mgr[7828]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:50:32 ceph-node-5 client[4990]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:50:34 ceph-node-4 osd[2595]: NOTICE: Data replication completed for object pool
2025-02-24 10:51:09 ceph-node-1 radosgw[7308]: DEBUG: Data replication completed for object pool
2025-02-24 10:50:41 ceph-node-2 radosgw[1498]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:50:59 ceph-node-1 mds[8184]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:15 ceph-node-2 mds[9343]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:59 ceph-node-1 mds[7058]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:51:25 ceph-node-5 radosgw[9589]: INFO: PG 1.2 replicated successfully
2025-02-24 10:51:08 ceph-node-4 osd[1843]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:41 ceph-node-5 mgr[4764]: DEBUG: Monitor map has been updated
2025-02-24 10:50:49 ceph-node-5 mgr[6537]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:50:44 ceph-node-5 mgr[9049]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:46 ceph-node-4 mds[6950]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:50:42 ceph-node-1 mon[1215]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:17 ceph-node-4 mon[3552]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:47 ceph-node-4 mgr[1902]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:50:52 ceph-node-5 mon[8082]: WARNING: Network partition detected between OSD nodes
2025-02-24 10:50:49 ceph-node-1 radosgw[4020]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:06 ceph-node-3 osd[2344]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:50:42 ceph-node-2 osd[8851]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:51:26 ceph-node-3 mds[1921]: INFO: OSD rebalancing completed
2025-02-24 10:50:47 ceph-node-4 mgr[2272]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:27 ceph-node-1 mgr[9831]: INFO: MGR module loaded successfully
2025-02-24 10:50:59 ceph-node-2 mds[9268]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:51 ceph-node-3 mds[9536]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:32 ceph-node-4 osd[4442]: DEBUG: OSD rebalancing completed
2025-02-24 10:50:47 ceph-node-4 mgr[8853]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:10 ceph-node-3 mgr[1710]: DEBUG: Monitor map has been updated
2025-02-24 10:50:32 ceph-node-1 osd[3405]: NOTICE: Data replication completed for object pool
2025-02-24 10:51:07 ceph-node-1 mgr[1009]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:50:59 ceph-node-3 mds[3838]: INFO: Monitor map has been updated
2025-02-24 10:51:21 ceph-node-5 mds[3865]: DEBUG: Data replication completed for object pool
2025-02-24 10:50:55 ceph-node-5 radosgw[1302]: INFO: OSD disk space utilization: 70%
2025-02-24 10:51:28 ceph-node-1 radosgw[4435]: NOTICE: OSD rebalancing completed
2025-02-24 10:51:04 ceph-node-4 client[1883]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:50:32 ceph-node-1 client[7436]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:51:20 ceph-node-3 mds[8076]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:50:40 ceph-node-1 client[2086]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:54 ceph-node-2 osd[4100]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:50:49 ceph-node-3 mds[9912]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:05 ceph-node-5 radosgw[6112]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:51:03 ceph-node-4 mon[3785]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:17 ceph-node-3 mon[9554]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:06 ceph-node-2 mgr[2486]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:50:34 ceph-node-3 client[5439]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:50:34 ceph-node-4 mgr[1179]: DEBUG: Data replication completed for object pool
2025-02-24 10:50:40 ceph-node-3 mgr[5738]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:50:57 ceph-node-2 client[8095]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:01 ceph-node-4 client[4246]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:51:13 ceph-node-2 mon[3123]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:50:42 ceph-node-3 mds[8430]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:17 ceph-node-2 client[6386]: NOTICE: MGR module loaded successfully
2025-02-24 10:50:40 ceph-node-3 mgr[2506]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:48 ceph-node-3 client[8101]: DEBUG: Monitor map has been updated
2025-02-24 10:50:39 ceph-node-3 mds[3635]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:50:40 ceph-node-1 mds[9754]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:08 ceph-node-5 radosgw[7853]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:50:33 ceph-node-3 mon[2787]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:31 ceph-node-1 radosgw[1479]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:50:40 ceph-node-1 mgr[4970]: DEBUG: OSD rebalancing completed
2025-02-24 10:51:14 ceph-node-2 mon[4576]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:29 ceph-node-4 radosgw[4280]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:50:36 ceph-node-2 mgr[8169]: INFO: OSD rebalancing completed
2025-02-24 10:50:42 ceph-node-3 mds[5948]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:50:44 ceph-node-1 client[1933]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:50:49 ceph-node-2 mon[5054]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:28 ceph-node-3 osd[4642]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:05 ceph-node-3 radosgw[9937]: DEBUG: MGR module loaded successfully
2025-02-24 10:50:53 ceph-node-3 mgr[9280]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:28 ceph-node-1 mgr[1109]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:05 ceph-node-2 mds[6586]: NOTICE: OSD rebalancing completed
2025-02-24 10:50:53 ceph-node-3 client[8034]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:10 ceph-node-2 radosgw[1284]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:08 ceph-node-5 mon[8003]: NOTICE: Monitor map has been updated
2025-02-24 10:50:57 ceph-node-1 mgr[5623]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:14 ceph-node-5 mgr[6393]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:13 ceph-node-5 client[7598]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:51:03 ceph-node-1 radosgw[3273]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:20 ceph-node-1 client[7964]: INFO: OSD rebalancing completed
2025-02-24 10:50:45 ceph-node-2 mgr[1071]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:30 ceph-node-1 client[7485]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:06 ceph-node-3 mon[8048]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:40 ceph-node-1 radosgw[3879]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:31 ceph-node-2 radosgw[6124]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:50:46 ceph-node-3 osd[6637]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:50:47 ceph-node-3 radosgw[4163]: INFO: OSD disk space utilization: 70%
2025-02-24 10:50:47 ceph-node-2 mds[3231]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:50 ceph-node-4 mds[2097]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:51:11 ceph-node-2 mon[2963]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:23 ceph-node-5 client[8768]: WARNING: Monitor mon1 failed to reach quorum
2025-02-24 10:50:49 ceph-node-2 radosgw[9647]: CRITICAL: Disk failure reported on ceph-node-4
2025-02-24 10:50:39 ceph-node-1 osd[2964]: CRITICAL: Uncorrectable ECC error detected in MDS cache
2025-02-24 10:50:59 ceph-node-2 mgr[2557]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:44 ceph-node-4 mgr[4930]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:57 ceph-node-2 client[7547]: INFO: OSD rebalancing completed
2025-02-24 10:51:19 ceph-node-3 radosgw[6646]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:17 ceph-node-2 mgr[5179]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:23 ceph-node-1 radosgw[8361]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:51:07 ceph-node-5 osd[4348]: INFO: Monitor map has been updated
2025-02-24 10:50:54 ceph-node-4 mds[8630]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:51:01 ceph-node-1 osd[4908]: DEBUG: OSD rebalancing completed
2025-02-24 10:50:39 ceph-node-5 mon[1841]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:51:27 ceph-node-3 mgr[7354]: CRITICAL: Authentication failure in RADOS Gateway
2025-02-24 10:51:20 ceph-node-3 client[2595]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:50:50 ceph-node-2 client[4154]: NOTICE: MGR module loaded successfully
2025-02-24 10:51:02 ceph-node-1 radosgw[1237]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:51:17 ceph-node-3 client[4083]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:50:31 ceph-node-1 radosgw[3444]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:51:27 ceph-node-1 osd[8809]: DEBUG: Data replication completed for object pool
2025-02-24 10:50:33 ceph-node-4 client[9206]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:50:57 ceph-node-5 mon[6839]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:50:49 ceph-node-2 radosgw[2219]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:07 ceph-node-3 radosgw[5038]: DEBUG: MGR module loaded successfully
2025-02-24 10:50:45 ceph-node-3 client[8432]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:51:09 ceph-node-3 mon[4032]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:45 ceph-node-4 radosgw[6140]: INFO: OSD disk space utilization: 70%
2025-02-24 10:50:45 ceph-node-5 mds[9103]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:10 ceph-node-2 osd[2035]: INFO: OSD rebalancing completed
2025-02-24 10:50:34 ceph-node-1 client[1563]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:36 ceph-node-5 radosgw[1977]: CRITICAL: Network partition detected between OSD nodes
2025-02-24 10:51:07 ceph-node-2 mon[1974]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:39 ceph-node-5 mds[7766]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:51:17 ceph-node-2 client[4685]: ERROR: Disk failure reported on ceph-node-4
2025-02-24 10:51:31 ceph-node-4 osd[7283]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:56 ceph-node-5 mgr[1664]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:50:33 ceph-node-2 osd[7940]: NOTICE: Monitor map has been updated
2025-02-24 10:51:18 ceph-node-2 client[7234]: INFO: OSD disk space utilization: 70%
2025-02-24 10:51:07 ceph-node-1 mds[8580]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:50:50 ceph-node-4 client[3451]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:27 ceph-node-1 mgr[4287]: WARNING: CRITICAL: Data loss detected on pool 1
2025-02-24 10:50:54 ceph-node-5 radosgw[1598]: ERROR: OSD 3.1 marked down due to heartbeat failure
2025-02-24 10:50:39 ceph-node-1 mgr[2989]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:50:44 ceph-node-2 radosgw[9728]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:09 ceph-node-5 osd[9423]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:34 ceph-node-3 radosgw[4630]: NOTICE: OSD rebalancing completed
2025-02-24 10:50:43 ceph-node-2 radosgw[5630]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:51:06 ceph-node-5 mon[6841]: INFO: OSD disk space utilization: 70%
2025-02-24 10:50:42 ceph-node-3 mgr[8795]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:50:34 ceph-node-2 mds[9965]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:50:57 ceph-node-5 client[2616]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:51:29 ceph-node-2 radosgw[6900]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:04 ceph-node-1 mgr[7242]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:50:37 ceph-node-1 osd[4598]: NOTICE: Monitor map has been updated
2025-02-24 10:50:40 ceph-node-4 mon[3115]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:37 ceph-node-2 client[2876]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:50:54 ceph-node-1 radosgw[1897]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:34 ceph-node-2 mds[5560]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:40 ceph-node-2 osd[9685]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:31 ceph-node-4 osd[3440]: INFO: PG 1.2 replicated successfully
2025-02-24 10:51:12 ceph-node-1 radosgw[8419]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:00 ceph-node-4 mgr[1413]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:50:32 ceph-node-1 radosgw[2807]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:08 ceph-node-4 mon[1468]: NOTICE: Monitor map has been updated
2025-02-24 10:50:57 ceph-node-2 mds[5536]: DEBUG: OSD rebalancing completed
2025-02-24 10:51:11 ceph-node-3 radosgw[5601]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:05 ceph-node-4 mon[1849]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:47 ceph-node-3 radosgw[2111]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:47 ceph-node-3 osd[7380]: ERROR: OSD disk space utilization exceeded 90%
2025-02-24 10:50:51 ceph-node-5 mgr[2507]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:28 ceph-node-1 osd[1200]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:54 ceph-node-3 client[7953]: INFO: Data replication completed for object pool
2025-02-24 10:50:48 ceph-node-3 mon[5256]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:51:24 ceph-node-1 mgr[1687]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:28 ceph-node-1 mds[1666]: INFO: PG 1.2 replicated successfully
2025-02-24 10:50:56 ceph-node-2 osd[7510]: DEBUG: OSD rebalancing completed
2025-02-24 10:51:03 ceph-node-1 osd[1363]: DEBUG: MGR module loaded successfully
2025-02-24 10:50:37 ceph-node-3 client[3984]: INFO: Data replication completed for object pool
2025-02-24 10:51:21 ceph-node-4 osd[5639]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:50:56 ceph-node-3 mds[6106]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:26 ceph-node-2 mds[8221]: NOTICE: Monitor map has been updated
2025-02-24 10:50:51 ceph-node-1 radosgw[9585]: INFO: Data replication completed for object pool
2025-02-24 10:51:01 ceph-node-1 osd[4900]: INFO: MGR module loaded successfully
2025-02-24 10:51:00 ceph-node-4 client[1356]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:19 ceph-node-1 radosgw[2029]: DEBUG: Data replication completed for object pool
2025-02-24 10:50:41 ceph-node-1 osd[8928]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:23 ceph-node-2 mon[5204]: INFO: PG 1.2 replicated successfully
2025-02-24 10:50:35 ceph-node-4 mon[4576]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:47 ceph-node-5 client[2476]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:55 ceph-node-2 radosgw[5229]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:23 ceph-node-3 mon[7354]: INFO: OSD rebalancing completed
2025-02-24 10:50:42 ceph-node-3 mon[3111]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:50:34 ceph-node-4 client[4106]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:50:56 ceph-node-5 radosgw[2272]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:50:39 ceph-node-4 osd[1101]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:34 ceph-node-3 client[2816]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:50:38 ceph-node-3 osd[7373]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:50:44 ceph-node-5 osd[5995]: DEBUG: Data replication completed for object pool
2025-02-24 10:50:58 ceph-node-3 radosgw[6392]: INFO: OSD disk space utilization: 70%
2025-02-24 10:51:12 ceph-node-4 mds[6071]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:43 ceph-node-1 osd[6960]: INFO: Monitor map has been updated
2025-02-24 10:50:37 ceph-node-3 mgr[8079]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:51:27 ceph-node-1 mgr[9948]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:51:18 ceph-node-2 mds[6027]: DEBUG: OSD rebalancing completed
2025-02-24 10:51:01 ceph-node-2 mon[1632]: ERROR: CRITICAL: Data loss detected on pool 1
2025-02-24 10:51:26 ceph-node-5 mon[8784]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:50:34 ceph-node-5 radosgw[5707]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:50:46 ceph-node-5 mgr[7440]: INFO: Data replication completed for object pool
2025-02-24 10:50:41 ceph-node-3 mon[6074]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:50:56 ceph-node-2 mon[1386]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:50:48 ceph-node-4 client[2255]: INFO: OSD rebalancing completed
2025-02-24 10:51:19 ceph-node-3 radosgw[2631]: WARNING: Disk failure reported on ceph-node-4
2025-02-24 10:50:54 ceph-node-1 mgr[8017]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:21 ceph-node-2 radosgw[1152]: INFO: OSD rebalancing completed
2025-02-24 10:51:14 ceph-node-4 radosgw[3997]: WARNING: CRITICAL: Data loss detected on pool 1
2025-02-24 10:50:54 ceph-node-2 osd[9920]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:50:52 ceph-node-3 radosgw[4781]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:51:14 ceph-node-4 client[2154]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:23 ceph-node-4 osd[1713]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:43 ceph-node-1 client[7466]: NOTICE: MGR module loaded successfully
2025-02-24 10:50:46 ceph-node-1 mds[1349]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:09 ceph-node-3 mon[1691]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:50:53 ceph-node-5 client[5988]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:14 ceph-node-5 osd[9314]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:50:37 ceph-node-5 mds[8365]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:29 ceph-node-2 osd[6222]: INFO: OSD disk space utilization: 70%
2025-02-24 10:51:18 ceph-node-2 mon[8384]: ERROR: Slow request detected on OSD 4.3
2025-02-24 10:50:50 ceph-node-4 client[7313]: INFO: OSD disk space utilization: 70%
2025-02-24 10:51:14 ceph-node-5 mon[7132]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:50:32 ceph-node-2 mon[8536]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:27 ceph-node-3 osd[6430]: NOTICE: Monitor map has been updated
2025-02-24 10:51:26 ceph-node-3 client[3781]: ERROR: High I/O latency detected in radosgw service
2025-02-24 10:51:06 ceph-node-1 mgr[3974]: DEBUG: Monitor map has been updated
2025-02-24 10:51:25 ceph-node-3 osd[1859]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:18 ceph-node-5 mgr[2606]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:25 ceph-node-1 mgr[6343]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:19 ceph-node-5 osd[6320]: INFO: MGR module loaded successfully
2025-02-24 10:51:23 ceph-node-4 mon[5626]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:50:54 ceph-node-5 client[1449]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:37 ceph-node-4 client[5479]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:06 ceph-node-2 mgr[7301]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:02 ceph-node-5 mgr[2032]: INFO: Monitor map has been updated
2025-02-24 10:50:38 ceph-node-5 mds[1609]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:51:02 ceph-node-1 client[4729]: INFO: OSD disk space utilization: 70%
2025-02-24 10:50:44 ceph-node-3 osd[3309]: DEBUG: Monitor map has been updated
2025-02-24 10:51:04 ceph-node-5 mgr[5228]: DEBUG: Monitor map has been updated
2025-02-24 10:51:09 ceph-node-5 client[1114]: DEBUG: OSD rebalancing completed
2025-02-24 10:51:09 ceph-node-5 radosgw[8768]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:01 ceph-node-1 client[4898]: INFO: Data replication completed for object pool
2025-02-24 10:50:48 ceph-node-4 mgr[6862]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:32 ceph-node-2 mgr[1328]: NOTICE: MGR module loaded successfully
2025-02-24 10:51:12 ceph-node-4 osd[2954]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:36 ceph-node-1 osd[1414]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:51:03 ceph-node-1 mgr[8485]: DEBUG: Monitor map has been updated
2025-02-24 10:51:24 ceph-node-5 osd[3204]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:50:47 ceph-node-1 client[4942]: INFO: Data replication completed for object pool
2025-02-24 10:50:41 ceph-node-2 radosgw[3611]: DEBUG: MGR module loaded successfully
2025-02-24 10:50:46 ceph-node-1 mds[3071]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:50:47 ceph-node-5 osd[7084]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:40 ceph-node-5 osd[5287]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:46 ceph-node-5 radosgw[7241]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:51:06 ceph-node-4 mgr[1290]: NOTICE: OSD rebalancing completed
2025-02-24 10:50:35 ceph-node-4 radosgw[3630]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:51:15 ceph-node-1 mds[9068]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:28 ceph-node-1 radosgw[6405]: NOTICE: Monitor map has been updated
2025-02-24 10:50:52 ceph-node-4 mds[2113]: INFO: OSD disk space utilization: 70%
2025-02-24 10:51:14 ceph-node-2 client[7036]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:51:16 ceph-node-2 client[5254]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:27 ceph-node-1 mon[4177]: INFO: Data replication completed for object pool
2025-02-24 10:50:39 ceph-node-4 mds[8501]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:50:51 ceph-node-2 mds[3271]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:51:20 ceph-node-4 osd[1773]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:50:42 ceph-node-1 mgr[9524]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:50:37 ceph-node-2 mon[3022]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:18 ceph-node-1 osd[2447]: DEBUG: OSD rebalancing completed
2025-02-24 10:51:21 ceph-node-3 mgr[2178]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:50:42 ceph-node-1 mds[6752]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:07 ceph-node-4 mgr[5373]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:34 ceph-node-1 mds[2525]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:51 ceph-node-5 osd[5151]: NOTICE: OSD rebalancing completed
2025-02-24 10:50:46 ceph-node-3 mds[8709]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:25 ceph-node-2 radosgw[6104]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:19 ceph-node-2 mgr[8296]: DEBUG: OSD rebalancing completed
2025-02-24 10:50:55 ceph-node-3 client[4595]: INFO: Data replication completed for object pool
2025-02-24 10:51:02 ceph-node-5 mgr[4481]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:16 ceph-node-1 mds[8346]: NOTICE: MGR module loaded successfully
2025-02-24 10:51:10 ceph-node-2 client[8793]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:35 ceph-node-4 client[7492]: CRITICAL: CRITICAL: PG 2.1 stuck in inconsistent state
2025-02-24 10:51:25 ceph-node-3 mon[7055]: DEBUG: Monitor map has been updated
2025-02-24 10:51:21 ceph-node-3 radosgw[3456]: INFO: Data replication completed for object pool
2025-02-24 10:51:15 ceph-node-1 mon[3094]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:50:40 ceph-node-2 client[9325]: DEBUG: Monitor map has been updated
2025-02-24 10:50:51 ceph-node-5 client[9650]: ERROR: CRITICAL: Data loss detected on pool 1
2025-02-24 10:51:05 ceph-node-3 mon[8873]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:47 ceph-node-4 mds[6945]: NOTICE: MGR module loaded successfully
2025-02-24 10:51:00 ceph-node-5 osd[7303]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:27 ceph-node-5 mgr[3331]: CRITICAL: CRITICAL: PG 2.1 stuck in inconsistent state
2025-02-24 10:51:13 ceph-node-5 mgr[4728]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:51:07 ceph-node-1 radosgw[2620]: INFO: OSD rebalancing completed
2025-02-24 10:51:25 ceph-node-2 mon[4207]: DEBUG: Data replication completed for object pool
2025-02-24 10:50:59 ceph-node-5 radosgw[1639]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:50:48 ceph-node-5 mgr[8295]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:50:51 ceph-node-2 mon[6916]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:50:40 ceph-node-4 client[9585]: NOTICE: Data replication completed for object pool
2025-02-24 10:51:16 ceph-node-4 client[5425]: NOTICE: MGR module loaded successfully
2025-02-24 10:51:17 ceph-node-2 mgr[5310]: NOTICE: Monitor map has been updated
2025-02-24 10:51:23 ceph-node-1 mon[4210]: INFO: MGR module loaded successfully
2025-02-24 10:50:35 ceph-node-5 mgr[7493]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:54 ceph-node-4 radosgw[5410]: INFO: OSD disk space utilization: 70%
2025-02-24 10:51:22 ceph-node-3 osd[8892]: NOTICE: OSD rebalancing completed
2025-02-24 10:51:18 ceph-node-4 mds[7721]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:15 ceph-node-1 mgr[6006]: DEBUG: Monitor map has been updated
2025-02-24 10:50:57 ceph-node-3 radosgw[9063]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:56 ceph-node-1 mon[1438]: INFO: MGR module loaded successfully
2025-02-24 10:51:03 ceph-node-1 mds[3629]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:50:52 ceph-node-5 radosgw[3156]: INFO: OSD rebalancing completed
2025-02-24 10:50:39 ceph-node-5 radosgw[2199]: DEBUG: OSD rebalancing completed
2025-02-24 10:50:42 ceph-node-5 client[8266]: DEBUG: Monitor map has been updated
2025-02-24 10:51:19 ceph-node-3 radosgw[8331]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:50:51 ceph-node-1 osd[6090]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:50:40 ceph-node-5 mds[8802]: NOTICE: MGR module loaded successfully
2025-02-24 10:50:47 ceph-node-1 mgr[9307]: NOTICE: Monitor map has been updated
2025-02-24 10:51:15 ceph-node-2 osd[9592]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:50:57 ceph-node-3 radosgw[1764]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:27 ceph-node-3 mds[9906]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:51:24 ceph-node-3 osd[8707]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:24 ceph-node-5 osd[4471]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:03 ceph-node-2 mgr[1806]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:51:29 ceph-node-3 osd[8375]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:51:19 ceph-node-3 mon[2934]: CRITICAL: Monitor mon1 failed to reach quorum
2025-02-24 10:50:33 ceph-node-3 radosgw[2397]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:52 ceph-node-1 mds[1951]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:50:41 ceph-node-1 radosgw[1495]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:06 ceph-node-3 client[9276]: INFO: PG 1.2 replicated successfully
2025-02-24 10:51:08 ceph-node-3 mds[9921]: WARNING: Slow request detected on OSD 4.3
2025-02-24 10:51:24 ceph-node-1 mds[7840]: DEBUG: Data replication completed for object pool
2025-02-24 10:50:31 ceph-node-3 mgr[8510]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:34 ceph-node-3 mon[6664]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:50:55 ceph-node-1 mon[7315]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:51:19 ceph-node-3 mds[6442]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:51:05 ceph-node-3 radosgw[4677]: ERROR: CRITICAL: PG 2.1 stuck in inconsistent state
2025-02-24 10:51:09 ceph-node-4 osd[9343]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:04 ceph-node-5 client[8058]: NOTICE: OSD rebalancing completed
2025-02-24 10:51:02 ceph-node-2 client[7860]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:53 ceph-node-4 mon[5458]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:06 ceph-node-2 mgr[6004]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:51:09 ceph-node-3 radosgw[6848]: INFO: Data replication completed for object pool
2025-02-24 10:51:03 ceph-node-3 osd[7249]: ERROR: Cluster status: HEALTH_WARN
2025-02-24 10:50:50 ceph-node-1 osd[9741]: NOTICE: Monitor map has been updated
2025-02-24 10:51:14 ceph-node-5 client[5551]: NOTICE: OSD rebalancing completed
2025-02-24 10:51:05 ceph-node-5 client[5076]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:51:27 ceph-node-1 mds[3807]: NOTICE: OSD rebalancing completed
2025-02-24 10:51:10 ceph-node-5 mon[6197]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:37 ceph-node-1 client[4678]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:19 ceph-node-1 osd[3433]: NOTICE: OSD rebalancing completed
2025-02-24 10:50:38 ceph-node-2 mon[3561]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:50:32 ceph-node-1 client[3561]: ERROR: MDS failure detected: metadata inconsistency
2025-02-24 10:51:31 ceph-node-4 osd[3097]: INFO: MGR module loaded successfully
2025-02-24 10:51:06 ceph-node-3 client[7031]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:51:05 ceph-node-3 osd[5577]: DEBUG: Data replication completed for object pool
2025-02-24 10:50:54 ceph-node-3 client[9362]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:50:33 ceph-node-3 mds[5018]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:47 ceph-node-1 mds[2187]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:12 ceph-node-4 client[3218]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:50:45 ceph-node-3 osd[4919]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:50:47 ceph-node-2 mon[7920]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:49 ceph-node-2 client[8910]: ERROR: Monitor mon1 failed to reach quorum
2025-02-24 10:50:41 ceph-node-5 mgr[5282]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:23 ceph-node-3 mds[5205]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:31 ceph-node-2 mon[1475]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:51:09 ceph-node-2 radosgw[4425]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:02 ceph-node-5 radosgw[6817]: NOTICE: OSD rebalancing completed
2025-02-24 10:50:45 ceph-node-4 mgr[1876]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:30 ceph-node-1 mgr[4183]: NOTICE: Data replication completed for object pool
2025-02-24 10:50:43 ceph-node-2 osd[6062]: ERROR: Network partition detected between OSD nodes
2025-02-24 10:51:10 ceph-node-4 client[3835]: NOTICE: Monitor map has been updated
2025-02-24 10:51:23 ceph-node-3 osd[2737]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:20 ceph-node-5 osd[7422]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:50:52 ceph-node-2 mds[2702]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:50:58 ceph-node-1 mds[5734]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:19 ceph-node-1 osd[7227]: DEBUG: MGR module loaded successfully
2025-02-24 10:50:47 ceph-node-1 client[7875]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:14 ceph-node-2 mon[1083]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:51:24 ceph-node-1 mon[3239]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:50:50 ceph-node-4 osd[6695]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:50:54 ceph-node-2 mds[1508]: DEBUG: Data replication completed for object pool
2025-02-24 10:50:36 ceph-node-2 mds[7342]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:50:45 ceph-node-3 mgr[3658]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:51:05 ceph-node-4 osd[7354]: INFO: MGR module loaded successfully
2025-02-24 10:50:52 ceph-node-1 radosgw[5709]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:51:14 ceph-node-5 client[8988]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:21 ceph-node-1 client[4080]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:50:35 ceph-node-2 radosgw[4525]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:50:38 ceph-node-2 osd[7415]: INFO: MGR module loaded successfully
2025-02-24 10:50:37 ceph-node-5 mgr[6533]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:13 ceph-node-4 mon[7258]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:50:56 ceph-node-3 mon[7397]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:44 ceph-node-1 radosgw[7206]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:43 ceph-node-4 mds[7539]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:51:10 ceph-node-3 mgr[3205]: WARNING: Journal write failure detected on OSD node
2025-02-24 10:50:51 ceph-node-4 mon[3830]: INFO: Data replication completed for object pool
2025-02-24 10:50:55 ceph-node-2 mds[3792]: NOTICE: Monitor map has been updated
2025-02-24 10:51:30 ceph-node-5 osd[6878]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:50:31 ceph-node-4 osd[6768]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:50:54 ceph-node-3 client[5402]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:26 ceph-node-4 mgr[4004]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:11 ceph-node-3 osd[8306]: NOTICE: OSD rebalancing completed
2025-02-24 10:51:28 ceph-node-1 mgr[5289]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:51:27 ceph-node-1 osd[5073]: INFO: Monitor map has been updated
2025-02-24 10:50:43 ceph-node-2 mgr[5735]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:51:11 ceph-node-5 osd[3935]: NOTICE: MGR module loaded successfully
2025-02-24 10:50:42 ceph-node-2 mon[2201]: NOTICE: MGR module loaded successfully
2025-02-24 10:50:54 ceph-node-5 radosgw[1079]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:50:40 ceph-node-1 client[6951]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:19 ceph-node-5 mgr[8059]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:25 ceph-node-1 mds[2318]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:10 ceph-node-4 mon[2659]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:26 ceph-node-4 mgr[8066]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:25 ceph-node-5 mon[8999]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:51:22 ceph-node-2 radosgw[9992]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:51:19 ceph-node-5 mds[1306]: INFO: PG 1.2 replicated successfully
2025-02-24 10:51:19 ceph-node-3 radosgw[6274]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:13 ceph-node-2 mon[9067]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:50:55 ceph-node-5 mon[1360]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:08 ceph-node-5 client[8922]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:50:45 ceph-node-5 mon[7142]: ERROR: Network partition detected between OSD nodes
2025-02-24 10:50:50 ceph-node-3 osd[3817]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:50:58 ceph-node-4 mgr[1588]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:03 ceph-node-4 radosgw[9515]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:50:45 ceph-node-5 client[3998]: CRITICAL: CRITICAL: PG 2.1 stuck in inconsistent state
2025-02-24 10:50:47 ceph-node-3 osd[7183]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:51:31 ceph-node-4 mgr[6837]: WARNING: Cluster status: HEALTH_WARN
2025-02-24 10:51:18 ceph-node-5 mds[6326]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:08 ceph-node-5 client[3254]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:04 ceph-node-5 osd[2106]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:51:14 ceph-node-1 radosgw[7023]: CRITICAL: Uncorrectable ECC error detected in MDS cache
2025-02-24 10:50:39 ceph-node-3 mgr[4158]: CRITICAL: OSD disk space utilization exceeded 90%
2025-02-24 10:51:05 ceph-node-1 osd[7438]: WARNING: Monitor down, unable to connect to quorum
2025-02-24 10:51:06 ceph-node-3 osd[5858]: DEBUG: Data replication completed for object pool
2025-02-24 10:50:59 ceph-node-5 radosgw[6987]: DEBUG: Data replication completed for object pool
2025-02-24 10:50:32 ceph-node-4 osd[9809]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:50:53 ceph-node-2 osd[7776]: INFO: PG 1.2 replicated successfully
2025-02-24 10:50:44 ceph-node-3 client[2148]: DEBUG: Data replication completed for object pool
2025-02-24 10:50:44 ceph-node-5 mds[2838]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:51:01 ceph-node-3 osd[1539]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:56 ceph-node-5 osd[1973]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:27 ceph-node-1 mon[2031]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:50:45 ceph-node-2 osd[7433]: NOTICE: Monitor map has been updated
2025-02-24 10:51:22 ceph-node-3 mds[2616]: NOTICE: MGR module loaded successfully
2025-02-24 10:50:42 ceph-node-5 mds[8686]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:50:47 ceph-node-2 mds[3402]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:29 ceph-node-3 radosgw[7673]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:50:42 ceph-node-5 osd[1589]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:05 ceph-node-4 osd[9068]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:14 ceph-node-2 mon[5967]: CRITICAL: Client connection timeout detected
2025-02-24 10:50:49 ceph-node-3 mon[7915]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:47 ceph-node-1 mds[5471]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:50:34 ceph-node-5 mgr[6129]: INFO: MGR module loaded successfully
2025-02-24 10:51:05 ceph-node-2 mon[6016]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:50:50 ceph-node-1 radosgw[8676]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:26 ceph-node-5 mgr[5412]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:13 ceph-node-1 osd[7436]: INFO: OSD rebalancing completed
2025-02-24 10:51:18 ceph-node-3 radosgw[4958]: NOTICE: OSD rebalancing completed
2025-02-24 10:50:45 ceph-node-5 mon[8008]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:51:13 ceph-node-1 osd[9175]: WARNING: Authentication failure in RADOS Gateway
2025-02-24 10:51:30 ceph-node-5 client[4945]: WARNING: OSD disk space utilization exceeded 90%
2025-02-24 10:51:26 ceph-node-5 osd[4562]: INFO: OSD disk space utilization: 70%
2025-02-24 10:51:13 ceph-node-1 mon[3739]: ERROR: High I/O latency detected in radosgw service
2025-02-24 10:50:57 ceph-node-5 radosgw[6159]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:06 ceph-node-1 osd[3217]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:51:08 ceph-node-1 osd[3891]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:50:54 ceph-node-3 mgr[7385]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:45 ceph-node-1 radosgw[4238]: WARNING: Uncorrectable ECC error detected in MDS cache
2025-02-24 10:50:57 ceph-node-3 mon[8993]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:10 ceph-node-1 osd[2984]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:42 ceph-node-2 mon[5617]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:50:53 ceph-node-1 radosgw[3124]: CRITICAL: MDS failure detected: metadata inconsistency
2025-02-24 10:51:05 ceph-node-5 client[6758]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:51:02 ceph-node-1 mds[7675]: WARNING: OSD disk space utilization exceeded 90%
2025-02-24 10:51:00 ceph-node-2 mgr[1215]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:51:18 ceph-node-2 client[3136]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:50:32 ceph-node-3 radosgw[1591]: INFO: Data replication completed for object pool
2025-02-24 10:51:20 ceph-node-5 client[9124]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:21 ceph-node-5 mon[8733]: ERROR: OSD 3.1 marked down due to heartbeat failure
2025-02-24 10:50:53 ceph-node-2 mgr[8728]: DEBUG: MGR module loaded successfully
2025-02-24 10:50:51 ceph-node-4 radosgw[1970]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:31 ceph-node-2 radosgw[7052]: WARNING: Client connection timeout detected
2025-02-24 10:50:36 ceph-node-2 mon[1613]: INFO: OSD rebalancing completed
2025-02-24 10:50:40 ceph-node-2 osd[5718]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:35 ceph-node-2 mon[6865]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:50:49 ceph-node-5 mds[6665]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:51:29 ceph-node-1 radosgw[3338]: INFO: OSD disk space utilization: 70%
2025-02-24 10:51:26 ceph-node-4 osd[3046]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:11 ceph-node-3 radosgw[8305]: INFO: OSD disk space utilization: 70%
2025-02-24 10:51:14 ceph-node-5 client[9797]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:51:08 ceph-node-3 client[7797]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:23 ceph-node-3 mds[3483]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:16 ceph-node-1 osd[8954]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:50:36 ceph-node-5 mon[5611]: WARNING: Uncorrectable ECC error detected in MDS cache
2025-02-24 10:51:27 ceph-node-2 radosgw[8332]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:50:58 ceph-node-5 client[8323]: CRITICAL: Cluster status: HEALTH_WARN
2025-02-24 10:50:48 ceph-node-1 client[7152]: INFO: MGR module loaded successfully
2025-02-24 10:51:31 ceph-node-3 mds[4564]: NOTICE: MGR module loaded successfully
2025-02-24 10:50:45 ceph-node-3 client[5658]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:11 ceph-node-4 client[4736]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:51:29 ceph-node-1 mgr[3908]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:50:33 ceph-node-1 osd[3816]: INFO: MGR module loaded successfully
2025-02-24 10:51:02 ceph-node-1 mds[9501]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:51:18 ceph-node-5 osd[8468]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:51:07 ceph-node-4 mon[9824]: NOTICE: Monitor map has been updated
2025-02-24 10:50:39 ceph-node-3 mds[8763]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:51:31 ceph-node-1 mds[7622]: NOTICE: Data replication completed for object pool
2025-02-24 10:51:14 ceph-node-1 radosgw[3104]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:22 ceph-node-4 mds[8225]: CRITICAL: Uncorrectable ECC error detected in MDS cache
2025-02-24 10:50:43 ceph-node-5 radosgw[2093]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:29 ceph-node-4 mgr[1492]: INFO: OSD disk space utilization: 70%
2025-02-24 10:50:40 ceph-node-3 mds[2761]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:09 ceph-node-3 radosgw[8584]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:51:18 ceph-node-1 radosgw[6408]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:51:08 ceph-node-2 mgr[6270]: INFO: MGR module loaded successfully
2025-02-24 10:50:58 ceph-node-4 mon[2108]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:51:20 ceph-node-3 client[8352]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:50:55 ceph-node-4 osd[1476]: INFO: OSD rebalancing completed
2025-02-24 10:50:45 ceph-node-1 mds[7549]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:31 ceph-node-2 client[4237]: NOTICE: OSD rebalancing completed
2025-02-24 10:50:49 ceph-node-1 client[1500]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:50:49 ceph-node-4 mon[5247]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:51:04 ceph-node-5 osd[5703]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:25 ceph-node-1 osd[3709]: WARNING: Disk failure reported on ceph-node-4
2025-02-24 10:51:19 ceph-node-3 osd[7978]: NOTICE: OSD rebalancing completed
2025-02-24 10:50:36 ceph-node-1 mds[6341]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:51:02 ceph-node-3 osd[3637]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:50 ceph-node-5 mgr[5632]: WARNING: OSD 3.1 marked down due to heartbeat failure
2025-02-24 10:50:38 ceph-node-2 mgr[9660]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:50:49 ceph-node-4 client[3235]: DEBUG: OSD rebalancing completed
2025-02-24 10:51:23 ceph-node-4 mgr[4854]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:51:07 ceph-node-5 radosgw[4328]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:55 ceph-node-4 mds[7037]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:51:23 ceph-node-4 radosgw[4668]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:50:35 ceph-node-4 osd[3077]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:51:17 ceph-node-1 osd[8471]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:03 ceph-node-1 mds[3199]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:27 ceph-node-2 mon[7591]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:51 ceph-node-2 mds[9373]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:06 ceph-node-4 radosgw[4218]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:52 ceph-node-1 osd[3557]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:12 ceph-node-4 mds[3969]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:10 ceph-node-5 mon[2820]: INFO: MGR module loaded successfully
2025-02-24 10:50:32 ceph-node-4 radosgw[9575]: DEBUG: OSD rebalancing completed
2025-02-24 10:50:37 ceph-node-1 radosgw[7473]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:51:21 ceph-node-5 mgr[9931]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:50:59 ceph-node-4 mgr[7879]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:28 ceph-node-3 client[2719]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:19 ceph-node-5 mon[6271]: WARNING: Cluster status: HEALTH_WARN
2025-02-24 10:51:29 ceph-node-3 radosgw[3187]: CRITICAL: Disk failure reported on ceph-node-4
2025-02-24 10:50:44 ceph-node-2 mgr[6416]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:51:18 ceph-node-1 mon[2359]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:11 ceph-node-1 mon[7854]: DEBUG: Monitor map has been updated
2025-02-24 10:51:29 ceph-node-1 client[4582]: DEBUG: Data replication completed for object pool
2025-02-24 10:50:54 ceph-node-4 osd[7340]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:50:46 ceph-node-3 radosgw[6611]: INFO: Data replication completed for object pool
2025-02-24 10:50:32 ceph-node-3 osd[9094]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:51:20 ceph-node-1 osd[4759]: INFO: Data replication completed for object pool
2025-02-24 10:50:38 ceph-node-1 client[5538]: INFO: Monitor map has been updated
2025-02-24 10:51:16 ceph-node-5 client[1223]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:50:59 ceph-node-5 mgr[4006]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:51:28 ceph-node-4 osd[3301]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:50:33 ceph-node-1 mon[8278]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:50:55 ceph-node-2 radosgw[2029]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:31 ceph-node-3 osd[7139]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:51:13 ceph-node-5 mds[4759]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:35 ceph-node-4 osd[3640]: DEBUG: MGR module loaded successfully
2025-02-24 10:50:49 ceph-node-5 mgr[2832]: NOTICE: Monitor map has been updated
2025-02-24 10:51:26 ceph-node-2 radosgw[7931]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:29 ceph-node-3 mgr[8566]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:51:20 ceph-node-1 mon[4588]: CRITICAL: CRITICAL: PG 2.1 stuck in inconsistent state
2025-02-24 10:51:28 ceph-node-2 mon[5717]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:50:35 ceph-node-3 mon[6205]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:51:07 ceph-node-5 mon[6963]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:15 ceph-node-2 mds[1221]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:51:30 ceph-node-2 osd[4351]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:48 ceph-node-5 radosgw[4365]: ERROR: OSD disk space utilization exceeded 90%
2025-02-24 10:51:16 ceph-node-5 mgr[3386]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:50:31 ceph-node-3 mds[8903]: NOTICE: OSD rebalancing completed
2025-02-24 10:51:29 ceph-node-1 mon[9510]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:50:45 ceph-node-1 mds[3124]: NOTICE: Monitor map has been updated
2025-02-24 10:51:02 ceph-node-5 osd[5935]: INFO: PG 1.2 replicated successfully
2025-02-24 10:50:50 ceph-node-4 mds[1532]: CRITICAL: CRITICAL: Data loss detected on pool 1
2025-02-24 10:50:31 ceph-node-5 radosgw[5441]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:09 ceph-node-3 radosgw[8860]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:51:18 ceph-node-3 mds[8510]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:50:48 ceph-node-1 client[9628]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:51:07 ceph-node-5 client[7378]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:08 ceph-node-1 osd[3478]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:50:32 ceph-node-1 client[5303]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:51:05 ceph-node-4 mon[7552]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:50:37 ceph-node-3 radosgw[7379]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:15 ceph-node-3 radosgw[2271]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:40 ceph-node-5 mgr[4775]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:51:31 ceph-node-1 client[1038]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:51:31 ceph-node-4 osd[1557]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:22 ceph-node-3 osd[6120]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:27 ceph-node-1 mgr[5722]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:39 ceph-node-1 client[1157]: INFO: OSD disk space utilization: 70%
2025-02-24 10:50:50 ceph-node-3 osd[1332]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:20 ceph-node-1 mon[9300]: INFO: OSD rebalancing completed
2025-02-24 10:51:11 ceph-node-1 radosgw[5889]: NOTICE: MGR module loaded successfully
2025-02-24 10:51:08 ceph-node-1 client[8083]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:50:48 ceph-node-1 radosgw[2750]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:51:21 ceph-node-4 mgr[8215]: NOTICE: OSD rebalancing completed
2025-02-24 10:50:41 ceph-node-3 osd[1715]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:50:32 ceph-node-4 mgr[7949]: DEBUG: MGR module loaded successfully
2025-02-24 10:50:37 ceph-node-5 client[4803]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:50:49 ceph-node-3 radosgw[6125]: DEBUG: Data replication completed for object pool
2025-02-24 10:50:33 ceph-node-4 radosgw[6240]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:51:06 ceph-node-1 client[9744]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:50:54 ceph-node-2 mon[9652]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:50:41 ceph-node-1 mgr[9318]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:00 ceph-node-1 client[4231]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:50:36 ceph-node-2 mon[5757]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:51:07 ceph-node-4 osd[6858]: NOTICE: Data replication completed for object pool
2025-02-24 10:50:52 ceph-node-1 radosgw[2593]: DEBUG: Monitor map has been updated
2025-02-24 10:50:44 ceph-node-2 mds[6453]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:36 ceph-node-2 mon[5660]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:06 ceph-node-2 mds[3954]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:13 ceph-node-5 mgr[8896]: INFO: Monitor map has been updated
2025-02-24 10:50:45 ceph-node-2 radosgw[3749]: INFO: MGR module loaded successfully
2025-02-24 10:50:50 ceph-node-1 osd[1142]: DEBUG: Monitor map has been updated
2025-02-24 10:51:02 ceph-node-4 client[5837]: INFO: PG 1.2 replicated successfully
2025-02-24 10:50:31 ceph-node-5 mds[8934]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:50:33 ceph-node-4 mgr[4345]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:05 ceph-node-5 radosgw[6360]: INFO: OSD rebalancing completed
2025-02-24 10:51:09 ceph-node-5 radosgw[3610]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:03 ceph-node-2 client[3171]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:45 ceph-node-2 radosgw[5364]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:25 ceph-node-1 osd[9813]: NOTICE: Data replication completed for object pool
2025-02-24 10:51:05 ceph-node-3 mds[3253]: NOTICE: Data replication completed for object pool
2025-02-24 10:51:20 ceph-node-3 mon[3628]: DEBUG: Monitor map has been updated
2025-02-24 10:51:10 ceph-node-5 osd[1415]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:51:26 ceph-node-4 client[7312]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:50:34 ceph-node-2 mon[5701]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:39 ceph-node-4 mgr[7940]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:50:43 ceph-node-1 mds[8880]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:50:42 ceph-node-1 radosgw[9526]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:45 ceph-node-5 mds[3314]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:11 ceph-node-1 osd[5113]: ERROR: Slow request detected on OSD 4.3
2025-02-24 10:50:38 ceph-node-5 radosgw[8518]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:03 ceph-node-4 client[6612]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:50:34 ceph-node-1 osd[9698]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:51:25 ceph-node-5 client[7133]: CRITICAL: Uncorrectable ECC error detected in MDS cache
2025-02-24 10:50:58 ceph-node-5 osd[6064]: NOTICE: Data replication completed for object pool
2025-02-24 10:50:46 ceph-node-3 mon[4314]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:24 ceph-node-5 mon[9674]: NOTICE: Monitor map has been updated
2025-02-24 10:50:31 ceph-node-1 mon[5930]: DEBUG: Monitor map has been updated
2025-02-24 10:50:54 ceph-node-1 mds[6421]: INFO: Monitor map has been updated
2025-02-24 10:50:51 ceph-node-1 osd[8720]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:55 ceph-node-2 client[5942]: INFO: PG 1.2 replicated successfully
2025-02-24 10:51:00 ceph-node-3 mon[5217]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:50:44 ceph-node-3 mgr[2564]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:08 ceph-node-2 mon[8245]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:53 ceph-node-1 radosgw[7307]: ERROR: Monitor down, unable to connect to quorum
2025-02-24 10:50:33 ceph-node-4 mon[1622]: CRITICAL: Disk failure reported on ceph-node-4
2025-02-24 10:50:43 ceph-node-2 osd[3250]: DEBUG: Data replication completed for object pool
2025-02-24 10:50:50 ceph-node-5 mon[5823]: ERROR: MDS failure detected: metadata inconsistency
2025-02-24 10:50:45 ceph-node-4 mds[1122]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:51:12 ceph-node-2 osd[6648]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:50:41 ceph-node-2 mon[7436]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:16 ceph-node-3 osd[3552]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:51:13 ceph-node-1 client[3268]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:11 ceph-node-4 client[9215]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:51:04 ceph-node-4 osd[1506]: DEBUG: OSD rebalancing completed
2025-02-24 10:50:44 ceph-node-2 mgr[5776]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:45 ceph-node-4 mgr[5285]: INFO: Data replication completed for object pool
2025-02-24 10:51:17 ceph-node-5 osd[4734]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:51:07 ceph-node-3 mon[4804]: INFO: PG 1.2 replicated successfully
2025-02-24 10:51:06 ceph-node-4 mgr[4822]: ERROR: Network partition detected between OSD nodes
2025-02-24 10:51:23 ceph-node-3 client[4286]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:35 ceph-node-3 mds[8376]: DEBUG: Data replication completed for object pool
2025-02-24 10:50:42 ceph-node-3 mon[2163]: CRITICAL: Monitor mon1 failed to reach quorum
2025-02-24 10:50:55 ceph-node-2 client[7487]: NOTICE: MGR module loaded successfully
2025-02-24 10:51:23 ceph-node-2 radosgw[8051]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:57 ceph-node-2 client[6774]: NOTICE: OSD rebalancing completed
2025-02-24 10:50:48 ceph-node-3 mds[9415]: NOTICE: MGR module loaded successfully
2025-02-24 10:51:27 ceph-node-2 osd[2541]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:24 ceph-node-5 mds[2453]: INFO: OSD disk space utilization: 70%
2025-02-24 10:50:37 ceph-node-1 client[3529]: DEBUG: Monitor map has been updated
2025-02-24 10:51:12 ceph-node-5 mgr[6079]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:15 ceph-node-5 client[6667]: NOTICE: OSD rebalancing completed
2025-02-24 10:51:11 ceph-node-3 mgr[8570]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:51:09 ceph-node-1 osd[9156]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:51:29 ceph-node-5 osd[7229]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:17 ceph-node-4 mgr[2652]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:51:15 ceph-node-1 mgr[3125]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:44 ceph-node-2 mds[6693]: DEBUG: Monitor map has been updated
2025-02-24 10:51:31 ceph-node-5 mds[3934]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:10 ceph-node-4 osd[8443]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:50:33 ceph-node-3 osd[3234]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:51:26 ceph-node-2 client[1600]: INFO: PG 1.2 replicated successfully
2025-02-24 10:50:40 ceph-node-3 osd[3523]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:51:11 ceph-node-2 radosgw[8034]: DEBUG: Monitor map has been updated
2025-02-24 10:51:10 ceph-node-1 radosgw[6453]: INFO: Monitor map has been updated
2025-02-24 10:50:35 ceph-node-3 mon[8736]: CRITICAL: CRITICAL: PG 2.1 stuck in inconsistent state
2025-02-24 10:51:05 ceph-node-4 osd[2687]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:18 ceph-node-2 radosgw[1987]: INFO: PG 1.2 replicated successfully
2025-02-24 10:50:41 ceph-node-3 mds[7630]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:21 ceph-node-1 client[5175]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:24 ceph-node-3 radosgw[7856]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:50:44 ceph-node-2 mgr[8802]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:51:03 ceph-node-1 radosgw[4179]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:50:46 ceph-node-3 client[4049]: INFO: OSD disk space utilization: 70%
2025-02-24 10:51:07 ceph-node-1 mon[1049]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:50:35 ceph-node-1 osd[3520]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:50:51 ceph-node-3 mgr[8811]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:51:15 ceph-node-1 mds[8827]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:39 ceph-node-5 osd[7298]: NOTICE: Monitor map has been updated
2025-02-24 10:51:24 ceph-node-2 mgr[4084]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:50:50 ceph-node-4 mgr[8636]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:50:47 ceph-node-1 mon[1506]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:39 ceph-node-4 osd[3372]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:50:37 ceph-node-1 osd[7227]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:50:42 ceph-node-3 osd[3935]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:06 ceph-node-5 radosgw[2046]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:51:01 ceph-node-1 mgr[2800]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:50:48 ceph-node-1 mds[8458]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:50:44 ceph-node-1 mds[1513]: NOTICE: Data replication completed for object pool
2025-02-24 10:50:41 ceph-node-4 client[7342]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:51:05 ceph-node-3 mon[2925]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:51:01 ceph-node-3 client[2605]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:51:02 ceph-node-3 mon[8550]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:20 ceph-node-3 client[3043]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:50:48 ceph-node-5 client[8563]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:20 ceph-node-5 client[4792]: INFO: Monitor map has been updated
2025-02-24 10:51:15 ceph-node-4 mds[5830]: ERROR: Disk failure reported on ceph-node-4
2025-02-24 10:50:59 ceph-node-4 client[7035]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:51 ceph-node-5 mon[7490]: NOTICE: OSD rebalancing completed
2025-02-24 10:50:42 ceph-node-4 mgr[7554]: WARNING: Data corruption detected in object pool
2025-02-24 10:51:08 ceph-node-3 client[3724]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:42 ceph-node-3 mds[3828]: NOTICE: OSD rebalancing completed
2025-02-24 10:51:23 ceph-node-3 mon[4726]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:50:36 ceph-node-4 osd[8017]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:50:44 ceph-node-5 mds[8593]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:08 ceph-node-1 mgr[8760]: WARNING: OSD disk space utilization exceeded 90%
2025-02-24 10:51:20 ceph-node-3 mgr[2578]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:50:46 ceph-node-1 client[3651]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:51:21 ceph-node-4 osd[4932]: INFO: MGR module loaded successfully
2025-02-24 10:51:08 ceph-node-3 client[6357]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:51:04 ceph-node-5 radosgw[1922]: INFO: PG 1.2 replicated successfully
2025-02-24 10:50:50 ceph-node-3 osd[5693]: INFO: OSD rebalancing completed
2025-02-24 10:50:51 ceph-node-1 mds[5810]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:39 ceph-node-4 mon[3641]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:18 ceph-node-5 osd[2383]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:50:54 ceph-node-3 client[5450]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:50:53 ceph-node-5 mds[1940]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:07 ceph-node-1 mon[8889]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:51:10 ceph-node-3 mds[8848]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:50 ceph-node-1 osd[7993]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:03 ceph-node-3 radosgw[7418]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:50:36 ceph-node-1 mgr[5127]: NOTICE: Monitor map has been updated
2025-02-24 10:50:41 ceph-node-5 mon[1399]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:51:18 ceph-node-4 mon[1278]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:52 ceph-node-5 client[4301]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:33 ceph-node-5 mds[2889]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:57 ceph-node-2 radosgw[6218]: NOTICE: Data replication completed for object pool
2025-02-24 10:51:28 ceph-node-3 mgr[3397]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:51 ceph-node-1 osd[1283]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:51:11 ceph-node-4 radosgw[7298]: NOTICE: MGR module loaded successfully
2025-02-24 10:50:50 ceph-node-4 mgr[8844]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:30 ceph-node-3 client[6806]: INFO: MGR module loaded successfully
2025-02-24 10:51:30 ceph-node-1 mds[3797]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:31 ceph-node-2 radosgw[7172]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:11 ceph-node-4 mds[2222]: NOTICE: Monitor map has been updated
2025-02-24 10:50:34 ceph-node-1 client[7478]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:51:19 ceph-node-3 mds[1050]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:06 ceph-node-3 osd[9298]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:51:03 ceph-node-5 client[5439]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:33 ceph-node-1 client[9341]: DEBUG: Monitor map has been updated
2025-02-24 10:50:41 ceph-node-5 osd[4808]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:51:24 ceph-node-2 mgr[9330]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:51:30 ceph-node-3 client[9122]: NOTICE: Data replication completed for object pool
2025-02-24 10:50:37 ceph-node-4 osd[3646]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:50:53 ceph-node-4 mon[3686]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:13 ceph-node-3 client[7936]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:00 ceph-node-1 mon[7538]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:50:42 ceph-node-3 mon[4751]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:23 ceph-node-2 osd[6820]: NOTICE: Monitor map has been updated
2025-02-24 10:50:58 ceph-node-1 mon[8540]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:28 ceph-node-2 mds[2237]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:51:22 ceph-node-3 osd[4356]: NOTICE: Data replication completed for object pool
2025-02-24 10:51:23 ceph-node-3 client[9772]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:38 ceph-node-4 osd[9590]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:23 ceph-node-4 radosgw[9930]: NOTICE: Monitor map has been updated
2025-02-24 10:50:31 ceph-node-1 osd[8735]: INFO: Monitor map has been updated
2025-02-24 10:51:06 ceph-node-2 mon[8417]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:08 ceph-node-3 mon[6166]: NOTICE: MGR module loaded successfully
2025-02-24 10:50:59 ceph-node-5 mon[9911]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:18 ceph-node-3 mds[1347]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:29 ceph-node-1 osd[6235]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:51:31 ceph-node-2 mds[9770]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:51:30 ceph-node-3 mds[9271]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:50:43 ceph-node-5 radosgw[3022]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:16 ceph-node-1 mgr[6917]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:39 ceph-node-5 mds[6730]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:50:51 ceph-node-3 mds[1655]: ERROR: Monitor mon1 failed to reach quorum
2025-02-24 10:50:51 ceph-node-5 radosgw[9357]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:51:02 ceph-node-1 mon[6733]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:21 ceph-node-5 mgr[3113]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:51:10 ceph-node-3 mgr[7042]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:01 ceph-node-3 osd[5702]: INFO: Monitor map has been updated
2025-02-24 10:51:28 ceph-node-2 osd[9236]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:50:37 ceph-node-3 client[5979]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:50:34 ceph-node-3 client[1447]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:23 ceph-node-4 mgr[5758]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:50:41 ceph-node-2 osd[1944]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:51 ceph-node-4 client[8783]: INFO: PG 1.2 replicated successfully
2025-02-24 10:51:21 ceph-node-2 mgr[4138]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:36 ceph-node-4 mon[7794]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:51:31 ceph-node-1 mds[2561]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:50:45 ceph-node-2 osd[1184]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:11 ceph-node-5 osd[1866]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:28 ceph-node-3 client[1827]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:18 ceph-node-5 mds[8660]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:34 ceph-node-2 client[4761]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:18 ceph-node-2 client[6678]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:51:11 ceph-node-1 client[4075]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:50 ceph-node-1 mon[3165]: INFO: OSD rebalancing completed
2025-02-24 10:50:39 ceph-node-5 mds[8063]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:51:20 ceph-node-4 radosgw[5795]: DEBUG: OSD rebalancing completed
2025-02-24 10:50:32 ceph-node-5 radosgw[8308]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:11 ceph-node-5 mgr[2894]: DEBUG: Monitor map has been updated
2025-02-24 10:50:41 ceph-node-3 mon[3009]: INFO: Monitor map has been updated
2025-02-24 10:51:16 ceph-node-3 radosgw[3446]: NOTICE: OSD rebalancing completed
2025-02-24 10:51:17 ceph-node-4 osd[8921]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:51:29 ceph-node-2 radosgw[4758]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:30 ceph-node-3 client[6766]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:18 ceph-node-2 mgr[9212]: INFO: MGR module loaded successfully
2025-02-24 10:50:40 ceph-node-4 mgr[4380]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:51:09 ceph-node-3 mgr[4527]: INFO: OSD disk space utilization: 70%
2025-02-24 10:51:12 ceph-node-4 osd[8552]: ERROR: Cluster status: HEALTH_WARN
2025-02-24 10:50:48 ceph-node-3 mds[8833]: DEBUG: Data replication completed for object pool
2025-02-24 10:50:32 ceph-node-5 mon[5718]: INFO: OSD rebalancing completed
2025-02-24 10:51:18 ceph-node-4 radosgw[2440]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:50:50 ceph-node-1 mds[4723]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:47 ceph-node-2 mds[1465]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:18 ceph-node-3 mds[9574]: NOTICE: Monitor map has been updated
2025-02-24 10:50:40 ceph-node-5 radosgw[1034]: INFO: MGR module loaded successfully
2025-02-24 10:50:39 ceph-node-2 mds[5130]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:50:42 ceph-node-4 osd[2400]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:50:32 ceph-node-2 radosgw[6561]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:51 ceph-node-4 mgr[2968]: NOTICE: Data replication completed for object pool
2025-02-24 10:51:22 ceph-node-1 osd[6998]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:50:37 ceph-node-3 mon[5403]: INFO: OSD disk space utilization: 70%
2025-02-24 10:51:26 ceph-node-2 mds[1813]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:50:42 ceph-node-2 mgr[9051]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:50:35 ceph-node-5 mgr[7505]: INFO: Data replication completed for object pool
2025-02-24 10:51:17 ceph-node-3 client[2917]: INFO: OSD rebalancing completed
2025-02-24 10:51:29 ceph-node-4 mds[3774]: INFO: OSD disk space utilization: 70%
2025-02-24 10:51:01 ceph-node-4 mon[3161]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:33 ceph-node-3 radosgw[3623]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:51:25 ceph-node-1 osd[1045]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:27 ceph-node-5 mgr[6291]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:50:43 ceph-node-1 client[5358]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:49 ceph-node-5 radosgw[7164]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:00 ceph-node-2 mon[7896]: ERROR: MDS failure detected: metadata inconsistency
2025-02-24 10:50:48 ceph-node-1 osd[9520]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:01 ceph-node-3 mds[7802]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:51:16 ceph-node-5 mgr[9032]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:50:45 ceph-node-4 mds[1750]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:31 ceph-node-1 osd[2232]: NOTICE: Monitor map has been updated
2025-02-24 10:50:42 ceph-node-5 osd[4790]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:00 ceph-node-3 mds[6986]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:30 ceph-node-4 radosgw[1042]: WARNING: Monitor down, unable to connect to quorum
2025-02-24 10:51:12 ceph-node-5 mds[6999]: ERROR: High I/O latency detected in radosgw service
2025-02-24 10:51:25 ceph-node-5 mon[6770]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:51:02 ceph-node-5 client[5538]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:33 ceph-node-5 osd[7247]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:59 ceph-node-3 mds[2536]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:51:19 ceph-node-4 osd[3315]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:47 ceph-node-2 mon[7963]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:51:26 ceph-node-2 osd[1832]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:57 ceph-node-1 radosgw[9522]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:56 ceph-node-2 client[4075]: NOTICE: MGR module loaded successfully
2025-02-24 10:51:16 ceph-node-3 client[2785]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:24 ceph-node-5 client[2433]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:50:53 ceph-node-2 osd[1703]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:57 ceph-node-2 osd[2005]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:06 ceph-node-2 client[8222]: INFO: Monitor map has been updated
2025-02-24 10:51:16 ceph-node-4 osd[5529]: INFO: Data replication completed for object pool
2025-02-24 10:50:32 ceph-node-4 radosgw[6569]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:50:33 ceph-node-2 osd[3311]: CRITICAL: OSD disk space utilization exceeded 90%
2025-02-24 10:50:33 ceph-node-2 osd[4110]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:50:39 ceph-node-1 client[2830]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:51:23 ceph-node-1 mds[1878]: INFO: MGR module loaded successfully
2025-02-24 10:51:27 ceph-node-3 osd[1103]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:10 ceph-node-2 mgr[2069]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:51:11 ceph-node-4 radosgw[6249]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:50:35 ceph-node-3 mon[3725]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:16 ceph-node-4 mgr[3006]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:39 ceph-node-3 mds[8784]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:50:56 ceph-node-1 mgr[2897]: WARNING: Cluster status: HEALTH_WARN
2025-02-24 10:50:46 ceph-node-5 mds[9421]: INFO: Data replication completed for object pool
2025-02-24 10:51:31 ceph-node-5 mon[5351]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:51:10 ceph-node-4 mgr[5442]: INFO: OSD rebalancing completed
2025-02-24 10:50:57 ceph-node-5 client[5152]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:47 ceph-node-3 osd[7466]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:50:38 ceph-node-1 client[8300]: WARNING: Network partition detected between OSD nodes
2025-02-24 10:50:57 ceph-node-3 client[6022]: CRITICAL: High I/O latency detected in radosgw service
2025-02-24 10:50:55 ceph-node-5 osd[4306]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:51:26 ceph-node-4 mgr[4628]: NOTICE: MGR module loaded successfully
2025-02-24 10:51:25 ceph-node-4 mgr[4265]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:23 ceph-node-1 mds[2687]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:27 ceph-node-2 client[7565]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:51:11 ceph-node-1 mgr[6633]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:25 ceph-node-4 client[8300]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:50:48 ceph-node-1 mon[2662]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:30 ceph-node-1 mon[9089]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:50:55 ceph-node-4 mon[9886]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:50:35 ceph-node-2 mds[8242]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:51:00 ceph-node-3 mds[3935]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:20 ceph-node-1 osd[4307]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:50:31 ceph-node-1 mds[7621]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:51:06 ceph-node-4 osd[6671]: NOTICE: Data replication completed for object pool
2025-02-24 10:50:40 ceph-node-1 mgr[3406]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:01 ceph-node-1 mon[4314]: NOTICE: Monitor map has been updated
2025-02-24 10:50:36 ceph-node-3 radosgw[5314]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:40 ceph-node-4 client[3052]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:51:07 ceph-node-4 radosgw[3337]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:50:36 ceph-node-1 mds[6601]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:50:40 ceph-node-1 mgr[1613]: NOTICE: Monitor map has been updated
2025-02-24 10:50:40 ceph-node-5 mgr[9033]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:51:04 ceph-node-1 mon[6955]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:24 ceph-node-1 mds[7310]: CRITICAL: Data corruption detected in object pool
2025-02-24 10:50:56 ceph-node-2 mon[3179]: WARNING: High I/O latency detected in radosgw service
2025-02-24 10:50:49 ceph-node-5 mds[8592]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:19 ceph-node-5 radosgw[8063]: NOTICE: Data replication completed for object pool
2025-02-24 10:51:01 ceph-node-1 mgr[6400]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:50:50 ceph-node-2 client[9788]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:50:58 ceph-node-5 radosgw[3822]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:50 ceph-node-5 osd[8822]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:50:52 ceph-node-5 client[2032]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:51:22 ceph-node-2 osd[2736]: CRITICAL: MDS failure detected: metadata inconsistency
2025-02-24 10:50:56 ceph-node-3 radosgw[6769]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:51:28 ceph-node-4 radosgw[4290]: ERROR: OSD 3.1 marked down due to heartbeat failure
2025-02-24 10:50:49 ceph-node-3 osd[1439]: DEBUG: OSD rebalancing completed
2025-02-24 10:50:50 ceph-node-2 mgr[7147]: DEBUG: Monitor map has been updated
2025-02-24 10:51:25 ceph-node-3 mgr[6695]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:51:22 ceph-node-1 client[3050]: INFO: Monitor map has been updated
2025-02-24 10:50:48 ceph-node-1 mds[3675]: INFO: OSD rebalancing completed
2025-02-24 10:51:26 ceph-node-2 radosgw[9296]: NOTICE: MGR module loaded successfully
2025-02-24 10:50:53 ceph-node-5 radosgw[5106]: INFO: OSD rebalancing completed
2025-02-24 10:51:21 ceph-node-2 mon[9796]: NOTICE: Monitor map has been updated
2025-02-24 10:51:23 ceph-node-4 client[5534]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:10 ceph-node-4 client[4944]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:48 ceph-node-3 mgr[7596]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:04 ceph-node-2 mgr[4491]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:21 ceph-node-1 mon[5529]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:24 ceph-node-4 mon[1343]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:50:36 ceph-node-4 osd[2631]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:50:35 ceph-node-2 mds[4765]: INFO: Monitor map has been updated
2025-02-24 10:50:44 ceph-node-1 osd[8709]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:51:16 ceph-node-2 mgr[8527]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:50:45 ceph-node-5 mgr[1050]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:51:07 ceph-node-1 osd[1895]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:11 ceph-node-4 mgr[5838]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:50:33 ceph-node-5 mds[6831]: INFO: Monitor map has been updated
2025-02-24 10:51:30 ceph-node-5 osd[8555]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:42 ceph-node-1 mds[3731]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:16 ceph-node-4 osd[1795]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:24 ceph-node-4 mgr[7195]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:50:51 ceph-node-1 osd[2868]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:51:15 ceph-node-4 osd[2834]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:58 ceph-node-4 client[9070]: CRITICAL: Data corruption detected in object pool
2025-02-24 10:50:38 ceph-node-2 mgr[3121]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:10 ceph-node-4 mon[6442]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:00 ceph-node-1 osd[1150]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:20 ceph-node-4 mon[3717]: CRITICAL: Client connection timeout detected
2025-02-24 10:51:14 ceph-node-3 mds[9857]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:29 ceph-node-1 radosgw[6001]: INFO: OSD rebalancing completed
2025-02-24 10:51:09 ceph-node-1 mgr[1327]: WARNING: Data corruption detected in object pool
2025-02-24 10:51:29 ceph-node-3 client[1963]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:38 ceph-node-5 mon[4437]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:51:02 ceph-node-3 radosgw[7735]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:34 ceph-node-5 client[7870]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:05 ceph-node-4 mgr[3884]: INFO: PG 1.2 replicated successfully
2025-02-24 10:51:16 ceph-node-1 radosgw[9333]: ERROR: OSD disk space utilization exceeded 90%
2025-02-24 10:51:25 ceph-node-1 osd[6431]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:52 ceph-node-4 osd[8493]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:50:56 ceph-node-3 client[3158]: INFO: OSD rebalancing completed
2025-02-24 10:50:33 ceph-node-5 mon[9285]: DEBUG: MGR module loaded successfully
2025-02-24 10:50:35 ceph-node-1 client[9512]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:50:58 ceph-node-1 client[5907]: DEBUG: MGR module loaded successfully
2025-02-24 10:50:47 ceph-node-3 mon[7779]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:50:39 ceph-node-2 mon[3331]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:51:02 ceph-node-3 mgr[9634]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:13 ceph-node-3 osd[3448]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:50:58 ceph-node-4 client[1646]: DEBUG: Monitor map has been updated
2025-02-24 10:50:45 ceph-node-2 mgr[6857]: INFO: Data replication completed for object pool
2025-02-24 10:50:50 ceph-node-4 mon[4909]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:58 ceph-node-2 osd[1963]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:09 ceph-node-3 mgr[3093]: NOTICE: OSD rebalancing completed
2025-02-24 10:50:33 ceph-node-5 mon[2194]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:26 ceph-node-4 mon[3408]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:50:50 ceph-node-2 osd[3695]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:11 ceph-node-1 client[6213]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:51:01 ceph-node-4 mon[4115]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:35 ceph-node-3 osd[1820]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:51:02 ceph-node-2 mgr[3539]: INFO: Monitor map has been updated
2025-02-24 10:50:53 ceph-node-1 mon[1187]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:18 ceph-node-3 mgr[9292]: DEBUG: Monitor map has been updated
2025-02-24 10:50:54 ceph-node-4 mon[9821]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:04 ceph-node-3 client[3059]: NOTICE: OSD rebalancing completed
2025-02-24 10:50:58 ceph-node-5 mgr[9706]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:20 ceph-node-4 mon[4487]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:01 ceph-node-5 osd[4422]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:25 ceph-node-1 mgr[5951]: NOTICE: Data replication completed for object pool
2025-02-24 10:50:36 ceph-node-4 mgr[7504]: INFO: Data replication completed for object pool
2025-02-24 10:51:31 ceph-node-1 mds[1782]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:29 ceph-node-2 client[5489]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:50:50 ceph-node-3 mon[5625]: WARNING: Journal write failure detected on OSD node
2025-02-24 10:50:53 ceph-node-4 mds[1592]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:50:48 ceph-node-2 client[5114]: WARNING: CRITICAL: PG 2.1 stuck in inconsistent state
2025-02-24 10:51:17 ceph-node-5 radosgw[1445]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:14 ceph-node-2 mds[9101]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:29 ceph-node-5 osd[8374]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:50:33 ceph-node-3 osd[5539]: DEBUG: Data replication completed for object pool
2025-02-24 10:50:44 ceph-node-3 osd[8556]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:51:20 ceph-node-3 mgr[2030]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:50:52 ceph-node-5 osd[5365]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:46 ceph-node-3 mds[6121]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:16 ceph-node-5 client[3218]: DEBUG: Monitor map has been updated
2025-02-24 10:51:00 ceph-node-5 osd[3650]: CRITICAL: Slow request detected on OSD 4.3
2025-02-24 10:51:03 ceph-node-1 client[2137]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:51:07 ceph-node-3 client[5226]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:50:52 ceph-node-2 mon[8520]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:25 ceph-node-3 mgr[7981]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:51:22 ceph-node-4 mon[2343]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:08 ceph-node-3 mon[9257]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:19 ceph-node-2 osd[3938]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:50:47 ceph-node-5 mon[8534]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:51:07 ceph-node-3 client[7883]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:48 ceph-node-3 client[8296]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:51:11 ceph-node-3 mds[9933]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:10 ceph-node-2 client[2972]: INFO: PG 1.2 replicated successfully
2025-02-24 10:50:56 ceph-node-5 osd[5821]: DEBUG: OSD rebalancing completed
2025-02-24 10:50:35 ceph-node-5 mgr[4110]: DEBUG: OSD rebalancing completed
2025-02-24 10:50:52 ceph-node-4 mgr[3520]: INFO: Monitor map has been updated
2025-02-24 10:50:31 ceph-node-1 mgr[8762]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:50 ceph-node-1 osd[3327]: INFO: Monitor map has been updated
2025-02-24 10:50:41 ceph-node-2 client[7325]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:48 ceph-node-1 osd[6234]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:51:30 ceph-node-3 mon[2957]: NOTICE: Data replication completed for object pool
2025-02-24 10:50:39 ceph-node-5 mon[6380]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:39 ceph-node-2 mon[2886]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:30 ceph-node-1 mds[3288]: DEBUG: Data replication completed for object pool
2025-02-24 10:50:47 ceph-node-2 client[1632]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:50:50 ceph-node-2 client[3444]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:50:40 ceph-node-1 mds[5141]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:17 ceph-node-1 osd[4275]: INFO: OSD rebalancing completed
2025-02-24 10:51:06 ceph-node-3 mds[9105]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:50:38 ceph-node-1 client[4497]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:39 ceph-node-1 radosgw[9561]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:18 ceph-node-3 client[3647]: WARNING: CRITICAL: Data loss detected on pool 1
2025-02-24 10:51:17 ceph-node-3 osd[9160]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:51:19 ceph-node-2 client[1524]: DEBUG: Monitor map has been updated
2025-02-24 10:50:52 ceph-node-4 osd[3677]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:09 ceph-node-3 mon[6119]: DEBUG: Monitor map has been updated
2025-02-24 10:51:24 ceph-node-5 mon[6339]: NOTICE: MGR module loaded successfully
2025-02-24 10:50:44 ceph-node-3 client[5955]: CRITICAL: Authentication failure in RADOS Gateway
2025-02-24 10:51:09 ceph-node-1 mon[6855]: INFO: OSD disk space utilization: 70%
2025-02-24 10:50:46 ceph-node-3 radosgw[1157]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:51:18 ceph-node-3 mgr[8551]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:51:16 ceph-node-4 osd[5410]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:33 ceph-node-4 mgr[6362]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:51:23 ceph-node-5 osd[8313]: CRITICAL: MDS failure detected: metadata inconsistency
2025-02-24 10:51:24 ceph-node-5 radosgw[5786]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:09 ceph-node-4 mds[5403]: NOTICE: OSD rebalancing completed
2025-02-24 10:51:28 ceph-node-1 osd[5239]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:22 ceph-node-2 mds[2726]: WARNING: Data corruption detected in object pool
2025-02-24 10:50:45 ceph-node-3 client[5362]: INFO: MGR module loaded successfully
2025-02-24 10:51:25 ceph-node-3 mgr[4113]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:50:57 ceph-node-4 client[9782]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:52 ceph-node-3 mds[1965]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:22 ceph-node-1 radosgw[2940]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:12 ceph-node-5 mgr[2536]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:17 ceph-node-4 radosgw[1717]: INFO: PG 1.2 replicated successfully
2025-02-24 10:50:34 ceph-node-5 osd[3973]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:51:17 ceph-node-5 mgr[6381]: NOTICE: Data replication completed for object pool
2025-02-24 10:50:31 ceph-node-1 osd[8806]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:12 ceph-node-1 mon[6646]: INFO: Data replication completed for object pool
2025-02-24 10:51:23 ceph-node-5 mds[7878]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:59 ceph-node-5 osd[1256]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:27 ceph-node-4 radosgw[9533]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:14 ceph-node-1 radosgw[7650]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:50:48 ceph-node-5 osd[1454]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:48 ceph-node-1 client[4623]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:43 ceph-node-2 client[4906]: INFO: OSD disk space utilization: 70%
2025-02-24 10:51:06 ceph-node-3 client[5875]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:09 ceph-node-5 mgr[7797]: WARNING: Client connection timeout detected
2025-02-24 10:51:29 ceph-node-3 osd[9908]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:50:34 ceph-node-5 osd[1161]: NOTICE: MGR module loaded successfully
2025-02-24 10:50:38 ceph-node-5 mgr[3748]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:10 ceph-node-2 radosgw[4664]: INFO: OSD rebalancing completed
2025-02-24 10:50:50 ceph-node-1 mds[5088]: NOTICE: Data replication completed for object pool
2025-02-24 10:51:25 ceph-node-4 mds[8275]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:50:50 ceph-node-2 osd[6734]: CRITICAL: High I/O latency detected in radosgw service
2025-02-24 10:51:03 ceph-node-3 radosgw[9354]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:50:50 ceph-node-4 radosgw[2479]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:56 ceph-node-2 osd[5447]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:50:49 ceph-node-2 radosgw[2127]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:50:56 ceph-node-3 mon[1441]: INFO: PG 1.2 replicated successfully
2025-02-24 10:51:28 ceph-node-5 radosgw[5400]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:50:54 ceph-node-4 osd[7966]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:34 ceph-node-3 osd[9260]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:14 ceph-node-4 osd[1047]: ERROR: OSD 3.1 marked down due to heartbeat failure
2025-02-24 10:51:03 ceph-node-1 mon[2972]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:10 ceph-node-2 mds[1510]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:50:57 ceph-node-2 mgr[8335]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:50:59 ceph-node-3 client[4294]: INFO: Data replication completed for object pool
2025-02-24 10:50:32 ceph-node-5 client[2828]: CRITICAL: Slow request detected on OSD 4.3
2025-02-24 10:50:31 ceph-node-4 client[4809]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:50:55 ceph-node-5 mgr[5961]: DEBUG: OSD rebalancing completed
2025-02-24 10:51:16 ceph-node-4 mds[7256]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:37 ceph-node-4 mgr[1782]: ERROR: Uncorrectable ECC error detected in MDS cache
2025-02-24 10:50:52 ceph-node-2 mon[1826]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:50:34 ceph-node-1 client[7101]: INFO: OSD disk space utilization: 70%
2025-02-24 10:50:34 ceph-node-5 radosgw[2750]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:08 ceph-node-1 osd[5094]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:10 ceph-node-1 mds[4554]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:51:25 ceph-node-1 mds[6486]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:50:37 ceph-node-4 mds[7047]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:15 ceph-node-4 mon[2457]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:51:22 ceph-node-5 mgr[4587]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:27 ceph-node-5 mgr[4724]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:50:56 ceph-node-1 mds[9100]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:23 ceph-node-3 mon[7150]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:52 ceph-node-5 client[3787]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:58 ceph-node-4 osd[1970]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:00 ceph-node-2 osd[8516]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:51:00 ceph-node-4 client[6951]: ERROR: Network partition detected between OSD nodes
2025-02-24 10:50:48 ceph-node-3 mon[6545]: INFO: Monitor map has been updated
2025-02-24 10:50:32 ceph-node-4 mgr[9704]: NOTICE: Monitor map has been updated
2025-02-24 10:51:03 ceph-node-5 mgr[6533]: INFO: Data replication completed for object pool
2025-02-24 10:51:08 ceph-node-1 mgr[1348]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:50:49 ceph-node-4 mon[5306]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:19 ceph-node-4 osd[9577]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:51:18 ceph-node-1 mds[7604]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:27 ceph-node-3 osd[8127]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:50:46 ceph-node-2 osd[1257]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:51:02 ceph-node-3 radosgw[1186]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:51:20 ceph-node-2 osd[7064]: DEBUG: Monitor map has been updated
2025-02-24 10:51:23 ceph-node-1 mon[4189]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:50:49 ceph-node-2 radosgw[8525]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:50:50 ceph-node-1 osd[9049]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:56 ceph-node-1 client[8162]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:50:47 ceph-node-5 mds[5128]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:20 ceph-node-2 mgr[9390]: INFO: MGR module loaded successfully
2025-02-24 10:50:38 ceph-node-3 osd[1545]: NOTICE: OSD rebalancing completed
2025-02-24 10:51:07 ceph-node-5 client[7677]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:50:39 ceph-node-5 osd[3190]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:51:11 ceph-node-3 mon[2141]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:50:50 ceph-node-3 osd[8180]: NOTICE: Data replication completed for object pool
2025-02-24 10:50:46 ceph-node-1 mon[3602]: INFO: OSD disk space utilization: 70%
2025-02-24 10:50:38 ceph-node-4 osd[3891]: CRITICAL: OSD disk space utilization exceeded 90%
2025-02-24 10:50:41 ceph-node-1 radosgw[2959]: NOTICE: Monitor map has been updated
2025-02-24 10:51:22 ceph-node-3 radosgw[2310]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:45 ceph-node-2 osd[2946]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:35 ceph-node-4 osd[9249]: DEBUG: OSD rebalancing completed
2025-02-24 10:51:21 ceph-node-3 radosgw[5870]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:57 ceph-node-1 mon[5983]: NOTICE: MGR module loaded successfully
2025-02-24 10:50:56 ceph-node-2 radosgw[7407]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:07 ceph-node-2 radosgw[7275]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:51:04 ceph-node-3 osd[3393]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:50:34 ceph-node-3 radosgw[2151]: NOTICE: MGR module loaded successfully
2025-02-24 10:51:16 ceph-node-1 client[1167]: DEBUG: OSD rebalancing completed
2025-02-24 10:50:48 ceph-node-4 client[9611]: INFO: PG 1.2 replicated successfully
2025-02-24 10:51:12 ceph-node-1 mds[3700]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:51:31 ceph-node-2 radosgw[7294]: NOTICE: MGR module loaded successfully
2025-02-24 10:51:19 ceph-node-2 radosgw[6418]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:27 ceph-node-5 mon[4677]: INFO: OSD rebalancing completed
2025-02-24 10:50:46 ceph-node-2 osd[4341]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:51:21 ceph-node-1 mds[5615]: DEBUG: Monitor map has been updated
2025-02-24 10:51:01 ceph-node-5 radosgw[3183]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:51:20 ceph-node-2 radosgw[8123]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:51:09 ceph-node-1 mgr[4498]: INFO: PG 1.2 replicated successfully
2025-02-24 10:50:31 ceph-node-4 mds[5556]: WARNING: Cluster status: HEALTH_WARN
2025-02-24 10:50:57 ceph-node-1 mds[4826]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:18 ceph-node-3 client[1077]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:50:55 ceph-node-5 mon[7946]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:30 ceph-node-1 mds[9173]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:24 ceph-node-3 mon[5922]: INFO: OSD rebalancing completed
2025-02-24 10:51:30 ceph-node-5 mds[2483]: INFO: Monitor map has been updated
2025-02-24 10:50:39 ceph-node-5 radosgw[2058]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:26 ceph-node-3 mds[7140]: INFO: Data replication completed for object pool
2025-02-24 10:50:40 ceph-node-5 mon[8707]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:06 ceph-node-4 radosgw[1341]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:51:20 ceph-node-5 radosgw[8865]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:02 ceph-node-4 osd[5547]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:50:35 ceph-node-5 radosgw[3411]: INFO: Data replication completed for object pool
2025-02-24 10:50:40 ceph-node-3 mds[4701]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:50:57 ceph-node-5 mgr[9892]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:28 ceph-node-1 radosgw[6758]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:51:23 ceph-node-4 client[7188]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:39 ceph-node-4 client[5815]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:50:43 ceph-node-4 client[1673]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:51 ceph-node-5 mds[5664]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:50:43 ceph-node-2 mgr[1744]: CRITICAL: CRITICAL: Data loss detected on pool 1
2025-02-24 10:50:59 ceph-node-5 mds[3948]: WARNING: Uncorrectable ECC error detected in MDS cache
2025-02-24 10:51:29 ceph-node-4 mds[3628]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:48 ceph-node-3 mds[5330]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:51 ceph-node-3 mgr[5574]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:52 ceph-node-1 osd[5397]: ERROR: Network partition detected between OSD nodes
2025-02-24 10:50:51 ceph-node-4 mds[1039]: ERROR: CRITICAL: Data loss detected on pool 1
2025-02-24 10:50:31 ceph-node-1 radosgw[6675]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:51:20 ceph-node-1 mds[7067]: WARNING: Uncorrectable ECC error detected in MDS cache
2025-02-24 10:50:56 ceph-node-5 mds[2552]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:50:55 ceph-node-5 osd[5373]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:37 ceph-node-2 radosgw[3178]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:50:56 ceph-node-3 osd[4155]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:51:26 ceph-node-1 mon[7612]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:45 ceph-node-4 client[9419]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:51:21 ceph-node-3 mgr[4587]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:17 ceph-node-3 radosgw[8646]: INFO: Monitor map has been updated
2025-02-24 10:50:41 ceph-node-3 radosgw[1994]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:51:06 ceph-node-3 mgr[4862]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:51:19 ceph-node-4 osd[8816]: ERROR: OSD 3.1 marked down due to heartbeat failure
2025-02-24 10:50:46 ceph-node-4 client[6698]: INFO: Data replication completed for object pool
2025-02-24 10:51:04 ceph-node-1 mds[7094]: INFO: OSD disk space utilization: 70%
2025-02-24 10:50:42 ceph-node-5 client[2781]: WARNING: Client connection timeout detected
2025-02-24 10:50:37 ceph-node-4 mon[2232]: INFO: MGR module loaded successfully
2025-02-24 10:51:26 ceph-node-2 mon[3029]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:50:41 ceph-node-1 mgr[7858]: INFO: MGR module loaded successfully
2025-02-24 10:51:27 ceph-node-1 radosgw[6759]: DEBUG: OSD rebalancing completed
2025-02-24 10:50:34 ceph-node-4 osd[6880]: INFO: OSD rebalancing completed
2025-02-24 10:51:04 ceph-node-1 mgr[6612]: NOTICE: Monitor map has been updated
2025-02-24 10:51:02 ceph-node-2 mgr[6179]: NOTICE: Data replication completed for object pool
2025-02-24 10:51:06 ceph-node-3 radosgw[3466]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:51:23 ceph-node-4 mds[3240]: INFO: MGR module loaded successfully
2025-02-24 10:51:20 ceph-node-1 mds[3599]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:33 ceph-node-2 client[4617]: DEBUG: Monitor map has been updated
2025-02-24 10:51:22 ceph-node-3 radosgw[8857]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:22 ceph-node-3 osd[4218]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:14 ceph-node-2 mgr[4785]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:21 ceph-node-4 mds[7390]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:50:54 ceph-node-4 osd[4026]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:28 ceph-node-1 mds[1089]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:51:13 ceph-node-5 client[8851]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:51:16 ceph-node-2 radosgw[6336]: INFO: Monitor map has been updated
2025-02-24 10:51:23 ceph-node-4 client[6883]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:50:58 ceph-node-3 client[1611]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:23 ceph-node-4 radosgw[7971]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:00 ceph-node-4 osd[7000]: DEBUG: Monitor map has been updated
2025-02-24 10:50:33 ceph-node-2 radosgw[8942]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:50:51 ceph-node-3 mds[9566]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:00 ceph-node-2 osd[3622]: INFO: OSD rebalancing completed
2025-02-24 10:51:12 ceph-node-3 radosgw[7930]: DEBUG: MGR module loaded successfully
2025-02-24 10:50:48 ceph-node-3 radosgw[4888]: INFO: PG 1.2 replicated successfully
2025-02-24 10:50:46 ceph-node-2 osd[8219]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:51:07 ceph-node-3 mds[2590]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:37 ceph-node-1 client[9784]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:30 ceph-node-4 client[2907]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:50 ceph-node-5 osd[7523]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:45 ceph-node-1 radosgw[1506]: WARNING: MDS failure detected: metadata inconsistency
2025-02-24 10:51:04 ceph-node-1 client[1842]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:50:47 ceph-node-1 mgr[4117]: INFO: Monitor map has been updated
2025-02-24 10:50:47 ceph-node-3 mds[8527]: DEBUG: Monitor map has been updated
2025-02-24 10:51:19 ceph-node-4 mgr[9946]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:14 ceph-node-5 client[3527]: NOTICE: Data replication completed for object pool
2025-02-24 10:50:34 ceph-node-2 osd[1388]: INFO: PG 1.2 replicated successfully
2025-02-24 10:51:15 ceph-node-2 mds[8028]: NOTICE: Data replication completed for object pool
2025-02-24 10:50:35 ceph-node-3 mon[7326]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:02 ceph-node-3 mgr[5660]: INFO: MGR module loaded successfully
2025-02-24 10:51:07 ceph-node-1 osd[2418]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:00 ceph-node-2 client[4383]: CRITICAL: Data corruption detected in object pool
2025-02-24 10:50:33 ceph-node-4 client[9042]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:51:03 ceph-node-1 client[3057]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:12 ceph-node-1 mgr[3278]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:05 ceph-node-4 osd[9006]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:11 ceph-node-4 mgr[1186]: NOTICE: MGR module loaded successfully
2025-02-24 10:51:14 ceph-node-5 mds[5871]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:48 ceph-node-1 mon[7007]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:28 ceph-node-5 osd[5737]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:23 ceph-node-1 radosgw[4303]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:01 ceph-node-2 mgr[2730]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:51:04 ceph-node-5 mon[3230]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:51:28 ceph-node-2 radosgw[1799]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:50:51 ceph-node-4 mds[7108]: CRITICAL: Slow request detected on OSD 4.3
2025-02-24 10:51:14 ceph-node-1 mon[6277]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:50:54 ceph-node-2 mds[4955]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:43 ceph-node-5 mds[3510]: ERROR: Monitor mon1 failed to reach quorum
2025-02-24 10:51:02 ceph-node-2 mds[7371]: INFO: Monitor map has been updated
2025-02-24 10:51:15 ceph-node-1 mds[5090]: ERROR: Monitor down, unable to connect to quorum
2025-02-24 10:51:05 ceph-node-3 radosgw[6337]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:51:30 ceph-node-1 mon[4322]: WARNING: Network partition detected between OSD nodes
2025-02-24 10:51:17 ceph-node-5 osd[3626]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:50:37 ceph-node-5 client[3395]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:30 ceph-node-2 mon[9485]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:51:24 ceph-node-2 mon[3010]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:18 ceph-node-4 radosgw[6681]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:44 ceph-node-4 mon[4077]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:51:28 ceph-node-4 mgr[1412]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:20 ceph-node-1 mgr[1449]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:51 ceph-node-5 mds[3388]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:05 ceph-node-5 mon[2118]: INFO: PG 1.2 replicated successfully
2025-02-24 10:50:49 ceph-node-3 client[1877]: INFO: OSD disk space utilization: 70%
2025-02-24 10:50:33 ceph-node-5 mds[2305]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:51:00 ceph-node-4 client[9934]: DEBUG: OSD rebalancing completed
2025-02-24 10:51:05 ceph-node-4 mon[1635]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:50:35 ceph-node-5 mds[3178]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:50:53 ceph-node-5 mon[7065]: DEBUG: Data replication completed for object pool
2025-02-24 10:50:54 ceph-node-5 mgr[6519]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:50:44 ceph-node-5 mds[2381]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:50:40 ceph-node-3 mon[9053]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:49 ceph-node-1 osd[7851]: DEBUG: MGR module loaded successfully
2025-02-24 10:50:54 ceph-node-5 mds[4859]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:22 ceph-node-1 mon[9348]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:09 ceph-node-3 mds[9246]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:35 ceph-node-4 mgr[3863]: NOTICE: Monitor map has been updated
2025-02-24 10:50:35 ceph-node-5 radosgw[2658]: INFO: OSD rebalancing completed
2025-02-24 10:50:52 ceph-node-5 mon[4072]: ERROR: Authentication failure in RADOS Gateway
2025-02-24 10:50:54 ceph-node-2 mgr[1612]: DEBUG: OSD rebalancing completed
2025-02-24 10:51:07 ceph-node-3 mon[2350]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:51:03 ceph-node-4 mgr[9686]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:50:58 ceph-node-3 client[7326]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:51:05 ceph-node-1 client[2479]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:50:40 ceph-node-1 radosgw[4100]: DEBUG: Monitor map has been updated
2025-02-24 10:50:31 ceph-node-3 mon[9580]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:04 ceph-node-2 mgr[8541]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:50:41 ceph-node-3 mds[2898]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:07 ceph-node-2 osd[9605]: ERROR: High I/O latency detected in radosgw service
2025-02-24 10:50:55 ceph-node-5 client[8734]: CRITICAL: Authentication failure in RADOS Gateway
2025-02-24 10:50:44 ceph-node-4 client[2811]: WARNING: Uncorrectable ECC error detected in MDS cache
2025-02-24 10:51:14 ceph-node-4 client[2923]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:26 ceph-node-4 mds[8253]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:50:40 ceph-node-5 osd[7742]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:51:13 ceph-node-3 mds[1641]: INFO: MGR module loaded successfully
2025-02-24 10:50:48 ceph-node-2 mgr[5609]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:51:10 ceph-node-5 mon[7094]: ERROR: OSD 3.1 marked down due to heartbeat failure
2025-02-24 10:51:11 ceph-node-2 osd[5833]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:03 ceph-node-5 radosgw[4391]: INFO: PG 1.2 replicated successfully
2025-02-24 10:50:36 ceph-node-4 osd[6118]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:57 ceph-node-2 mon[6154]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:29 ceph-node-5 mds[2830]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:51:15 ceph-node-2 client[3331]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:12 ceph-node-2 osd[5663]: INFO: PG 1.2 replicated successfully
2025-02-24 10:50:47 ceph-node-2 mon[8915]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:11 ceph-node-1 mon[4177]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:00 ceph-node-5 mds[4239]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:55 ceph-node-1 mds[9041]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:51:28 ceph-node-4 client[3121]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:50:55 ceph-node-5 osd[5061]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:26 ceph-node-2 mgr[4507]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:51:03 ceph-node-4 client[9232]: DEBUG: Monitor map has been updated
2025-02-24 10:51:07 ceph-node-3 mgr[4295]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:50:57 ceph-node-5 mgr[2332]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:50:43 ceph-node-1 radosgw[6881]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:50:52 ceph-node-4 client[9941]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:07 ceph-node-2 osd[2363]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:04 ceph-node-1 radosgw[6349]: INFO: Monitor map has been updated
2025-02-24 10:50:56 ceph-node-2 client[8805]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:50:51 ceph-node-4 mgr[7747]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:51 ceph-node-5 client[7333]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:15 ceph-node-3 osd[5252]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:51:04 ceph-node-1 mds[7798]: INFO: Monitor map has been updated
2025-02-24 10:51:26 ceph-node-5 mon[2959]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:26 ceph-node-4 mds[6666]: NOTICE: OSD rebalancing completed
2025-02-24 10:51:27 ceph-node-1 mds[3171]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:13 ceph-node-4 mds[3313]: CRITICAL: CRITICAL: Data loss detected on pool 1
2025-02-24 10:51:11 ceph-node-5 osd[6910]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:50:41 ceph-node-4 client[6576]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:50:59 ceph-node-3 mon[1532]: ERROR: MDS failure detected: metadata inconsistency
2025-02-24 10:51:28 ceph-node-1 mds[1746]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:24 ceph-node-1 mon[9044]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:30 ceph-node-1 osd[2011]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:24 ceph-node-1 mon[1452]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:00 ceph-node-2 client[2182]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:40 ceph-node-4 mon[4075]: NOTICE: MGR module loaded successfully
2025-02-24 10:50:55 ceph-node-1 radosgw[2638]: NOTICE: Monitor map has been updated
2025-02-24 10:51:30 ceph-node-3 radosgw[3178]: WARNING: Monitor mon1 failed to reach quorum
2025-02-24 10:50:52 ceph-node-3 radosgw[2068]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:51:06 ceph-node-4 mgr[6137]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:51:12 ceph-node-4 mds[1318]: CRITICAL: Disk failure reported on ceph-node-4
2025-02-24 10:50:45 ceph-node-2 client[8307]: DEBUG: Monitor map has been updated
2025-02-24 10:51:23 ceph-node-3 mon[3847]: DEBUG: Data replication completed for object pool
2025-02-24 10:50:48 ceph-node-5 mds[3289]: INFO: OSD disk space utilization: 70%
2025-02-24 10:50:53 ceph-node-1 radosgw[7281]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:05 ceph-node-1 client[6169]: WARNING: CRITICAL: Data loss detected on pool 1
2025-02-24 10:50:44 ceph-node-2 client[2864]: NOTICE: Data replication completed for object pool
2025-02-24 10:51:22 ceph-node-4 osd[9432]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:50:52 ceph-node-2 radosgw[8983]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:50:55 ceph-node-2 mon[1423]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:57 ceph-node-3 mon[4198]: DEBUG: MGR module loaded successfully
2025-02-24 10:50:35 ceph-node-3 mds[4507]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:29 ceph-node-4 client[4205]: WARNING: Slow request detected on OSD 4.3
2025-02-24 10:51:11 ceph-node-2 radosgw[9434]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:51:28 ceph-node-2 mgr[7979]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:32 ceph-node-1 client[7284]: NOTICE: Data replication completed for object pool
2025-02-24 10:51:11 ceph-node-4 mon[6969]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:50:43 ceph-node-4 mgr[5759]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:50:32 ceph-node-1 mgr[1613]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:50:53 ceph-node-5 client[2528]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:16 ceph-node-1 osd[7045]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:51:25 ceph-node-5 osd[4701]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:48 ceph-node-4 mon[4745]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:27 ceph-node-5 radosgw[8626]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:24 ceph-node-5 radosgw[7415]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:50:32 ceph-node-4 radosgw[6872]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:35 ceph-node-5 osd[8698]: WARNING: Slow request detected on OSD 4.3
2025-02-24 10:51:06 ceph-node-1 radosgw[8344]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:51:29 ceph-node-5 osd[6172]: ERROR: Network partition detected between OSD nodes
2025-02-24 10:50:31 ceph-node-1 mds[6497]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:50:33 ceph-node-2 client[9230]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:07 ceph-node-5 osd[7865]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:50:38 ceph-node-5 client[1378]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:11 ceph-node-5 osd[5349]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:08 ceph-node-3 radosgw[2627]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:41 ceph-node-4 mds[6028]: INFO: PG 1.2 replicated successfully
2025-02-24 10:51:11 ceph-node-3 client[4890]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:50:43 ceph-node-1 osd[7590]: INFO: Monitor map has been updated
2025-02-24 10:50:43 ceph-node-5 osd[2011]: INFO: OSD disk space utilization: 70%
2025-02-24 10:51:10 ceph-node-4 mgr[8394]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:04 ceph-node-1 client[2557]: WARNING: OSD 3.1 marked down due to heartbeat failure
2025-02-24 10:51:28 ceph-node-4 radosgw[2506]: NOTICE: MGR module loaded successfully
2025-02-24 10:51:15 ceph-node-5 client[6387]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:18 ceph-node-2 radosgw[4007]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:39 ceph-node-1 client[6524]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:51:17 ceph-node-4 mds[2717]: NOTICE: Monitor map has been updated
2025-02-24 10:51:26 ceph-node-2 mds[2131]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:53 ceph-node-4 mgr[7848]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:06 ceph-node-1 osd[9311]: DEBUG: MGR module loaded successfully
2025-02-24 10:50:49 ceph-node-5 osd[9476]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:50:54 ceph-node-2 radosgw[6470]: WARNING: Monitor down, unable to connect to quorum
2025-02-24 10:51:26 ceph-node-2 osd[2121]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:50:32 ceph-node-4 osd[5962]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:51:25 ceph-node-4 client[6796]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:26 ceph-node-1 mds[6266]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:51:13 ceph-node-1 mon[4892]: INFO: PG 1.2 replicated successfully
2025-02-24 10:51:07 ceph-node-2 mgr[3556]: WARNING: Network partition detected between OSD nodes
2025-02-24 10:51:18 ceph-node-2 mgr[3061]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:47 ceph-node-2 mon[3537]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:23 ceph-node-5 osd[7314]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:50:43 ceph-node-2 mds[9908]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:03 ceph-node-3 mon[8778]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:50:58 ceph-node-1 osd[9130]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:16 ceph-node-5 mds[5571]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:51:14 ceph-node-5 osd[6703]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:43 ceph-node-3 mon[9143]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:22 ceph-node-5 mon[9764]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:51:10 ceph-node-5 mds[2885]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:50:40 ceph-node-3 radosgw[6866]: DEBUG: MGR module loaded successfully
2025-02-24 10:50:32 ceph-node-1 radosgw[5648]: NOTICE: MGR module loaded successfully
2025-02-24 10:50:53 ceph-node-4 mds[4310]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:50:34 ceph-node-3 mon[8077]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:05 ceph-node-5 mgr[7514]: ERROR: Monitor down, unable to connect to quorum
2025-02-24 10:51:04 ceph-node-4 client[1867]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:11 ceph-node-5 radosgw[4279]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:48 ceph-node-4 mgr[3419]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:33 ceph-node-4 radosgw[7731]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:50:56 ceph-node-5 mon[9185]: NOTICE: Monitor map has been updated
2025-02-24 10:50:55 ceph-node-5 osd[6967]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:50:45 ceph-node-1 osd[3078]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:08 ceph-node-2 osd[7967]: NOTICE: Data replication completed for object pool
2025-02-24 10:50:43 ceph-node-1 osd[2202]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:43 ceph-node-4 mds[8611]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:50:31 ceph-node-3 mds[8124]: INFO: MGR module loaded successfully
2025-02-24 10:50:46 ceph-node-2 mgr[1714]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:51:31 ceph-node-4 mds[7731]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:50:34 ceph-node-4 mds[2202]: ERROR: Monitor mon1 failed to reach quorum
2025-02-24 10:51:30 ceph-node-5 radosgw[2841]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:44 ceph-node-3 mon[4480]: WARNING: CRITICAL: PG 2.1 stuck in inconsistent state
2025-02-24 10:50:47 ceph-node-2 mds[8904]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:50:43 ceph-node-4 mgr[2310]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:50:50 ceph-node-2 osd[2950]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:01 ceph-node-1 osd[3897]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:50:58 ceph-node-3 mds[1886]: INFO: MGR module loaded successfully
2025-02-24 10:50:36 ceph-node-2 radosgw[8349]: DEBUG: OSD rebalancing completed
2025-02-24 10:50:43 ceph-node-4 osd[6502]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:50:43 ceph-node-5 osd[5042]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:13 ceph-node-2 osd[8021]: INFO: PG 1.2 replicated successfully
2025-02-24 10:50:40 ceph-node-1 client[9464]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:50:52 ceph-node-5 mgr[9190]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:31 ceph-node-1 mgr[7222]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:23 ceph-node-4 mds[4395]: DEBUG: OSD rebalancing completed
2025-02-24 10:51:16 ceph-node-1 mon[2729]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:41 ceph-node-5 mgr[4709]: DEBUG: OSD rebalancing completed
2025-02-24 10:50:51 ceph-node-5 mon[6711]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:17 ceph-node-5 client[4275]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:50:46 ceph-node-5 mds[3066]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:09 ceph-node-5 mgr[2352]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:50:37 ceph-node-5 mgr[8180]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:50:41 ceph-node-2 client[6346]: WARNING: Data corruption detected in object pool
2025-02-24 10:50:33 ceph-node-4 osd[3251]: WARNING: Journal write failure detected on OSD node
2025-02-24 10:51:19 ceph-node-1 osd[1032]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:51:20 ceph-node-2 radosgw[7525]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:51:22 ceph-node-2 mgr[1254]: INFO: OSD rebalancing completed
2025-02-24 10:50:59 ceph-node-4 client[9681]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:51:28 ceph-node-4 mgr[5512]: INFO: Data replication completed for object pool
2025-02-24 10:50:43 ceph-node-3 mon[1895]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:50:55 ceph-node-2 mds[8193]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:16 ceph-node-3 mds[4864]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:43 ceph-node-5 osd[4401]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:50:42 ceph-node-1 client[5154]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:50:57 ceph-node-4 osd[5868]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:51:27 ceph-node-3 client[6206]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:05 ceph-node-4 mon[8514]: ERROR: Cluster status: HEALTH_WARN
2025-02-24 10:50:40 ceph-node-1 mgr[3203]: DEBUG: MGR module loaded successfully
2025-02-24 10:50:42 ceph-node-2 client[6302]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:50:52 ceph-node-1 mgr[2623]: INFO: OSD disk space utilization: 70%
2025-02-24 10:51:08 ceph-node-5 osd[1688]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:50:47 ceph-node-5 osd[5982]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:51:00 ceph-node-3 radosgw[4751]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:38 ceph-node-5 mds[6397]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:50:47 ceph-node-4 osd[7361]: WARNING: Data corruption detected in object pool
2025-02-24 10:50:55 ceph-node-2 radosgw[9242]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:46 ceph-node-1 osd[7750]: INFO: OSD disk space utilization: 70%
2025-02-24 10:51:03 ceph-node-3 osd[2526]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:02 ceph-node-4 mgr[8196]: DEBUG: Data replication completed for object pool
2025-02-24 10:50:46 ceph-node-1 client[8393]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:42 ceph-node-4 osd[9665]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:37 ceph-node-3 mds[9051]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:50:34 ceph-node-3 mds[2124]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:13 ceph-node-4 osd[6886]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:51:20 ceph-node-5 mon[2937]: INFO: OSD rebalancing completed
2025-02-24 10:51:01 ceph-node-1 radosgw[8308]: NOTICE: OSD rebalancing completed
2025-02-24 10:51:09 ceph-node-5 mgr[4257]: INFO: Data replication completed for object pool
2025-02-24 10:51:16 ceph-node-4 client[9788]: ERROR: Uncorrectable ECC error detected in MDS cache
2025-02-24 10:50:34 ceph-node-5 client[6602]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:41 ceph-node-3 osd[9855]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:02 ceph-node-4 mds[3228]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:51:24 ceph-node-4 mds[8316]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:50:54 ceph-node-1 osd[9335]: INFO: Data replication completed for object pool
2025-02-24 10:51:08 ceph-node-3 radosgw[3781]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:34 ceph-node-1 client[1438]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:56 ceph-node-1 mds[9194]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:50:40 ceph-node-3 client[8280]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:44 ceph-node-2 radosgw[7036]: NOTICE: OSD rebalancing completed
2025-02-24 10:51:15 ceph-node-1 mds[4178]: DEBUG: OSD rebalancing completed
2025-02-24 10:50:39 ceph-node-3 mds[4389]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:17 ceph-node-3 radosgw[4391]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:50:46 ceph-node-1 mon[6139]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:08 ceph-node-1 mon[6404]: DEBUG: OSD rebalancing completed
2025-02-24 10:50:43 ceph-node-1 radosgw[9415]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:51:18 ceph-node-1 radosgw[5719]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:39 ceph-node-4 radosgw[5552]: NOTICE: MGR module loaded successfully
2025-02-24 10:51:14 ceph-node-5 client[1739]: ERROR: High I/O latency detected in radosgw service
2025-02-24 10:51:21 ceph-node-5 mgr[2688]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:50:54 ceph-node-2 osd[9154]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:11 ceph-node-3 mon[9590]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:02 ceph-node-3 client[4803]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:50:59 ceph-node-5 mgr[2142]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:50 ceph-node-1 mds[3848]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:50 ceph-node-1 mon[6974]: INFO: MGR module loaded successfully
2025-02-24 10:50:46 ceph-node-2 mon[8955]: DEBUG: MGR module loaded successfully
2025-02-24 10:50:40 ceph-node-2 mds[1393]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:15 ceph-node-3 mgr[8163]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:45 ceph-node-4 radosgw[8393]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:51:00 ceph-node-2 mon[1915]: WARNING: Cluster status: HEALTH_WARN
2025-02-24 10:51:26 ceph-node-2 osd[7220]: ERROR: Network partition detected between OSD nodes
2025-02-24 10:51:18 ceph-node-1 client[3835]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:50:45 ceph-node-3 mon[2915]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:51:04 ceph-node-4 mds[3497]: CRITICAL: Monitor mon1 failed to reach quorum
2025-02-24 10:51:07 ceph-node-4 osd[3668]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:50:52 ceph-node-2 mds[4525]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:45 ceph-node-4 mds[3498]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:26 ceph-node-1 client[9731]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:13 ceph-node-2 mgr[9708]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:39 ceph-node-1 mon[4353]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:50:39 ceph-node-4 mgr[5056]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:28 ceph-node-2 osd[2580]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:16 ceph-node-2 mon[4658]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:25 ceph-node-2 osd[1769]: CRITICAL: Journal write failure detected on OSD node
2025-02-24 10:50:48 ceph-node-4 mds[1075]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:31 ceph-node-4 mds[7700]: ERROR: CRITICAL: Data loss detected on pool 1
2025-02-24 10:51:12 ceph-node-1 osd[4284]: INFO: OSD disk space utilization: 70%
2025-02-24 10:51:01 ceph-node-4 radosgw[7323]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:07 ceph-node-5 radosgw[2999]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:08 ceph-node-1 osd[5589]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:09 ceph-node-2 mgr[2679]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:50:43 ceph-node-5 mds[5131]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:22 ceph-node-1 osd[5368]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:17 ceph-node-3 osd[5892]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:50:32 ceph-node-1 radosgw[2917]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:50:58 ceph-node-4 mon[6368]: INFO: MGR module loaded successfully
2025-02-24 10:51:14 ceph-node-5 radosgw[5250]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:01 ceph-node-4 client[6480]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:50:36 ceph-node-1 client[9204]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:09 ceph-node-5 radosgw[3848]: INFO: PG 1.2 replicated successfully
2025-02-24 10:51:10 ceph-node-1 radosgw[9782]: ERROR: Data corruption detected in object pool
2025-02-24 10:51:06 ceph-node-1 client[8007]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:50:56 ceph-node-5 osd[4599]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:51:03 ceph-node-1 mon[7221]: NOTICE: Data replication completed for object pool
2025-02-24 10:50:55 ceph-node-2 mon[1583]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:51:07 ceph-node-5 radosgw[8706]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:33 ceph-node-4 client[9973]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:58 ceph-node-5 mds[6887]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:27 ceph-node-2 client[8616]: NOTICE: OSD rebalancing completed
2025-02-24 10:51:22 ceph-node-4 osd[6175]: NOTICE: Monitor map has been updated
2025-02-24 10:51:07 ceph-node-5 mon[3186]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:05 ceph-node-2 mds[4751]: INFO: OSD rebalancing completed
2025-02-24 10:51:03 ceph-node-4 osd[4381]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:21 ceph-node-5 radosgw[6935]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:22 ceph-node-3 radosgw[6491]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:25 ceph-node-4 mds[6565]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:01 ceph-node-1 mon[5550]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:08 ceph-node-3 client[3839]: WARNING: Client connection timeout detected
2025-02-24 10:51:25 ceph-node-3 client[5488]: INFO: MGR module loaded successfully
2025-02-24 10:50:36 ceph-node-1 mon[4223]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:51:11 ceph-node-2 mgr[5135]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:28 ceph-node-1 mon[9877]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:50:51 ceph-node-3 mgr[1390]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:51:15 ceph-node-5 mds[2743]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:50:45 ceph-node-5 mon[2957]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:50:42 ceph-node-2 mds[6268]: ERROR: Client connection timeout detected
2025-02-24 10:51:04 ceph-node-5 mon[7499]: NOTICE: OSD rebalancing completed
2025-02-24 10:50:36 ceph-node-5 radosgw[8124]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:41 ceph-node-5 mgr[7759]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:51:08 ceph-node-4 mgr[4586]: INFO: MGR module loaded successfully
2025-02-24 10:50:35 ceph-node-2 radosgw[7582]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:19 ceph-node-1 mgr[3026]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:11 ceph-node-3 mon[9467]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:08 ceph-node-1 osd[6513]: NOTICE: Monitor map has been updated
2025-02-24 10:50:42 ceph-node-3 osd[4716]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:24 ceph-node-3 osd[2904]: NOTICE: MGR module loaded successfully
2025-02-24 10:50:57 ceph-node-5 mon[6479]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:51:00 ceph-node-4 osd[3172]: DEBUG: Monitor map has been updated
2025-02-24 10:51:30 ceph-node-1 client[8760]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:03 ceph-node-2 mds[1193]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:10 ceph-node-2 osd[4166]: DEBUG: OSD rebalancing completed
2025-02-24 10:51:01 ceph-node-5 mds[8928]: NOTICE: Data replication completed for object pool
2025-02-24 10:50:40 ceph-node-3 mgr[3592]: DEBUG: OSD rebalancing completed
2025-02-24 10:51:14 ceph-node-4 radosgw[9133]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:25 ceph-node-1 mon[4441]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:21 ceph-node-2 client[7401]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:50 ceph-node-5 osd[3912]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:46 ceph-node-2 mds[9352]: CRITICAL: Monitor down, unable to connect to quorum
2025-02-24 10:51:10 ceph-node-5 radosgw[2947]: ERROR: Cluster status: HEALTH_WARN
2025-02-24 10:51:18 ceph-node-2 osd[2214]: WARNING: High I/O latency detected in radosgw service
2025-02-24 10:50:33 ceph-node-1 mds[3237]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:01 ceph-node-3 radosgw[3744]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:21 ceph-node-1 mgr[7035]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:45 ceph-node-2 osd[2253]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:28 ceph-node-3 mon[8425]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:21 ceph-node-4 radosgw[5050]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:15 ceph-node-3 mgr[2684]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:10 ceph-node-2 mon[2039]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:51:06 ceph-node-1 osd[1142]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:39 ceph-node-1 client[6156]: ERROR: Client connection timeout detected
2025-02-24 10:50:57 ceph-node-5 mds[8757]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:47 ceph-node-3 mds[6449]: CRITICAL: OSD 3.1 marked down due to heartbeat failure
2025-02-24 10:51:13 ceph-node-4 mds[2225]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:06 ceph-node-3 client[2119]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:16 ceph-node-1 client[7679]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:20 ceph-node-3 radosgw[5470]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:51 ceph-node-1 mon[6813]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:50:57 ceph-node-1 mds[7215]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:31 ceph-node-5 radosgw[4306]: DEBUG: Monitor map has been updated
2025-02-24 10:50:55 ceph-node-4 radosgw[7856]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:50:42 ceph-node-4 mgr[9489]: NOTICE: Data replication completed for object pool
2025-02-24 10:51:23 ceph-node-2 radosgw[2921]: CRITICAL: Slow request detected on OSD 4.3
2025-02-24 10:50:49 ceph-node-5 mon[1812]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:50:45 ceph-node-1 mds[9505]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:50:44 ceph-node-5 mgr[4214]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:11 ceph-node-4 mon[4341]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:26 ceph-node-2 radosgw[4336]: CRITICAL: Slow request detected on OSD 4.3
2025-02-24 10:50:52 ceph-node-4 client[3060]: DEBUG: MGR module loaded successfully
2025-02-24 10:50:38 ceph-node-3 osd[8996]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:29 ceph-node-3 osd[5686]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:16 ceph-node-5 radosgw[1562]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:09 ceph-node-4 radosgw[6806]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:19 ceph-node-1 radosgw[3546]: NOTICE: OSD rebalancing completed
2025-02-24 10:51:19 ceph-node-5 radosgw[9307]: NOTICE: MGR module loaded successfully
2025-02-24 10:51:13 ceph-node-2 osd[2547]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:50:32 ceph-node-4 client[8762]: NOTICE: Monitor map has been updated
2025-02-24 10:51:14 ceph-node-5 radosgw[9590]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:51:15 ceph-node-5 mon[4837]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:50:45 ceph-node-2 radosgw[4204]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:24 ceph-node-5 osd[4728]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:50:52 ceph-node-5 mgr[8100]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:42 ceph-node-1 mon[2881]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:42 ceph-node-1 mds[7605]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:50:48 ceph-node-2 radosgw[5871]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:22 ceph-node-2 client[8057]: WARNING: Client connection timeout detected
2025-02-24 10:51:26 ceph-node-4 radosgw[3915]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:50:54 ceph-node-4 osd[7491]: NOTICE: Monitor map has been updated
2025-02-24 10:50:53 ceph-node-3 radosgw[4057]: DEBUG: MGR module loaded successfully
2025-02-24 10:50:31 ceph-node-2 mgr[5672]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:50:46 ceph-node-4 osd[6433]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:25 ceph-node-4 radosgw[7668]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:31 ceph-node-5 mds[3447]: INFO: Data replication completed for object pool
2025-02-24 10:50:35 ceph-node-3 radosgw[8797]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:50:38 ceph-node-4 mgr[3471]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:50:36 ceph-node-2 mon[3603]: DEBUG: OSD rebalancing completed
2025-02-24 10:51:00 ceph-node-1 radosgw[4438]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:50:41 ceph-node-4 mon[8316]: WARNING: Network partition detected between OSD nodes
2025-02-24 10:51:00 ceph-node-5 osd[1484]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:50:54 ceph-node-1 osd[4898]: WARNING: Monitor mon1 failed to reach quorum
2025-02-24 10:51:06 ceph-node-2 mon[2251]: INFO: Data replication completed for object pool
2025-02-24 10:50:45 ceph-node-3 mds[8474]: ERROR: Slow request detected on OSD 4.3
2025-02-24 10:51:27 ceph-node-3 mon[2483]: INFO: OSD rebalancing completed
2025-02-24 10:50:51 ceph-node-3 mon[5804]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:21 ceph-node-1 radosgw[5477]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:50:57 ceph-node-4 client[2596]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:03 ceph-node-4 mgr[6582]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:50:52 ceph-node-4 client[3208]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:48 ceph-node-1 mon[1743]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:31 ceph-node-4 client[9669]: INFO: Monitor map has been updated
2025-02-24 10:51:03 ceph-node-1 client[3808]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:07 ceph-node-4 mgr[3961]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:51:31 ceph-node-5 mon[4861]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:50:56 ceph-node-3 radosgw[8697]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:15 ceph-node-4 mgr[3300]: NOTICE: Data replication completed for object pool
2025-02-24 10:50:55 ceph-node-2 mds[8626]: NOTICE: Data replication completed for object pool
2025-02-24 10:50:55 ceph-node-3 mon[6549]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:51:08 ceph-node-5 osd[4451]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:51:16 ceph-node-5 mon[3058]: INFO: Monitor map has been updated
2025-02-24 10:51:07 ceph-node-3 mon[4686]: DEBUG: Data replication completed for object pool
2025-02-24 10:50:59 ceph-node-1 mgr[8772]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:15 ceph-node-1 osd[7243]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:51:13 ceph-node-5 osd[8036]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:39 ceph-node-4 client[6878]: INFO: Data replication completed for object pool
2025-02-24 10:50:44 ceph-node-5 client[1221]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:14 ceph-node-5 radosgw[9340]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:01 ceph-node-3 osd[4380]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:50:37 ceph-node-1 radosgw[1242]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:32 ceph-node-3 mds[1520]: INFO: OSD rebalancing completed
2025-02-24 10:50:57 ceph-node-5 osd[7965]: DEBUG: Data replication completed for object pool
2025-02-24 10:50:43 ceph-node-1 mds[5244]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:27 ceph-node-2 osd[2279]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:13 ceph-node-4 osd[7830]: INFO: PG 1.2 replicated successfully
2025-02-24 10:51:27 ceph-node-2 radosgw[1482]: WARNING: Cluster status: HEALTH_WARN
2025-02-24 10:50:51 ceph-node-1 client[9583]: INFO: Data replication completed for object pool
2025-02-24 10:50:37 ceph-node-4 mds[7054]: ERROR: Monitor down, unable to connect to quorum
2025-02-24 10:50:56 ceph-node-1 osd[8520]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:51:03 ceph-node-3 mds[4476]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:51:14 ceph-node-3 mds[1232]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:16 ceph-node-1 mds[2631]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:56 ceph-node-4 mgr[7545]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:50:45 ceph-node-2 client[2272]: INFO: OSD disk space utilization: 70%
2025-02-24 10:51:14 ceph-node-1 mgr[5155]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:00 ceph-node-4 client[4262]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:50:48 ceph-node-1 mds[5728]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:56 ceph-node-4 osd[8105]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:50:46 ceph-node-2 mon[1903]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:51:21 ceph-node-1 mds[5294]: CRITICAL: OSD disk space utilization exceeded 90%
2025-02-24 10:50:43 ceph-node-1 radosgw[7699]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:50:42 ceph-node-5 mon[9585]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:34 ceph-node-5 mgr[3290]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:50:40 ceph-node-5 osd[3409]: ERROR: CRITICAL: PG 2.1 stuck in inconsistent state
2025-02-24 10:51:22 ceph-node-2 client[2793]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:50:32 ceph-node-4 mon[6147]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:31 ceph-node-5 client[9696]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:50:35 ceph-node-2 mds[6721]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:51:18 ceph-node-1 mds[5152]: INFO: OSD rebalancing completed
2025-02-24 10:51:25 ceph-node-5 mds[3577]: NOTICE: Monitor map has been updated
2025-02-24 10:50:33 ceph-node-4 radosgw[8650]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:51:31 ceph-node-4 mon[2448]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:50:59 ceph-node-2 radosgw[8640]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:51:31 ceph-node-4 radosgw[5724]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:50:56 ceph-node-3 mds[4189]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:51:22 ceph-node-4 mon[9843]: INFO: Data replication completed for object pool
2025-02-24 10:50:35 ceph-node-3 mon[6343]: WARNING: High I/O latency detected in radosgw service
2025-02-24 10:51:16 ceph-node-4 mds[4009]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:50:41 ceph-node-2 mon[7985]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:28 ceph-node-3 client[7559]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:51:21 ceph-node-4 mds[4184]: CRITICAL: Cluster status: HEALTH_WARN
2025-02-24 10:51:07 ceph-node-2 client[3642]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:51:19 ceph-node-3 mgr[4397]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:10 ceph-node-5 mgr[1401]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:20 ceph-node-2 mgr[7541]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:50:33 ceph-node-2 client[7271]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:25 ceph-node-2 mon[8503]: CRITICAL: Cluster status: HEALTH_WARN
2025-02-24 10:51:24 ceph-node-2 mds[7904]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:05 ceph-node-1 mds[5947]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:01 ceph-node-5 mds[5262]: INFO: Monitor map has been updated
2025-02-24 10:51:24 ceph-node-1 mgr[7172]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:25 ceph-node-4 mon[7758]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:58 ceph-node-2 client[1437]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:28 ceph-node-1 mds[9375]: WARNING: CRITICAL: PG 2.1 stuck in inconsistent state
2025-02-24 10:51:00 ceph-node-1 radosgw[7153]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:17 ceph-node-4 client[4512]: CRITICAL: Authentication failure in RADOS Gateway
2025-02-24 10:50:50 ceph-node-4 osd[2952]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:51:07 ceph-node-2 mon[9374]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:50:58 ceph-node-5 mds[5299]: INFO: Monitor map has been updated
2025-02-24 10:50:35 ceph-node-4 client[9437]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:51:01 ceph-node-3 mds[7312]: INFO: MGR module loaded successfully
2025-02-24 10:51:15 ceph-node-4 osd[1313]: NOTICE: Data replication completed for object pool
2025-02-24 10:51:24 ceph-node-5 radosgw[5443]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:50:53 ceph-node-5 radosgw[1500]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:50:32 ceph-node-5 mds[8237]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:50:53 ceph-node-1 mgr[8840]: CRITICAL: Data corruption detected in object pool
2025-02-24 10:50:32 ceph-node-4 client[5638]: WARNING: CRITICAL: PG 2.1 stuck in inconsistent state
2025-02-24 10:50:33 ceph-node-3 radosgw[1945]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:51:12 ceph-node-1 osd[1025]: DEBUG: Monitor map has been updated
2025-02-24 10:50:39 ceph-node-3 mon[8023]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:11 ceph-node-5 mds[4536]: INFO: Data replication completed for object pool
2025-02-24 10:50:45 ceph-node-1 osd[6011]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:51:18 ceph-node-3 client[8741]: NOTICE: Monitor map has been updated
2025-02-24 10:51:14 ceph-node-1 mgr[4883]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:49 ceph-node-4 mgr[9076]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:30 ceph-node-1 mon[3831]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:51:08 ceph-node-2 mon[7294]: INFO: MGR module loaded successfully
2025-02-24 10:50:48 ceph-node-2 client[5827]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:50:48 ceph-node-5 osd[2040]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:51:29 ceph-node-4 osd[9686]: ERROR: Journal write failure detected on OSD node
2025-02-24 10:51:10 ceph-node-5 radosgw[1058]: DEBUG: Monitor map has been updated
2025-02-24 10:51:14 ceph-node-1 mon[2361]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:02 ceph-node-1 radosgw[2060]: NOTICE: Monitor map has been updated
2025-02-24 10:50:50 ceph-node-4 mds[9039]: WARNING: Journal write failure detected on OSD node
2025-02-24 10:50:42 ceph-node-1 mgr[5577]: INFO: Data replication completed for object pool
2025-02-24 10:50:53 ceph-node-2 mon[8694]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:50:42 ceph-node-1 radosgw[5363]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:13 ceph-node-1 radosgw[4538]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:02 ceph-node-3 radosgw[9564]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:14 ceph-node-4 radosgw[8548]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:48 ceph-node-5 mon[6649]: WARNING: Disk failure reported on ceph-node-4
2025-02-24 10:50:50 ceph-node-4 radosgw[3170]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:12 ceph-node-4 osd[8130]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:29 ceph-node-4 client[4970]: INFO: OSD disk space utilization: 70%
2025-02-24 10:50:57 ceph-node-5 radosgw[1713]: INFO: OSD disk space utilization: 70%
2025-02-24 10:50:35 ceph-node-2 mgr[7354]: INFO: Monitor map has been updated
2025-02-24 10:51:07 ceph-node-3 mon[1505]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:09 ceph-node-1 client[6686]: ERROR: Data corruption detected in object pool
2025-02-24 10:51:12 ceph-node-4 mgr[7630]: INFO: Monitor map has been updated
2025-02-24 10:51:29 ceph-node-3 osd[5511]: NOTICE: OSD rebalancing completed
2025-02-24 10:51:29 ceph-node-4 radosgw[9126]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:20 ceph-node-4 osd[2675]: INFO: Monitor map has been updated
2025-02-24 10:51:27 ceph-node-4 radosgw[7654]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:50:35 ceph-node-4 mgr[2614]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:26 ceph-node-2 radosgw[9199]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:51:01 ceph-node-1 mgr[2303]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:51:04 ceph-node-1 mds[1923]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:48 ceph-node-2 radosgw[5838]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:24 ceph-node-5 mds[5708]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:24 ceph-node-4 client[8485]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:50:55 ceph-node-2 osd[7798]: INFO: OSD disk space utilization: 70%
2025-02-24 10:51:26 ceph-node-5 mgr[7676]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:24 ceph-node-4 osd[9171]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:49 ceph-node-1 client[2730]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:51:23 ceph-node-4 radosgw[4907]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:11 ceph-node-2 osd[3654]: WARNING: Journal write failure detected on OSD node
2025-02-24 10:50:31 ceph-node-1 osd[7995]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:51:26 ceph-node-1 mgr[3499]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:04 ceph-node-2 osd[5201]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:08 ceph-node-4 radosgw[6232]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:50:48 ceph-node-3 osd[4283]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:04 ceph-node-4 client[7715]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:51:06 ceph-node-5 radosgw[7327]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:50:50 ceph-node-5 osd[6161]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:24 ceph-node-4 client[5807]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:20 ceph-node-4 mgr[9540]: INFO: Data replication completed for object pool
2025-02-24 10:50:52 ceph-node-5 mon[4359]: ERROR: CRITICAL: Data loss detected on pool 1
2025-02-24 10:51:21 ceph-node-4 mgr[6746]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:07 ceph-node-2 mgr[5508]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:50:41 ceph-node-3 client[1495]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:21 ceph-node-2 mds[3308]: ERROR: Cluster status: HEALTH_WARN
2025-02-24 10:50:49 ceph-node-4 osd[9782]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:51:26 ceph-node-2 mgr[8460]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:51:18 ceph-node-1 mds[7265]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:51:21 ceph-node-1 mon[1350]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:04 ceph-node-1 mgr[9967]: INFO: MGR module loaded successfully
2025-02-24 10:50:48 ceph-node-3 osd[5935]: NOTICE: Data replication completed for object pool
2025-02-24 10:51:09 ceph-node-5 client[2942]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:50:32 ceph-node-5 client[6848]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:50:37 ceph-node-1 osd[5324]: NOTICE: OSD rebalancing completed
2025-02-24 10:50:52 ceph-node-3 client[8099]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:38 ceph-node-2 radosgw[8290]: DEBUG: Monitor map has been updated
2025-02-24 10:50:42 ceph-node-1 radosgw[2883]: INFO: OSD disk space utilization: 70%
2025-02-24 10:50:51 ceph-node-3 osd[6013]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:51:31 ceph-node-4 mds[3795]: DEBUG: OSD rebalancing completed
2025-02-24 10:50:54 ceph-node-5 mds[9539]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:08 ceph-node-5 osd[6743]: NOTICE: MGR module loaded successfully
2025-02-24 10:51:07 ceph-node-4 mgr[1071]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:15 ceph-node-3 radosgw[6408]: CRITICAL: Cluster status: HEALTH_WARN
2025-02-24 10:51:23 ceph-node-1 osd[9527]: INFO: PG 1.2 replicated successfully
2025-02-24 10:51:16 ceph-node-4 mgr[4118]: ERROR: Disk failure reported on ceph-node-4
2025-02-24 10:50:38 ceph-node-2 radosgw[4232]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:41 ceph-node-4 mgr[4176]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:53 ceph-node-3 mds[9948]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:51:11 ceph-node-2 client[2745]: INFO: OSD rebalancing completed
2025-02-24 10:50:35 ceph-node-2 mon[7750]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:51:30 ceph-node-5 radosgw[9218]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:51:28 ceph-node-3 osd[8366]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:50:31 ceph-node-2 radosgw[4943]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:21 ceph-node-4 mon[8281]: INFO: OSD rebalancing completed
2025-02-24 10:50:31 ceph-node-5 radosgw[4243]: INFO: PG 1.2 replicated successfully
2025-02-24 10:51:02 ceph-node-4 radosgw[6390]: INFO: MGR module loaded successfully
2025-02-24 10:51:00 ceph-node-5 radosgw[5253]: INFO: Data replication completed for object pool
2025-02-24 10:50:49 ceph-node-1 radosgw[9890]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:29 ceph-node-5 client[5428]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:50:35 ceph-node-1 mon[9554]: INFO: Data replication completed for object pool
2025-02-24 10:50:39 ceph-node-2 mon[3395]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:09 ceph-node-4 mon[9317]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:51:10 ceph-node-2 client[9265]: INFO: MGR module loaded successfully
2025-02-24 10:50:37 ceph-node-4 osd[4873]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:50:59 ceph-node-5 client[1288]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:51:09 ceph-node-4 mds[5800]: INFO: OSD rebalancing completed
2025-02-24 10:50:43 ceph-node-1 osd[1044]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:50:55 ceph-node-1 mgr[6554]: CRITICAL: OSD 3.1 marked down due to heartbeat failure
2025-02-24 10:50:33 ceph-node-4 mds[9492]: DEBUG: OSD rebalancing completed
2025-02-24 10:51:12 ceph-node-2 client[5446]: INFO: Monitor map has been updated
2025-02-24 10:51:15 ceph-node-5 mon[5915]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:15 ceph-node-5 osd[8760]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:50:52 ceph-node-5 mon[7233]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:50:42 ceph-node-3 osd[1530]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:50:40 ceph-node-3 mgr[3717]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:09 ceph-node-5 radosgw[9010]: INFO: Monitor map has been updated
2025-02-24 10:50:54 ceph-node-4 osd[3957]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:12 ceph-node-2 client[5751]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:51:09 ceph-node-1 radosgw[2343]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:51:04 ceph-node-4 mds[1629]: DEBUG: OSD rebalancing completed
2025-02-24 10:51:25 ceph-node-4 mon[1610]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:51:07 ceph-node-3 mon[8222]: INFO: Monitor map has been updated
2025-02-24 10:51:09 ceph-node-1 client[6447]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:12 ceph-node-4 mgr[3084]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:50:41 ceph-node-2 mgr[7998]: NOTICE: OSD rebalancing completed
2025-02-24 10:51:18 ceph-node-2 osd[7912]: DEBUG: Data replication completed for object pool
2025-02-24 10:50:43 ceph-node-5 client[7479]: NOTICE: OSD rebalancing completed
2025-02-24 10:51:29 ceph-node-3 mon[2765]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:51:13 ceph-node-3 radosgw[6973]: NOTICE: OSD rebalancing completed
2025-02-24 10:51:05 ceph-node-4 mgr[3792]: INFO: Data replication completed for object pool
2025-02-24 10:51:07 ceph-node-3 client[9472]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:31 ceph-node-1 client[5481]: ERROR: OSD 3.1 marked down due to heartbeat failure
2025-02-24 10:51:13 ceph-node-2 osd[5770]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:51:10 ceph-node-4 osd[3181]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:26 ceph-node-4 radosgw[9130]: ERROR: OSD disk space utilization exceeded 90%
2025-02-24 10:51:04 ceph-node-3 mgr[7741]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:00 ceph-node-3 radosgw[7031]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:45 ceph-node-4 client[4759]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:26 ceph-node-4 radosgw[5486]: INFO: OSD disk space utilization: 70%
2025-02-24 10:51:16 ceph-node-3 mgr[6181]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:51:23 ceph-node-2 mds[1411]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:34 ceph-node-4 mds[6751]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:22 ceph-node-3 client[5598]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:50:52 ceph-node-5 radosgw[4810]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:50:50 ceph-node-2 mon[3806]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:59 ceph-node-4 osd[3079]: NOTICE: Data replication completed for object pool
2025-02-24 10:50:52 ceph-node-5 mgr[2951]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:50:34 ceph-node-3 mds[8333]: INFO: MGR module loaded successfully
2025-02-24 10:51:00 ceph-node-4 radosgw[2068]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:50:37 ceph-node-4 client[9051]: INFO: MGR module loaded successfully
2025-02-24 10:51:14 ceph-node-1 client[9870]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:50:48 ceph-node-2 radosgw[5691]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:12 ceph-node-3 mgr[8091]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:51:24 ceph-node-4 mon[3372]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:50:42 ceph-node-1 osd[1965]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:50:56 ceph-node-5 mds[7492]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:32 ceph-node-5 osd[8302]: NOTICE: OSD rebalancing completed
2025-02-24 10:51:02 ceph-node-3 mds[2460]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:26 ceph-node-1 client[4270]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:51:28 ceph-node-2 mgr[4073]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:49 ceph-node-3 mds[6822]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:07 ceph-node-1 radosgw[2585]: INFO: OSD disk space utilization: 70%
2025-02-24 10:51:06 ceph-node-3 client[8576]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:50:53 ceph-node-2 mgr[1804]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:06 ceph-node-2 osd[3214]: DEBUG: MGR module loaded successfully
2025-02-24 10:50:52 ceph-node-2 mds[2183]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:50:48 ceph-node-5 mon[7076]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:50:53 ceph-node-2 radosgw[6355]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:36 ceph-node-1 mon[1647]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:14 ceph-node-4 mds[9570]: DEBUG: Monitor map has been updated
2025-02-24 10:50:46 ceph-node-3 mds[8577]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:50:49 ceph-node-3 client[9523]: DEBUG: OSD rebalancing completed
2025-02-24 10:50:51 ceph-node-3 mds[7165]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:50:40 ceph-node-3 osd[5754]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:50:36 ceph-node-5 osd[3110]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:47 ceph-node-1 mon[6230]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:50:34 ceph-node-5 radosgw[1009]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:56 ceph-node-5 client[4511]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:29 ceph-node-2 mds[3719]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:07 ceph-node-1 mds[4711]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:57 ceph-node-2 osd[4305]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:50:43 ceph-node-4 osd[6930]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:51:09 ceph-node-5 radosgw[9214]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:23 ceph-node-4 mds[9162]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:51:25 ceph-node-2 mds[7882]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:50:54 ceph-node-3 radosgw[8151]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:13 ceph-node-4 mon[3472]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:12 ceph-node-5 osd[2191]: INFO: OSD disk space utilization: 70%
2025-02-24 10:50:52 ceph-node-5 mgr[3409]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:50:38 ceph-node-1 mds[5586]: WARNING: Data corruption detected in object pool
2025-02-24 10:50:54 ceph-node-3 radosgw[8386]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:50:39 ceph-node-2 mgr[6621]: INFO: Data replication completed for object pool
2025-02-24 10:51:30 ceph-node-4 client[7645]: NOTICE: OSD rebalancing completed
2025-02-24 10:51:30 ceph-node-4 mds[4882]: NOTICE: MGR module loaded successfully
2025-02-24 10:51:23 ceph-node-2 mon[4574]: DEBUG: OSD rebalancing completed
2025-02-24 10:50:59 ceph-node-2 mds[3388]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:50:47 ceph-node-5 radosgw[5224]: INFO: OSD disk space utilization: 70%
2025-02-24 10:51:26 ceph-node-5 mon[7179]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:47 ceph-node-3 mon[5640]: NOTICE: OSD rebalancing completed
2025-02-24 10:51:03 ceph-node-4 osd[9406]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:51:20 ceph-node-1 mds[9614]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:03 ceph-node-2 radosgw[1716]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:36 ceph-node-1 radosgw[5701]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:57 ceph-node-1 osd[5838]: INFO: Data replication completed for object pool
2025-02-24 10:50:38 ceph-node-2 mon[4332]: INFO: MGR module loaded successfully
2025-02-24 10:51:31 ceph-node-2 mds[4167]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:30 ceph-node-4 mds[2084]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:51:30 ceph-node-3 mds[2449]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:50:31 ceph-node-5 mds[5520]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:51:12 ceph-node-2 osd[4471]: NOTICE: OSD rebalancing completed
2025-02-24 10:51:24 ceph-node-1 mds[5496]: NOTICE: Monitor map has been updated
2025-02-24 10:51:24 ceph-node-5 mon[9994]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:31 ceph-node-3 mgr[5582]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:50:53 ceph-node-4 osd[3026]: INFO: OSD disk space utilization: 70%
2025-02-24 10:51:12 ceph-node-1 client[4247]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:51:27 ceph-node-5 osd[4398]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:50:48 ceph-node-2 osd[1914]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:51:30 ceph-node-4 mds[3037]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:50:41 ceph-node-2 radosgw[1004]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:50 ceph-node-1 mon[8879]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:18 ceph-node-2 client[4536]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:50:41 ceph-node-5 radosgw[6889]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:51:15 ceph-node-3 radosgw[2028]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:09 ceph-node-1 osd[8034]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:03 ceph-node-3 mds[3485]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:31 ceph-node-5 mon[3668]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:50:45 ceph-node-1 mds[2607]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:36 ceph-node-4 osd[4093]: CRITICAL: Client connection timeout detected
2025-02-24 10:51:30 ceph-node-3 client[9330]: WARNING: Journal write failure detected on OSD node
2025-02-24 10:50:39 ceph-node-3 client[8114]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:15 ceph-node-1 mds[9905]: NOTICE: OSD rebalancing completed
2025-02-24 10:51:23 ceph-node-5 mgr[4787]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:18 ceph-node-4 osd[4255]: INFO: Monitor map has been updated
2025-02-24 10:50:38 ceph-node-2 mgr[2836]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:51:05 ceph-node-4 osd[3402]: INFO: Data replication completed for object pool
2025-02-24 10:50:53 ceph-node-5 radosgw[7047]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:27 ceph-node-4 mon[7543]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:21 ceph-node-1 mon[3103]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:51:05 ceph-node-2 radosgw[7481]: INFO: OSD disk space utilization: 70%
2025-02-24 10:50:42 ceph-node-2 client[8857]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:50:50 ceph-node-5 mon[7385]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:50:40 ceph-node-1 mds[6029]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:04 ceph-node-4 osd[5229]: CRITICAL: OSD disk space utilization exceeded 90%
2025-02-24 10:50:55 ceph-node-1 mgr[8723]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:50:56 ceph-node-3 mgr[5934]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:12 ceph-node-4 mgr[2427]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:51:16 ceph-node-1 osd[1840]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:23 ceph-node-3 mds[8076]: INFO: OSD disk space utilization: 70%
2025-02-24 10:50:40 ceph-node-4 mgr[2110]: INFO: MGR module loaded successfully
2025-02-24 10:51:00 ceph-node-3 mds[7238]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:06 ceph-node-2 mgr[1813]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:50:34 ceph-node-5 mgr[1387]: CRITICAL: Disk failure reported on ceph-node-4
2025-02-24 10:51:31 ceph-node-4 osd[5752]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:51:05 ceph-node-5 osd[7582]: DEBUG: Monitor map has been updated
2025-02-24 10:51:23 ceph-node-2 osd[9306]: CRITICAL: High I/O latency detected in radosgw service
2025-02-24 10:51:07 ceph-node-1 client[9739]: INFO: Monitor map has been updated
2025-02-24 10:50:51 ceph-node-1 osd[8747]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:03 ceph-node-5 mgr[7904]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:51:02 ceph-node-4 mds[3232]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:50:39 ceph-node-5 radosgw[3311]: INFO: Monitor map has been updated
2025-02-24 10:51:05 ceph-node-3 mgr[7644]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:50:32 ceph-node-5 mon[1283]: NOTICE: MGR module loaded successfully
2025-02-24 10:51:02 ceph-node-3 mon[2658]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:51:03 ceph-node-5 osd[8992]: NOTICE: MGR module loaded successfully
2025-02-24 10:51:12 ceph-node-2 radosgw[1004]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:01 ceph-node-5 osd[2242]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:51:02 ceph-node-1 mon[1029]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:50:37 ceph-node-3 radosgw[8065]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:50:35 ceph-node-5 client[1436]: INFO: MGR module loaded successfully
2025-02-24 10:51:21 ceph-node-3 mon[8303]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:51:29 ceph-node-2 osd[9051]: DEBUG: Data replication completed for object pool
2025-02-24 10:50:45 ceph-node-2 mds[4123]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:02 ceph-node-2 osd[9508]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:51:12 ceph-node-5 mgr[7060]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:13 ceph-node-4 client[7415]: ERROR: Slow request detected on OSD 4.3
2025-02-24 10:51:19 ceph-node-5 client[3569]: INFO: OSD rebalancing completed
2025-02-24 10:51:15 ceph-node-2 mon[6106]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:50:31 ceph-node-3 mds[5830]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:50:51 ceph-node-2 radosgw[6309]: DEBUG: MGR module loaded successfully
2025-02-24 10:50:40 ceph-node-2 mgr[4697]: INFO: Data replication completed for object pool
2025-02-24 10:50:38 ceph-node-1 client[1488]: DEBUG: OSD rebalancing completed
2025-02-24 10:50:38 ceph-node-4 osd[5028]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:09 ceph-node-4 mgr[9229]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:56 ceph-node-2 mgr[8515]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:21 ceph-node-2 client[5557]: WARNING: Monitor mon1 failed to reach quorum
2025-02-24 10:51:29 ceph-node-2 mgr[5270]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:26 ceph-node-4 mgr[4138]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:31 ceph-node-2 radosgw[5030]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:50:44 ceph-node-3 client[9862]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:10 ceph-node-5 radosgw[7016]: CRITICAL: Network partition detected between OSD nodes
2025-02-24 10:51:16 ceph-node-2 mon[6317]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:01 ceph-node-1 mds[1990]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:10 ceph-node-3 mds[8046]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:59 ceph-node-1 mon[4685]: INFO: OSD rebalancing completed
2025-02-24 10:51:04 ceph-node-1 osd[1987]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:50:34 ceph-node-4 mds[5705]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:51:02 ceph-node-4 mds[1031]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:26 ceph-node-4 mds[5668]: NOTICE: OSD rebalancing completed
2025-02-24 10:51:22 ceph-node-3 mds[4459]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:51:04 ceph-node-5 radosgw[5752]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:30 ceph-node-3 osd[3984]: DEBUG: Monitor map has been updated
2025-02-24 10:51:10 ceph-node-3 mgr[5115]: DEBUG: MGR module loaded successfully
2025-02-24 10:50:45 ceph-node-4 client[8757]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:50:39 ceph-node-1 radosgw[9990]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:50:36 ceph-node-4 client[2182]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:39 ceph-node-3 client[4945]: ERROR: Journal write failure detected on OSD node
2025-02-24 10:51:08 ceph-node-1 mds[2683]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:50:34 ceph-node-2 mgr[4819]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:50:34 ceph-node-3 mds[6387]: ERROR: CRITICAL: Data loss detected on pool 1
2025-02-24 10:50:35 ceph-node-1 mds[9541]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:50:44 ceph-node-2 mon[7420]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:51:22 ceph-node-1 mon[8223]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:51:01 ceph-node-3 mgr[8834]: NOTICE: Monitor map has been updated
2025-02-24 10:50:41 ceph-node-4 osd[2127]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:26 ceph-node-2 client[8296]: INFO: PG 1.2 replicated successfully
2025-02-24 10:51:16 ceph-node-4 osd[4793]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:05 ceph-node-1 radosgw[1169]: INFO: OSD rebalancing completed
2025-02-24 10:50:34 ceph-node-5 mds[7490]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:51 ceph-node-4 mgr[4779]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:38 ceph-node-4 osd[3560]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:51:31 ceph-node-2 mgr[7516]: ERROR: OSD 3.1 marked down due to heartbeat failure
2025-02-24 10:51:28 ceph-node-3 mds[7655]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:40 ceph-node-3 client[5295]: INFO: Data replication completed for object pool
2025-02-24 10:51:19 ceph-node-3 osd[4565]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:01 ceph-node-2 mds[2926]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:50:56 ceph-node-1 osd[7882]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:50:49 ceph-node-1 mds[1726]: DEBUG: Data replication completed for object pool
2025-02-24 10:50:46 ceph-node-4 client[3585]: WARNING: MDS failure detected: metadata inconsistency
2025-02-24 10:51:20 ceph-node-3 radosgw[7561]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:12 ceph-node-3 mgr[3131]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:50:59 ceph-node-2 radosgw[4736]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:50:41 ceph-node-5 radosgw[3266]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:50:31 ceph-node-1 mds[9842]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:10 ceph-node-1 mon[4388]: INFO: Monitor map has been updated
2025-02-24 10:51:05 ceph-node-1 mds[4125]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:50:48 ceph-node-5 mgr[9981]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:51:21 ceph-node-1 mgr[5400]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:21 ceph-node-4 mds[8341]: DEBUG: OSD rebalancing completed
2025-02-24 10:51:05 ceph-node-1 osd[9958]: DEBUG: Data replication completed for object pool
2025-02-24 10:50:40 ceph-node-3 mon[7601]: NOTICE: Data replication completed for object pool
2025-02-24 10:51:30 ceph-node-1 mon[6153]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:48 ceph-node-3 mon[2079]: INFO: MGR module loaded successfully
2025-02-24 10:51:15 ceph-node-2 osd[6902]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:30 ceph-node-5 radosgw[1226]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:51:09 ceph-node-3 osd[7033]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:08 ceph-node-2 client[3989]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:10 ceph-node-5 mon[8575]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:50:33 ceph-node-2 client[4393]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:50:46 ceph-node-5 mds[9191]: INFO: MGR module loaded successfully
2025-02-24 10:51:22 ceph-node-2 client[2751]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:51:28 ceph-node-4 radosgw[6246]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:22 ceph-node-1 mon[9574]: CRITICAL: Monitor mon1 failed to reach quorum
2025-02-24 10:50:31 ceph-node-4 mgr[4437]: NOTICE: OSD rebalancing completed
2025-02-24 10:50:49 ceph-node-2 client[8764]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:12 ceph-node-4 osd[2464]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:51:25 ceph-node-4 client[4024]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:50:57 ceph-node-1 client[7611]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:51:27 ceph-node-2 mgr[4546]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:13 ceph-node-5 mon[9296]: NOTICE: OSD rebalancing completed
2025-02-24 10:50:48 ceph-node-3 mds[1779]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:12 ceph-node-3 mon[5440]: ERROR: CRITICAL: PG 2.1 stuck in inconsistent state
2025-02-24 10:50:46 ceph-node-5 osd[3519]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:02 ceph-node-3 radosgw[2445]: DEBUG: OSD rebalancing completed
2025-02-24 10:50:52 ceph-node-2 mgr[7375]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:15 ceph-node-3 client[9174]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:07 ceph-node-5 mds[3937]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:51:22 ceph-node-2 radosgw[4972]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:03 ceph-node-4 client[4077]: DEBUG: Monitor map has been updated
2025-02-24 10:50:41 ceph-node-2 osd[1673]: NOTICE: OSD rebalancing completed
2025-02-24 10:51:07 ceph-node-1 client[5850]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:50:45 ceph-node-3 mds[9531]: DEBUG: Data replication completed for object pool
2025-02-24 10:50:45 ceph-node-5 mgr[7717]: INFO: MGR module loaded successfully
2025-02-24 10:50:47 ceph-node-2 radosgw[3545]: WARNING: Journal write failure detected on OSD node
2025-02-24 10:51:14 ceph-node-3 mds[4226]: INFO: OSD disk space utilization: 70%
2025-02-24 10:51:12 ceph-node-1 mgr[2527]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:50:40 ceph-node-4 mgr[9840]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:51:03 ceph-node-1 mon[7787]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:05 ceph-node-4 client[6920]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:50:43 ceph-node-2 radosgw[5076]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:26 ceph-node-2 mds[5449]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:51:04 ceph-node-2 radosgw[7254]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:50:33 ceph-node-5 mon[7436]: INFO: OSD rebalancing completed
2025-02-24 10:50:38 ceph-node-4 client[8755]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:06 ceph-node-4 mon[3644]: WARNING: Data corruption detected in object pool
2025-02-24 10:50:35 ceph-node-4 radosgw[4707]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:47 ceph-node-5 client[9767]: ERROR: Uncorrectable ECC error detected in MDS cache
2025-02-24 10:50:55 ceph-node-1 mon[2840]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:51:11 ceph-node-5 radosgw[2592]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:50:38 ceph-node-5 mds[3413]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:37 ceph-node-2 mds[1357]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:47 ceph-node-2 client[9502]: DEBUG: MGR module loaded successfully
2025-02-24 10:50:43 ceph-node-2 mgr[5281]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:33 ceph-node-3 mds[3892]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:51:16 ceph-node-3 mon[6755]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:35 ceph-node-1 mon[4448]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:32 ceph-node-5 mds[5021]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:41 ceph-node-1 radosgw[2259]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:29 ceph-node-2 mds[1397]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:50:42 ceph-node-1 client[2520]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:51:15 ceph-node-3 mgr[8713]: INFO: PG 1.2 replicated successfully
2025-02-24 10:50:41 ceph-node-5 mgr[6716]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:51:30 ceph-node-1 mds[7067]: WARNING: Data corruption detected in object pool
2025-02-24 10:51:04 ceph-node-4 osd[1461]: INFO: PG 1.2 replicated successfully
2025-02-24 10:50:33 ceph-node-1 mon[2694]: DEBUG: Monitor map has been updated
2025-02-24 10:51:15 ceph-node-3 mds[7996]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:50:55 ceph-node-1 radosgw[3364]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:50:37 ceph-node-4 mon[2706]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:51:14 ceph-node-5 mds[9325]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:18 ceph-node-2 radosgw[1586]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:51:31 ceph-node-2 radosgw[8027]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:40 ceph-node-3 mds[5498]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:33 ceph-node-3 radosgw[5188]: INFO: MGR module loaded successfully
2025-02-24 10:51:28 ceph-node-5 mds[7985]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:50:44 ceph-node-3 osd[6170]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:35 ceph-node-3 client[2245]: NOTICE: MGR module loaded successfully
2025-02-24 10:50:37 ceph-node-1 radosgw[6832]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:50:33 ceph-node-5 mds[7483]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:10 ceph-node-5 mgr[6082]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:51:23 ceph-node-4 client[9660]: NOTICE: OSD rebalancing completed
2025-02-24 10:51:01 ceph-node-3 osd[6999]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:48 ceph-node-2 mgr[8731]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:53 ceph-node-4 mds[4231]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:50:49 ceph-node-2 radosgw[3920]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:30 ceph-node-1 mgr[3056]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:50:47 ceph-node-2 client[6245]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:20 ceph-node-5 osd[4101]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:50:35 ceph-node-5 radosgw[4447]: NOTICE: Data replication completed for object pool
2025-02-24 10:50:46 ceph-node-1 client[1607]: INFO: MGR module loaded successfully
2025-02-24 10:51:09 ceph-node-3 client[7438]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:39 ceph-node-3 osd[3150]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:50:51 ceph-node-1 mds[8449]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:25 ceph-node-3 radosgw[2393]: INFO: MGR module loaded successfully
2025-02-24 10:50:48 ceph-node-1 mds[6370]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:40 ceph-node-4 mds[9264]: NOTICE: OSD rebalancing completed
2025-02-24 10:51:04 ceph-node-5 client[3081]: NOTICE: Monitor map has been updated
2025-02-24 10:50:39 ceph-node-1 mds[2420]: CRITICAL: Slow request detected on OSD 4.3
2025-02-24 10:51:30 ceph-node-4 radosgw[4129]: ERROR: Cluster status: HEALTH_WARN
2025-02-24 10:51:23 ceph-node-3 osd[1682]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:51:13 ceph-node-3 mon[3208]: INFO: MGR module loaded successfully
2025-02-24 10:51:28 ceph-node-1 client[2067]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:03 ceph-node-4 radosgw[8959]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:51:02 ceph-node-1 mgr[5178]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:22 ceph-node-1 mds[9474]: WARNING: Slow request detected on OSD 4.3
2025-02-24 10:50:52 ceph-node-3 mgr[5862]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:50:40 ceph-node-3 mds[7891]: CRITICAL: Journal write failure detected on OSD node
2025-02-24 10:51:21 ceph-node-5 mds[5147]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:25 ceph-node-2 mgr[6384]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:51:07 ceph-node-5 mon[3518]: INFO: PG 1.2 replicated successfully
2025-02-24 10:50:41 ceph-node-1 radosgw[5915]: DEBUG: Monitor map has been updated
2025-02-24 10:51:13 ceph-node-4 radosgw[9170]: DEBUG: OSD rebalancing completed
2025-02-24 10:50:53 ceph-node-4 radosgw[6580]: INFO: MGR module loaded successfully
2025-02-24 10:50:46 ceph-node-1 radosgw[6799]: INFO: Data replication completed for object pool
2025-02-24 10:51:20 ceph-node-3 osd[4132]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:51:02 ceph-node-3 mgr[5396]: INFO: OSD disk space utilization: 70%
2025-02-24 10:51:01 ceph-node-1 mgr[6284]: NOTICE: MGR module loaded successfully
2025-02-24 10:50:52 ceph-node-5 mgr[2826]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:50:51 ceph-node-5 osd[9558]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:50:35 ceph-node-2 mgr[3147]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:51:22 ceph-node-2 radosgw[7190]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:01 ceph-node-3 mds[8429]: CRITICAL: OSD 3.1 marked down due to heartbeat failure
2025-02-24 10:50:35 ceph-node-1 osd[2384]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:50:38 ceph-node-4 mds[7500]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:50:46 ceph-node-3 osd[2590]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:01 ceph-node-2 osd[1045]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:50:45 ceph-node-2 mds[8648]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:50:46 ceph-node-4 mgr[8472]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:24 ceph-node-2 mon[2032]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:51:17 ceph-node-4 client[9520]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:51:11 ceph-node-2 radosgw[7407]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:22 ceph-node-5 osd[1280]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:50:59 ceph-node-2 mon[1114]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:25 ceph-node-5 mon[6390]: CRITICAL: Uncorrectable ECC error detected in MDS cache
2025-02-24 10:50:42 ceph-node-5 mon[9546]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:49 ceph-node-5 osd[6029]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:50:46 ceph-node-4 mgr[1005]: INFO: OSD disk space utilization: 70%
2025-02-24 10:50:39 ceph-node-3 mds[1650]: CRITICAL: MDS failure detected: metadata inconsistency
2025-02-24 10:51:31 ceph-node-4 client[1761]: DEBUG: OSD rebalancing completed
2025-02-24 10:51:23 ceph-node-3 mds[8333]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:35 ceph-node-2 mon[2133]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:44 ceph-node-1 mgr[3054]: WARNING: Journal write failure detected on OSD node
2025-02-24 10:50:47 ceph-node-1 osd[7896]: ERROR: OSD disk space utilization exceeded 90%
2025-02-24 10:51:18 ceph-node-4 radosgw[3201]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:51:29 ceph-node-4 radosgw[3422]: DEBUG: Data replication completed for object pool
2025-02-24 10:50:42 ceph-node-4 mgr[8862]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:51:03 ceph-node-4 mds[7761]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:10 ceph-node-2 mgr[2027]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:29 ceph-node-1 osd[2404]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:46 ceph-node-5 radosgw[3448]: ERROR: Network partition detected between OSD nodes
2025-02-24 10:50:56 ceph-node-3 mds[7366]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:01 ceph-node-3 mgr[4562]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:19 ceph-node-5 mgr[5152]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:07 ceph-node-1 osd[3650]: NOTICE: Monitor map has been updated
2025-02-24 10:51:00 ceph-node-4 radosgw[7143]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:06 ceph-node-4 osd[8367]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:51:16 ceph-node-1 mon[8440]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:51:14 ceph-node-4 mgr[5175]: NOTICE: OSD rebalancing completed
2025-02-24 10:50:43 ceph-node-3 radosgw[1413]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:05 ceph-node-2 mds[8152]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:50:58 ceph-node-5 radosgw[4832]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:07 ceph-node-2 radosgw[3747]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:50:47 ceph-node-4 mds[2505]: WARNING: Authentication failure in RADOS Gateway
2025-02-24 10:51:27 ceph-node-2 client[4706]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:51:16 ceph-node-4 client[9219]: WARNING: Monitor down, unable to connect to quorum
2025-02-24 10:51:10 ceph-node-3 mgr[5880]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:01 ceph-node-5 mgr[6373]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:28 ceph-node-1 client[4671]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:37 ceph-node-2 mon[1249]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:50:33 ceph-node-5 mds[7133]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:22 ceph-node-2 mds[4011]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:50:46 ceph-node-3 client[2897]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:19 ceph-node-1 mon[8662]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:07 ceph-node-3 mgr[1889]: CRITICAL: Monitor down, unable to connect to quorum
2025-02-24 10:50:48 ceph-node-4 osd[6569]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:51:23 ceph-node-1 radosgw[9786]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:10 ceph-node-3 radosgw[8693]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:16 ceph-node-5 mon[4247]: NOTICE: MGR module loaded successfully
2025-02-24 10:51:15 ceph-node-2 osd[7906]: INFO: OSD disk space utilization: 70%
2025-02-24 10:50:51 ceph-node-4 mds[7443]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:02 ceph-node-1 mds[1932]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:17 ceph-node-4 mon[9936]: NOTICE: Data replication completed for object pool
2025-02-24 10:51:11 ceph-node-1 osd[2047]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:51:22 ceph-node-2 osd[6974]: ERROR: Journal write failure detected on OSD node
2025-02-24 10:50:56 ceph-node-3 client[6801]: INFO: MGR module loaded successfully
2025-02-24 10:50:58 ceph-node-3 mgr[7037]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:16 ceph-node-5 mds[3703]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:50:53 ceph-node-3 radosgw[8520]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:10 ceph-node-2 client[2665]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:05 ceph-node-1 radosgw[2713]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:13 ceph-node-2 mon[7074]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:25 ceph-node-4 mon[4578]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:08 ceph-node-1 client[2515]: WARNING: Client connection timeout detected
2025-02-24 10:51:10 ceph-node-2 client[3423]: INFO: Data replication completed for object pool
2025-02-24 10:50:58 ceph-node-4 mds[4808]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:50:42 ceph-node-2 osd[9719]: CRITICAL: MDS failure detected: metadata inconsistency
2025-02-24 10:51:10 ceph-node-2 osd[8080]: INFO: OSD disk space utilization: 70%
2025-02-24 10:51:22 ceph-node-1 mgr[4836]: INFO: MGR module loaded successfully
2025-02-24 10:50:58 ceph-node-2 osd[3277]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:50:55 ceph-node-2 radosgw[4492]: NOTICE: Data replication completed for object pool
2025-02-24 10:50:35 ceph-node-5 client[3286]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:50:37 ceph-node-2 mgr[2371]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:28 ceph-node-2 osd[2045]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:51:31 ceph-node-3 mds[7788]: INFO: PG 1.2 replicated successfully
2025-02-24 10:51:13 ceph-node-4 mon[4982]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:03 ceph-node-2 radosgw[1298]: INFO: OSD disk space utilization: 70%
2025-02-24 10:51:25 ceph-node-5 mon[6052]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:24 ceph-node-4 osd[4777]: DEBUG: OSD rebalancing completed
2025-02-24 10:51:15 ceph-node-5 mds[1944]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:51:31 ceph-node-2 mgr[8260]: INFO: Data replication completed for object pool
2025-02-24 10:50:50 ceph-node-4 client[2055]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:50:58 ceph-node-4 mon[4605]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:50:46 ceph-node-3 mds[7057]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:22 ceph-node-2 radosgw[6981]: DEBUG: Monitor map has been updated
2025-02-24 10:51:27 ceph-node-5 mon[7962]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:50:38 ceph-node-5 mds[3188]: INFO: OSD rebalancing completed
2025-02-24 10:51:29 ceph-node-1 radosgw[4986]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:50:49 ceph-node-2 mon[5049]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:50:55 ceph-node-1 client[5862]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:07 ceph-node-2 osd[9001]: ERROR: Slow request detected on OSD 4.3
2025-02-24 10:50:35 ceph-node-2 radosgw[5557]: INFO: Monitor map has been updated
2025-02-24 10:50:34 ceph-node-3 osd[7828]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:21 ceph-node-4 client[5014]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:51:00 ceph-node-4 mon[2567]: INFO: MGR module loaded successfully
2025-02-24 10:51:07 ceph-node-3 mds[3284]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:30 ceph-node-4 mgr[8437]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:50:56 ceph-node-5 client[1627]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:50:51 ceph-node-3 radosgw[4022]: NOTICE: MGR module loaded successfully
2025-02-24 10:50:48 ceph-node-4 radosgw[2036]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:26 ceph-node-4 client[9195]: NOTICE: OSD rebalancing completed
2025-02-24 10:51:27 ceph-node-2 radosgw[5316]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:50:55 ceph-node-1 mgr[7581]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:13 ceph-node-5 osd[5972]: INFO: OSD disk space utilization: 70%
2025-02-24 10:51:12 ceph-node-5 osd[4738]: DEBUG: Data replication completed for object pool
2025-02-24 10:50:35 ceph-node-2 osd[9626]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:50:45 ceph-node-1 radosgw[6610]: ERROR: Monitor mon1 failed to reach quorum
2025-02-24 10:50:46 ceph-node-4 mon[1851]: INFO: OSD disk space utilization: 70%
2025-02-24 10:50:51 ceph-node-3 radosgw[2998]: INFO: OSD disk space utilization: 70%
2025-02-24 10:50:52 ceph-node-3 client[1354]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:37 ceph-node-3 mgr[8487]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:51:06 ceph-node-1 client[8136]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:50:42 ceph-node-2 osd[2867]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:21 ceph-node-2 client[6623]: CRITICAL: OSD disk space utilization exceeded 90%
2025-02-24 10:50:32 ceph-node-2 radosgw[7704]: DEBUG: MGR module loaded successfully
2025-02-24 10:50:34 ceph-node-1 client[8800]: NOTICE: Data replication completed for object pool
2025-02-24 10:51:18 ceph-node-2 mgr[4905]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:50:51 ceph-node-1 mgr[8085]: WARNING: OSD disk space utilization exceeded 90%
2025-02-24 10:50:42 ceph-node-4 mds[4750]: WARNING: Network partition detected between OSD nodes
2025-02-24 10:51:05 ceph-node-4 client[8433]: WARNING: Client connection timeout detected
2025-02-24 10:51:06 ceph-node-1 mon[5558]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:50:42 ceph-node-5 mds[3002]: ERROR: CRITICAL: PG 2.1 stuck in inconsistent state
2025-02-24 10:51:25 ceph-node-2 mon[8185]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:51:01 ceph-node-1 osd[9253]: CRITICAL: Cluster status: HEALTH_WARN
2025-02-24 10:50:58 ceph-node-2 osd[1647]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:23 ceph-node-3 client[9925]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:04 ceph-node-1 client[1003]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:50:49 ceph-node-3 mon[4795]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:36 ceph-node-2 mgr[1799]: WARNING: Network partition detected between OSD nodes
2025-02-24 10:50:57 ceph-node-4 mgr[7559]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:50:36 ceph-node-4 mds[1178]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:57 ceph-node-5 osd[1459]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:17 ceph-node-3 client[3795]: ERROR: Journal write failure detected on OSD node
2025-02-24 10:50:36 ceph-node-4 osd[5348]: NOTICE: Data replication completed for object pool
2025-02-24 10:51:30 ceph-node-2 radosgw[4016]: DEBUG: Monitor map has been updated
2025-02-24 10:51:22 ceph-node-5 mgr[5097]: INFO: PG 1.2 replicated successfully
2025-02-24 10:51:28 ceph-node-3 mds[6826]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:42 ceph-node-3 mgr[7674]: WARNING: Slow request detected on OSD 4.3
2025-02-24 10:50:48 ceph-node-5 client[7160]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:37 ceph-node-1 osd[6900]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:39 ceph-node-4 mds[7360]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:30 ceph-node-4 mds[9996]: INFO: OSD rebalancing completed
2025-02-24 10:50:45 ceph-node-4 mgr[9165]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:16 ceph-node-2 mon[4271]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:09 ceph-node-3 osd[1830]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:52 ceph-node-3 mds[2674]: NOTICE: OSD rebalancing completed
2025-02-24 10:50:42 ceph-node-4 osd[9272]: WARNING: Authentication failure in RADOS Gateway
2025-02-24 10:50:56 ceph-node-1 client[1135]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:33 ceph-node-3 mon[1053]: DEBUG: OSD rebalancing completed
2025-02-24 10:51:28 ceph-node-2 mgr[1398]: INFO: Monitor map has been updated
2025-02-24 10:50:51 ceph-node-1 osd[8795]: DEBUG: Data replication completed for object pool
2025-02-24 10:50:50 ceph-node-3 client[4484]: INFO: OSD disk space utilization: 70%
2025-02-24 10:50:52 ceph-node-2 mgr[8349]: NOTICE: OSD rebalancing completed
2025-02-24 10:50:38 ceph-node-4 radosgw[9382]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:50:34 ceph-node-2 mgr[5841]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:50:40 ceph-node-1 mgr[2329]: NOTICE: MGR module loaded successfully
2025-02-24 10:51:28 ceph-node-4 mon[3851]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:47 ceph-node-3 mds[2775]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:00 ceph-node-2 client[4210]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:14 ceph-node-1 osd[9875]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:50:44 ceph-node-2 osd[9689]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:51:18 ceph-node-4 osd[5774]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:52 ceph-node-2 radosgw[2661]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:51:14 ceph-node-1 osd[5527]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:59 ceph-node-4 radosgw[4551]: DEBUG: Monitor map has been updated
2025-02-24 10:50:43 ceph-node-4 radosgw[3423]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:23 ceph-node-3 osd[8066]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:14 ceph-node-2 radosgw[4344]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:06 ceph-node-3 mgr[9100]: INFO: PG 1.2 replicated successfully
2025-02-24 10:51:07 ceph-node-3 mgr[1823]: CRITICAL: Cluster status: HEALTH_WARN
2025-02-24 10:51:11 ceph-node-3 client[4064]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:59 ceph-node-5 mon[5078]: NOTICE: OSD rebalancing completed
2025-02-24 10:51:10 ceph-node-5 radosgw[5952]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:50:51 ceph-node-5 osd[9176]: ERROR: Journal write failure detected on OSD node
2025-02-24 10:50:33 ceph-node-1 mds[9991]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:51 ceph-node-1 osd[3720]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:25 ceph-node-2 mgr[8046]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:49 ceph-node-2 client[7059]: DEBUG: Monitor map has been updated
2025-02-24 10:51:11 ceph-node-3 mds[4598]: NOTICE: OSD rebalancing completed
2025-02-24 10:51:07 ceph-node-3 radosgw[5130]: INFO: OSD disk space utilization: 70%
2025-02-24 10:51:12 ceph-node-2 osd[2300]: INFO: Monitor map has been updated
2025-02-24 10:50:32 ceph-node-3 osd[5404]: CRITICAL: Journal write failure detected on OSD node
2025-02-24 10:50:46 ceph-node-4 client[1925]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:51:09 ceph-node-3 mon[8357]: INFO: Data replication completed for object pool
2025-02-24 10:50:58 ceph-node-1 mgr[1970]: INFO: MGR module loaded successfully
2025-02-24 10:51:12 ceph-node-2 mon[4102]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:09 ceph-node-1 client[1048]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:51:21 ceph-node-1 radosgw[5048]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:50:58 ceph-node-5 client[4417]: INFO: Data replication completed for object pool
2025-02-24 10:50:50 ceph-node-4 radosgw[9977]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:20 ceph-node-3 radosgw[6207]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:48 ceph-node-5 mds[7649]: DEBUG: OSD rebalancing completed
2025-02-24 10:50:57 ceph-node-5 osd[5368]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:08 ceph-node-2 mgr[7013]: INFO: MGR module loaded successfully
2025-02-24 10:51:06 ceph-node-1 radosgw[2411]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:51:21 ceph-node-2 mon[2282]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:05 ceph-node-4 radosgw[8612]: INFO: OSD disk space utilization: 70%
2025-02-24 10:50:42 ceph-node-2 radosgw[4885]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:12 ceph-node-5 mgr[6998]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:51:02 ceph-node-3 mgr[5040]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:51:00 ceph-node-2 mds[4484]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:50:55 ceph-node-1 mgr[9025]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:50:36 ceph-node-1 client[6967]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:54 ceph-node-4 client[5156]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:51:08 ceph-node-4 mon[8710]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:22 ceph-node-4 mds[7601]: WARNING: Journal write failure detected on OSD node
2025-02-24 10:50:42 ceph-node-5 osd[5263]: WARNING: OSD 3.1 marked down due to heartbeat failure
2025-02-24 10:50:33 ceph-node-2 mds[8356]: DEBUG: MGR module loaded successfully
2025-02-24 10:50:57 ceph-node-1 client[6956]: NOTICE: MGR module loaded successfully
2025-02-24 10:51:27 ceph-node-1 mgr[5105]: ERROR: Cluster status: HEALTH_WARN
2025-02-24 10:50:40 ceph-node-1 mon[8532]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:01 ceph-node-5 client[9684]: NOTICE: Monitor map has been updated
2025-02-24 10:51:00 ceph-node-2 osd[1869]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:50:31 ceph-node-2 osd[6429]: DEBUG: Monitor map has been updated
2025-02-24 10:50:32 ceph-node-3 mon[3423]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:12 ceph-node-3 osd[2291]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:50:45 ceph-node-2 client[7988]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:51:11 ceph-node-2 client[9622]: NOTICE: Data replication completed for object pool
2025-02-24 10:50:50 ceph-node-1 mds[9868]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:51:24 ceph-node-3 client[9860]: NOTICE: Data replication completed for object pool
2025-02-24 10:51:15 ceph-node-5 mds[2168]: INFO: OSD disk space utilization: 70%
2025-02-24 10:51:17 ceph-node-1 mgr[8975]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:50:56 ceph-node-5 client[8465]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:22 ceph-node-4 mds[4550]: INFO: MGR module loaded successfully
2025-02-24 10:50:50 ceph-node-4 mds[6171]: INFO: Monitor map has been updated
2025-02-24 10:50:42 ceph-node-4 radosgw[4825]: NOTICE: Data replication completed for object pool
2025-02-24 10:50:43 ceph-node-5 radosgw[1585]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:26 ceph-node-3 radosgw[5762]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:51:05 ceph-node-5 mgr[9921]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:18 ceph-node-2 mgr[7856]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:38 ceph-node-4 osd[5419]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:17 ceph-node-3 client[9839]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:50:54 ceph-node-4 mon[9189]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:28 ceph-node-2 client[6418]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:51:28 ceph-node-2 radosgw[7452]: CRITICAL: Uncorrectable ECC error detected in MDS cache
2025-02-24 10:50:56 ceph-node-2 mon[1196]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:51:08 ceph-node-4 radosgw[7833]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:07 ceph-node-3 osd[7021]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:25 ceph-node-5 mon[2595]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:50:55 ceph-node-3 mds[1938]: INFO: MGR module loaded successfully
2025-02-24 10:50:46 ceph-node-4 client[5114]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:31 ceph-node-1 client[3600]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:50:39 ceph-node-2 mon[4456]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:27 ceph-node-3 mds[1473]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:24 ceph-node-1 mds[4995]: INFO: PG 1.2 replicated successfully
2025-02-24 10:50:33 ceph-node-1 mon[4535]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:50:31 ceph-node-2 osd[9043]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:50:45 ceph-node-4 mgr[6481]: INFO: Data replication completed for object pool
2025-02-24 10:51:29 ceph-node-1 client[3156]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:51:06 ceph-node-4 mgr[3099]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:51:07 ceph-node-4 client[1996]: INFO: PG 1.2 replicated successfully
2025-02-24 10:50:54 ceph-node-4 osd[7231]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:52 ceph-node-5 mds[9962]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:24 ceph-node-2 mgr[5020]: ERROR: Disk failure reported on ceph-node-4
2025-02-24 10:50:57 ceph-node-4 mgr[3263]: NOTICE: OSD rebalancing completed
2025-02-24 10:50:33 ceph-node-1 mgr[1412]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:21 ceph-node-2 mds[4318]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:21 ceph-node-4 client[8286]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:43 ceph-node-4 mgr[3571]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:06 ceph-node-5 mds[9251]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:50:41 ceph-node-4 mgr[7203]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:50:42 ceph-node-5 mds[9310]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:55 ceph-node-2 mon[4939]: NOTICE: Monitor map has been updated
2025-02-24 10:50:55 ceph-node-4 mon[3557]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:20 ceph-node-4 mds[7226]: NOTICE: Monitor map has been updated
2025-02-24 10:50:34 ceph-node-5 client[1447]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:02 ceph-node-2 mds[6636]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:51:14 ceph-node-4 mon[3068]: DEBUG: Monitor map has been updated
2025-02-24 10:51:05 ceph-node-5 mds[5659]: INFO: MGR module loaded successfully
2025-02-24 10:50:52 ceph-node-3 mgr[8635]: WARNING: OSD disk space utilization exceeded 90%
2025-02-24 10:50:59 ceph-node-5 radosgw[4731]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:41 ceph-node-2 mds[5897]: INFO: Monitor map has been updated
2025-02-24 10:50:44 ceph-node-2 mds[2720]: INFO: MGR module loaded successfully
2025-02-24 10:51:03 ceph-node-1 client[3954]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:31 ceph-node-1 radosgw[2258]: CRITICAL: OSD disk space utilization exceeded 90%
2025-02-24 10:51:28 ceph-node-3 radosgw[7288]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:51:03 ceph-node-4 mgr[4061]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:51:21 ceph-node-3 mgr[6466]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:05 ceph-node-2 mds[3970]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:43 ceph-node-1 osd[9433]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:17 ceph-node-3 radosgw[5188]: DEBUG: Data replication completed for object pool
2025-02-24 10:50:53 ceph-node-2 mgr[5196]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:51:31 ceph-node-3 mon[6385]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:08 ceph-node-1 mon[7960]: INFO: Monitor map has been updated
2025-02-24 10:51:22 ceph-node-4 mgr[6592]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:50:49 ceph-node-1 radosgw[4181]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:50:47 ceph-node-1 osd[8000]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:50:43 ceph-node-5 osd[3766]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:27 ceph-node-3 client[5969]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:30 ceph-node-5 osd[6334]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:51:20 ceph-node-4 osd[9742]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:50:55 ceph-node-1 mds[1963]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:12 ceph-node-2 osd[6721]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:54 ceph-node-2 mgr[8941]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:44 ceph-node-5 mon[7990]: NOTICE: OSD rebalancing completed
2025-02-24 10:51:15 ceph-node-5 osd[1708]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:51:18 ceph-node-4 mgr[4737]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:16 ceph-node-4 mgr[5553]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:13 ceph-node-4 mon[2628]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:02 ceph-node-2 mon[8806]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:51:27 ceph-node-5 mon[6295]: DEBUG: Data replication completed for object pool
2025-02-24 10:50:45 ceph-node-2 osd[9527]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:50:31 ceph-node-5 mds[8131]: INFO: MGR module loaded successfully
2025-02-24 10:50:40 ceph-node-4 mon[8465]: INFO: MGR module loaded successfully
2025-02-24 10:50:41 ceph-node-4 mon[7604]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:03 ceph-node-4 osd[3154]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:50:39 ceph-node-5 osd[5507]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:43 ceph-node-1 radosgw[2623]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:16 ceph-node-4 mon[1533]: DEBUG: OSD rebalancing completed
2025-02-24 10:51:30 ceph-node-4 mgr[5674]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:50:38 ceph-node-3 mon[7127]: INFO: PG 1.2 replicated successfully
2025-02-24 10:50:34 ceph-node-1 mgr[8699]: DEBUG: Data replication completed for object pool
2025-02-24 10:50:58 ceph-node-5 radosgw[8548]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:50:42 ceph-node-5 mds[4985]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:24 ceph-node-3 mds[6674]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:50:59 ceph-node-3 mds[8309]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:51:13 ceph-node-4 osd[3476]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:51:23 ceph-node-1 radosgw[3990]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:51:10 ceph-node-5 osd[7335]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:27 ceph-node-3 mds[7596]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:57 ceph-node-3 mgr[3583]: NOTICE: Data replication completed for object pool
2025-02-24 10:51:25 ceph-node-5 mds[3718]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:10 ceph-node-4 radosgw[2216]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:03 ceph-node-3 radosgw[1886]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:50:52 ceph-node-1 mon[4860]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:37 ceph-node-4 client[1955]: CRITICAL: Monitor mon1 failed to reach quorum
2025-02-24 10:50:50 ceph-node-1 osd[1803]: CRITICAL: OSD disk space utilization exceeded 90%
2025-02-24 10:50:39 ceph-node-5 mgr[2796]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:51:13 ceph-node-1 client[9744]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:50:31 ceph-node-3 mgr[3684]: DEBUG: Monitor map has been updated
2025-02-24 10:50:32 ceph-node-4 mgr[1301]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:51:31 ceph-node-5 radosgw[2625]: INFO: MGR module loaded successfully
2025-02-24 10:51:30 ceph-node-5 client[3331]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:26 ceph-node-2 radosgw[3690]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:08 ceph-node-4 radosgw[5014]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:51:19 ceph-node-4 radosgw[2554]: INFO: PG 1.2 replicated successfully
2025-02-24 10:50:48 ceph-node-1 mds[6696]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:50:38 ceph-node-1 radosgw[3853]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:51:30 ceph-node-3 radosgw[9401]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:50:53 ceph-node-5 mgr[8433]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:50:48 ceph-node-3 mds[8214]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:51:25 ceph-node-4 mon[9299]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:50:31 ceph-node-5 radosgw[8907]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:50:44 ceph-node-4 osd[2105]: DEBUG: OSD rebalancing completed
2025-02-24 10:50:31 ceph-node-5 client[5162]: INFO: Monitor map has been updated
2025-02-24 10:51:03 ceph-node-4 radosgw[7602]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:51:17 ceph-node-1 client[7555]: ERROR: Monitor mon1 failed to reach quorum
2025-02-24 10:51:06 ceph-node-2 mds[4966]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:50:49 ceph-node-3 radosgw[1032]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:42 ceph-node-4 osd[5174]: INFO: Monitor map has been updated
2025-02-24 10:50:54 ceph-node-1 client[1216]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:08 ceph-node-4 mgr[4796]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:35 ceph-node-5 radosgw[5697]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:50:58 ceph-node-1 mon[3676]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:50:48 ceph-node-5 mds[5576]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:50:38 ceph-node-4 client[2188]: ERROR: High I/O latency detected in radosgw service
2025-02-24 10:51:08 ceph-node-1 mon[8446]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:31 ceph-node-3 mgr[3750]: WARNING: MDS failure detected: metadata inconsistency
2025-02-24 10:50:31 ceph-node-5 mds[3985]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:51:18 ceph-node-2 client[1684]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:18 ceph-node-1 mon[2543]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:50:51 ceph-node-4 radosgw[4038]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:50:52 ceph-node-1 mds[7543]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:51:02 ceph-node-4 mds[4266]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:29 ceph-node-3 mgr[8412]: INFO: OSD rebalancing completed
2025-02-24 10:50:48 ceph-node-2 mon[7805]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:07 ceph-node-2 mds[2785]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:25 ceph-node-5 mon[5369]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:18 ceph-node-1 radosgw[9199]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:50:42 ceph-node-3 mgr[3206]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:51:09 ceph-node-4 client[7404]: INFO: OSD rebalancing completed
2025-02-24 10:51:23 ceph-node-4 radosgw[5211]: NOTICE: OSD rebalancing completed
2025-02-24 10:50:57 ceph-node-5 mds[1772]: CRITICAL: Network partition detected between OSD nodes
2025-02-24 10:50:37 ceph-node-2 client[2047]: NOTICE: MGR module loaded successfully
2025-02-24 10:51:03 ceph-node-1 mgr[7397]: ERROR: Journal write failure detected on OSD node
2025-02-24 10:51:05 ceph-node-4 osd[2727]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:18 ceph-node-1 mgr[7990]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:34 ceph-node-4 client[3059]: INFO: PG 1.2 replicated successfully
2025-02-24 10:50:36 ceph-node-4 mgr[9359]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:50:52 ceph-node-4 mon[7140]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:14 ceph-node-1 mgr[1405]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:46 ceph-node-2 mgr[8358]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:21 ceph-node-4 mds[6833]: INFO: PG 1.2 replicated successfully
2025-02-24 10:51:28 ceph-node-3 mgr[4257]: ERROR: Disk failure reported on ceph-node-4
2025-02-24 10:50:46 ceph-node-2 osd[3650]: DEBUG: Monitor map has been updated
2025-02-24 10:51:29 ceph-node-2 mds[2228]: ERROR: Journal write failure detected on OSD node
2025-02-24 10:51:00 ceph-node-1 mgr[5226]: CRITICAL: Network partition detected between OSD nodes
2025-02-24 10:50:40 ceph-node-4 client[8574]: NOTICE: MGR module loaded successfully
2025-02-24 10:51:01 ceph-node-2 mds[7053]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:56 ceph-node-2 radosgw[2332]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:50:58 ceph-node-2 mds[1336]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:50:42 ceph-node-2 mgr[6733]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:51:30 ceph-node-5 radosgw[9355]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:51:03 ceph-node-5 osd[8481]: DEBUG: OSD rebalancing completed
2025-02-24 10:51:00 ceph-node-1 radosgw[7365]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:50:42 ceph-node-2 mds[1861]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:10 ceph-node-2 osd[6576]: INFO: PG 1.2 replicated successfully
2025-02-24 10:50:47 ceph-node-4 osd[8664]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:03 ceph-node-1 client[7745]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:38 ceph-node-4 radosgw[7093]: ERROR: Data corruption detected in object pool
2025-02-24 10:51:15 ceph-node-2 osd[7983]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:36 ceph-node-3 radosgw[5790]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:03 ceph-node-4 mds[7151]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:49 ceph-node-3 mgr[1736]: NOTICE: MGR module loaded successfully
2025-02-24 10:50:42 ceph-node-3 client[2366]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:51:26 ceph-node-4 mgr[9934]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:51 ceph-node-1 osd[4799]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:51:27 ceph-node-2 radosgw[4798]: NOTICE: Data replication completed for object pool
2025-02-24 10:51:08 ceph-node-4 radosgw[2124]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:51:05 ceph-node-5 osd[4705]: ERROR: Client connection timeout detected
2025-02-24 10:50:36 ceph-node-4 mon[2827]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:25 ceph-node-4 radosgw[5997]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:43 ceph-node-3 mgr[5191]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:51:12 ceph-node-1 osd[5629]: INFO: Monitor map has been updated
2025-02-24 10:51:27 ceph-node-2 mon[5240]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:40 ceph-node-4 radosgw[7085]: DEBUG: Monitor map has been updated
2025-02-24 10:50:37 ceph-node-4 osd[6493]: NOTICE: OSD rebalancing completed
2025-02-24 10:50:48 ceph-node-2 mgr[4399]: NOTICE: Data replication completed for object pool
2025-02-24 10:51:06 ceph-node-1 osd[3061]: DEBUG: OSD rebalancing completed
2025-02-24 10:51:19 ceph-node-3 mon[6638]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:27 ceph-node-2 osd[5671]: INFO: PG 1.2 replicated successfully
2025-02-24 10:51:05 ceph-node-3 mgr[8452]: ERROR: OSD 3.1 marked down due to heartbeat failure
2025-02-24 10:50:49 ceph-node-4 client[1130]: INFO: PG 1.2 replicated successfully
2025-02-24 10:50:34 ceph-node-4 mon[2197]: NOTICE: MGR module loaded successfully
2025-02-24 10:51:29 ceph-node-3 radosgw[7965]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:59 ceph-node-1 mon[7927]: NOTICE: MGR module loaded successfully
2025-02-24 10:50:53 ceph-node-5 client[8038]: INFO: Data replication completed for object pool
2025-02-24 10:50:49 ceph-node-3 mgr[7572]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:50:32 ceph-node-1 mgr[7096]: DEBUG: Data replication completed for object pool
2025-02-24 10:50:57 ceph-node-2 client[2938]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:54 ceph-node-5 radosgw[5253]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:00 ceph-node-2 radosgw[3789]: DEBUG: Monitor map has been updated
2025-02-24 10:50:34 ceph-node-4 client[9155]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:58 ceph-node-4 mds[9935]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:08 ceph-node-1 client[5566]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:17 ceph-node-2 mds[8948]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:00 ceph-node-3 mds[1576]: INFO: Data replication completed for object pool
2025-02-24 10:50:54 ceph-node-3 mgr[2998]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:27 ceph-node-5 radosgw[7542]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:12 ceph-node-1 osd[9222]: DEBUG: Monitor map has been updated
2025-02-24 10:51:10 ceph-node-3 radosgw[5718]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:50:55 ceph-node-3 mgr[6160]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:24 ceph-node-1 mds[6855]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:50:59 ceph-node-1 mgr[8336]: INFO: PG 1.2 replicated successfully
2025-02-24 10:51:29 ceph-node-1 client[8989]: ERROR: CRITICAL: Data loss detected on pool 1
2025-02-24 10:51:20 ceph-node-1 osd[8304]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:50:58 ceph-node-4 radosgw[1156]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:51:18 ceph-node-4 radosgw[3479]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:07 ceph-node-4 radosgw[3183]: INFO: OSD disk space utilization: 70%
2025-02-24 10:50:43 ceph-node-4 mon[2617]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:51:00 ceph-node-4 mon[3101]: DEBUG: Monitor map has been updated
2025-02-24 10:51:13 ceph-node-1 radosgw[8931]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:51:02 ceph-node-3 mds[6346]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:21 ceph-node-2 radosgw[4770]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:00 ceph-node-4 client[7060]: ERROR: OSD disk space utilization exceeded 90%
2025-02-24 10:50:44 ceph-node-2 mgr[5847]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:50:55 ceph-node-2 mon[2446]: NOTICE: OSD rebalancing completed
2025-02-24 10:51:09 ceph-node-1 radosgw[3024]: NOTICE: Monitor map has been updated
2025-02-24 10:51:26 ceph-node-5 mgr[2331]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:50:38 ceph-node-4 mon[7990]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:57 ceph-node-5 osd[2352]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:26 ceph-node-5 client[5024]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:50:55 ceph-node-5 mon[8184]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:50:41 ceph-node-1 osd[6473]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:05 ceph-node-3 mgr[4835]: NOTICE: OSD rebalancing completed
2025-02-24 10:50:37 ceph-node-3 mon[6237]: NOTICE: OSD rebalancing completed
2025-02-24 10:51:17 ceph-node-4 radosgw[8876]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:50:50 ceph-node-1 osd[2010]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:11 ceph-node-4 mon[7397]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:50:32 ceph-node-3 mgr[1083]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:51:29 ceph-node-5 mds[5904]: DEBUG: Monitor map has been updated
2025-02-24 10:51:01 ceph-node-5 radosgw[9238]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:50:34 ceph-node-1 osd[5653]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:00 ceph-node-2 mds[1615]: ERROR: Slow request detected on OSD 4.3
2025-02-24 10:50:52 ceph-node-3 mon[5573]: INFO: PG 1.2 replicated successfully
2025-02-24 10:50:51 ceph-node-2 radosgw[3654]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:51:21 ceph-node-2 mon[4509]: INFO: OSD disk space utilization: 70%
2025-02-24 10:51:17 ceph-node-2 radosgw[7515]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:57 ceph-node-4 mgr[5764]: CRITICAL: OSD 3.1 marked down due to heartbeat failure
2025-02-24 10:50:45 ceph-node-5 radosgw[3158]: INFO: MGR module loaded successfully
2025-02-24 10:51:29 ceph-node-4 mds[8706]: CRITICAL: Monitor down, unable to connect to quorum
2025-02-24 10:50:58 ceph-node-3 mgr[6544]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:50:40 ceph-node-2 client[5476]: NOTICE: Data replication completed for object pool
2025-02-24 10:51:06 ceph-node-1 client[3404]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:50:56 ceph-node-1 osd[8458]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:56 ceph-node-2 osd[6620]: ERROR: Disk failure reported on ceph-node-4
2025-02-24 10:50:49 ceph-node-2 mon[6647]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:50:52 ceph-node-4 client[2036]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:16 ceph-node-5 radosgw[5063]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:51 ceph-node-2 client[2540]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:42 ceph-node-2 client[2207]: INFO: Monitor map has been updated
2025-02-24 10:51:24 ceph-node-5 client[1493]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:50:39 ceph-node-4 mds[3817]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:11 ceph-node-5 mgr[1430]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:09 ceph-node-2 osd[5580]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:15 ceph-node-5 mon[5116]: ERROR: High I/O latency detected in radosgw service
2025-02-24 10:51:12 ceph-node-4 mds[4799]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:20 ceph-node-5 mds[8613]: NOTICE: MGR module loaded successfully
2025-02-24 10:50:33 ceph-node-2 osd[9440]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:51:06 ceph-node-2 osd[9534]: INFO: OSD rebalancing completed
2025-02-24 10:51:22 ceph-node-3 mgr[4099]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:37 ceph-node-2 mds[1527]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:05 ceph-node-2 mon[3538]: INFO: MGR module loaded successfully
2025-02-24 10:50:42 ceph-node-4 osd[6780]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:50:34 ceph-node-3 mon[6513]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:33 ceph-node-3 osd[1461]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:41 ceph-node-4 osd[8841]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:51:27 ceph-node-1 mds[1028]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:50:59 ceph-node-1 mon[1501]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:31 ceph-node-5 mds[4727]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:51:14 ceph-node-4 mgr[9839]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:51 ceph-node-1 client[2304]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:33 ceph-node-3 osd[2964]: ERROR: OSD 3.1 marked down due to heartbeat failure
2025-02-24 10:50:53 ceph-node-3 radosgw[5444]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:01 ceph-node-3 client[4721]: INFO: MGR module loaded successfully
2025-02-24 10:51:31 ceph-node-3 client[5975]: CRITICAL: Cluster status: HEALTH_WARN
2025-02-24 10:51:19 ceph-node-2 client[3905]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:06 ceph-node-3 mds[8811]: WARNING: Uncorrectable ECC error detected in MDS cache
2025-02-24 10:50:54 ceph-node-4 osd[3305]: INFO: MGR module loaded successfully
2025-02-24 10:50:35 ceph-node-2 mds[5321]: INFO: PG 1.2 replicated successfully
2025-02-24 10:50:31 ceph-node-3 mgr[8426]: INFO: PG 1.2 replicated successfully
2025-02-24 10:50:44 ceph-node-4 mon[1114]: DEBUG: OSD rebalancing completed
2025-02-24 10:50:31 ceph-node-4 radosgw[4555]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:11 ceph-node-1 mgr[4092]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:02 ceph-node-2 osd[6160]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:50:49 ceph-node-5 mds[6509]: INFO: Monitor map has been updated
2025-02-24 10:50:34 ceph-node-1 radosgw[9637]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:43 ceph-node-4 mon[2373]: NOTICE: Data replication completed for object pool
2025-02-24 10:51:12 ceph-node-4 client[1555]: INFO: OSD rebalancing completed
2025-02-24 10:50:57 ceph-node-1 osd[3663]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:31 ceph-node-2 mon[1815]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:50:35 ceph-node-3 mon[8598]: DEBUG: OSD rebalancing completed
2025-02-24 10:50:50 ceph-node-2 mon[7786]: INFO: PG 1.2 replicated successfully
2025-02-24 10:50:33 ceph-node-3 client[8298]: DEBUG: Monitor map has been updated
2025-02-24 10:50:45 ceph-node-3 mgr[5191]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:50:49 ceph-node-1 mds[7879]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:50:41 ceph-node-1 mds[9290]: DEBUG: Monitor map has been updated
2025-02-24 10:51:04 ceph-node-2 mgr[8958]: NOTICE: MGR module loaded successfully
2025-02-24 10:51:10 ceph-node-3 client[9910]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:50:58 ceph-node-4 osd[9505]: WARNING: Network partition detected between OSD nodes
2025-02-24 10:51:17 ceph-node-1 radosgw[2629]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:36 ceph-node-5 radosgw[2427]: ERROR: MDS failure detected: metadata inconsistency
2025-02-24 10:51:07 ceph-node-3 radosgw[1435]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:03 ceph-node-3 radosgw[9800]: INFO: PG 1.2 replicated successfully
2025-02-24 10:51:05 ceph-node-4 radosgw[4069]: WARNING: Monitor down, unable to connect to quorum
2025-02-24 10:50:53 ceph-node-2 mds[2900]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:51:20 ceph-node-5 mds[9404]: INFO: OSD rebalancing completed
2025-02-24 10:51:04 ceph-node-1 mgr[3820]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:50:33 ceph-node-3 radosgw[8467]: NOTICE: Data replication completed for object pool
2025-02-24 10:51:11 ceph-node-3 mon[1918]: INFO: PG 1.2 replicated successfully
2025-02-24 10:51:29 ceph-node-3 osd[5999]: NOTICE: MGR module loaded successfully
2025-02-24 10:50:41 ceph-node-1 client[5584]: INFO: OSD rebalancing completed
2025-02-24 10:51:20 ceph-node-1 mgr[9349]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:50:49 ceph-node-5 mds[7439]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:51:06 ceph-node-3 mgr[9381]: NOTICE: OSD rebalancing completed
2025-02-24 10:50:39 ceph-node-3 mon[6140]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:51:17 ceph-node-4 mon[9152]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:50:44 ceph-node-1 client[7594]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:50:50 ceph-node-2 radosgw[9013]: ERROR: CRITICAL: Data loss detected on pool 1
2025-02-24 10:50:57 ceph-node-1 mds[2819]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:51:00 ceph-node-3 mgr[5104]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:04 ceph-node-4 mon[3431]: ERROR: Client connection timeout detected
2025-02-24 10:50:57 ceph-node-3 client[5835]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:40 ceph-node-2 mgr[4794]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:50:55 ceph-node-3 osd[4499]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:52 ceph-node-4 mgr[1110]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:50:50 ceph-node-5 mds[4150]: DEBUG: Monitor map has been updated
2025-02-24 10:51:19 ceph-node-1 mgr[9746]: DEBUG: Data replication completed for object pool
2025-02-24 10:50:37 ceph-node-1 mds[8985]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:53 ceph-node-1 osd[7930]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:06 ceph-node-1 mds[7360]: INFO: OSD disk space utilization: 70%
2025-02-24 10:50:59 ceph-node-4 osd[3106]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:03 ceph-node-2 mds[2523]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:41 ceph-node-5 mgr[7239]: DEBUG: OSD rebalancing completed
2025-02-24 10:50:49 ceph-node-1 mds[4044]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:51:20 ceph-node-4 mon[1317]: DEBUG: OSD rebalancing completed
2025-02-24 10:50:40 ceph-node-5 mgr[4556]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:51:13 ceph-node-2 mgr[2097]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:50:38 ceph-node-3 mds[6152]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:15 ceph-node-4 client[4805]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:51:16 ceph-node-5 client[2158]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:35 ceph-node-1 mon[6230]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:38 ceph-node-4 osd[9509]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:51:28 ceph-node-3 radosgw[4989]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:10 ceph-node-5 mon[8083]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:00 ceph-node-5 mon[3753]: CRITICAL: Disk failure reported on ceph-node-4
2025-02-24 10:51:18 ceph-node-2 mon[7163]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:59 ceph-node-3 mon[3599]: WARNING: Monitor mon1 failed to reach quorum
2025-02-24 10:50:58 ceph-node-2 mgr[6168]: DEBUG: OSD rebalancing completed
2025-02-24 10:51:15 ceph-node-5 client[5296]: ERROR: Disk failure reported on ceph-node-4
2025-02-24 10:51:12 ceph-node-2 osd[8017]: DEBUG: Monitor map has been updated
2025-02-24 10:50:56 ceph-node-1 radosgw[5805]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:50:38 ceph-node-4 mon[8130]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:50:51 ceph-node-4 radosgw[1981]: INFO: Monitor map has been updated
2025-02-24 10:50:47 ceph-node-4 client[7001]: INFO: OSD rebalancing completed
2025-02-24 10:50:36 ceph-node-3 mgr[1032]: DEBUG: Monitor map has been updated
2025-02-24 10:51:06 ceph-node-2 mon[7656]: CRITICAL: CRITICAL: Data loss detected on pool 1
2025-02-24 10:51:06 ceph-node-2 mgr[8936]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:50:44 ceph-node-5 radosgw[3317]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:50:38 ceph-node-4 osd[6274]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:30 ceph-node-1 mon[8375]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:39 ceph-node-5 mds[3775]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:37 ceph-node-2 mgr[6595]: INFO: MGR module loaded successfully
2025-02-24 10:50:32 ceph-node-2 client[9168]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:50:32 ceph-node-2 mon[3940]: DEBUG: Monitor map has been updated
2025-02-24 10:51:02 ceph-node-5 osd[4300]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:07 ceph-node-4 mds[7042]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:51:21 ceph-node-2 mon[5704]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:50:34 ceph-node-5 client[6115]: INFO: Monitor map has been updated
2025-02-24 10:50:44 ceph-node-3 client[2844]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:22 ceph-node-2 mgr[7583]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:50:35 ceph-node-4 client[2372]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:50:45 ceph-node-4 mds[4650]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:18 ceph-node-2 osd[7692]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:50:58 ceph-node-5 osd[5472]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:53 ceph-node-5 client[2332]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:50:40 ceph-node-2 mon[8584]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:41 ceph-node-1 mon[3508]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:51:16 ceph-node-2 mgr[6874]: DEBUG: OSD rebalancing completed
2025-02-24 10:51:10 ceph-node-3 client[9085]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:42 ceph-node-3 mon[5067]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:07 ceph-node-4 mgr[4313]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:50:42 ceph-node-1 osd[6176]: CRITICAL: Monitor down, unable to connect to quorum
2025-02-24 10:51:31 ceph-node-5 radosgw[1557]: DEBUG: Monitor map has been updated
2025-02-24 10:51:25 ceph-node-2 mon[7846]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:17 ceph-node-5 osd[7033]: NOTICE: OSD rebalancing completed
2025-02-24 10:50:59 ceph-node-2 radosgw[2254]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:16 ceph-node-3 mgr[9071]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:31 ceph-node-4 client[8338]: DEBUG: OSD rebalancing completed
2025-02-24 10:50:38 ceph-node-2 client[2253]: INFO: Monitor map has been updated
2025-02-24 10:51:05 ceph-node-2 mgr[9526]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:03 ceph-node-4 mon[2059]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:50:51 ceph-node-5 osd[5377]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:50:56 ceph-node-3 client[2817]: INFO: OSD disk space utilization: 70%
2025-02-24 10:51:29 ceph-node-1 mds[1626]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:00 ceph-node-2 mgr[9934]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:50:47 ceph-node-3 osd[3131]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:50:51 ceph-node-4 mgr[1690]: INFO: MGR module loaded successfully
2025-02-24 10:51:26 ceph-node-2 mgr[8444]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:51:24 ceph-node-5 mgr[6634]: DEBUG: MGR module loaded successfully
2025-02-24 10:50:49 ceph-node-4 osd[1495]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:21 ceph-node-4 osd[9967]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:57 ceph-node-3 client[3864]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:33 ceph-node-3 client[9085]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:14 ceph-node-2 osd[3893]: NOTICE: Data replication completed for object pool
2025-02-24 10:50:39 ceph-node-5 osd[1124]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:51:14 ceph-node-4 osd[9649]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:51:19 ceph-node-2 client[1479]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:32 ceph-node-5 osd[8031]: INFO: MGR module loaded successfully
2025-02-24 10:50:53 ceph-node-1 mgr[7171]: INFO: OSD disk space utilization: 70%
2025-02-24 10:50:34 ceph-node-3 mon[7119]: ERROR: MDS failure detected: metadata inconsistency
2025-02-24 10:50:50 ceph-node-5 mgr[9381]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:50:43 ceph-node-2 radosgw[7060]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:51:07 ceph-node-4 radosgw[3883]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:13 ceph-node-5 client[9091]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:18 ceph-node-2 mon[3756]: DEBUG: MGR module loaded successfully
2025-02-24 10:50:32 ceph-node-2 mds[1229]: INFO: Monitor map has been updated
2025-02-24 10:50:47 ceph-node-1 mgr[9188]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:48 ceph-node-1 radosgw[3135]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:53 ceph-node-5 mon[7825]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:10 ceph-node-2 mgr[5447]: INFO: PG 1.2 replicated successfully
2025-02-24 10:51:00 ceph-node-5 client[4714]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:50:57 ceph-node-1 client[5369]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:50:40 ceph-node-5 osd[5919]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:50:41 ceph-node-2 mon[3375]: INFO: OSD disk space utilization: 70%
2025-02-24 10:51:20 ceph-node-3 mgr[5527]: INFO: OSD rebalancing completed
2025-02-24 10:50:57 ceph-node-1 mgr[5833]: DEBUG: MGR module loaded successfully
2025-02-24 10:50:43 ceph-node-1 mds[1578]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:51 ceph-node-4 mds[1294]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:41 ceph-node-3 client[8662]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:50:59 ceph-node-1 client[5304]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:50:53 ceph-node-4 mon[8068]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:08 ceph-node-2 radosgw[5738]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:50:46 ceph-node-5 client[1942]: DEBUG: Monitor map has been updated
2025-02-24 10:50:58 ceph-node-5 osd[7346]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:40 ceph-node-2 osd[1428]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:50:45 ceph-node-5 mds[3178]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:47 ceph-node-2 client[7223]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:51:00 ceph-node-1 mon[8647]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:30 ceph-node-5 mgr[3136]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:51:22 ceph-node-1 mds[5273]: WARNING: Monitor mon1 failed to reach quorum
2025-02-24 10:50:49 ceph-node-2 mgr[3868]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:50:41 ceph-node-2 osd[9081]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:50:47 ceph-node-4 client[5475]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:12 ceph-node-4 mon[8980]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:51:22 ceph-node-4 mgr[9735]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:50:53 ceph-node-5 mgr[5210]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:50:49 ceph-node-1 mds[6320]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:51:18 ceph-node-5 radosgw[3049]: DEBUG: Monitor map has been updated
2025-02-24 10:50:33 ceph-node-2 client[4432]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:50:38 ceph-node-1 mgr[2433]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:51:31 ceph-node-1 osd[6479]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:50:35 ceph-node-5 mgr[1019]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:51:01 ceph-node-3 mds[3247]: NOTICE: Monitor map has been updated
2025-02-24 10:51:25 ceph-node-1 mgr[2585]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:30 ceph-node-3 radosgw[7281]: INFO: OSD rebalancing completed
2025-02-24 10:51:05 ceph-node-1 osd[7507]: INFO: MGR module loaded successfully
2025-02-24 10:51:02 ceph-node-2 mds[5443]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:56 ceph-node-1 mds[8971]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:50:36 ceph-node-2 radosgw[3438]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:20 ceph-node-3 osd[4750]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:51:02 ceph-node-5 mon[9107]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:34 ceph-node-4 mgr[9782]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:15 ceph-node-5 mgr[7151]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:51:12 ceph-node-5 client[8359]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:07 ceph-node-5 mon[6315]: ERROR: Network partition detected between OSD nodes
2025-02-24 10:51:19 ceph-node-3 osd[8770]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:50:44 ceph-node-4 mgr[3173]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:50:33 ceph-node-4 mds[8816]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:12 ceph-node-5 mgr[9745]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:11 ceph-node-1 mgr[9103]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:50:55 ceph-node-3 mon[8582]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:50:54 ceph-node-3 client[3082]: INFO: MGR module loaded successfully
2025-02-24 10:51:18 ceph-node-5 mds[6883]: NOTICE: MGR module loaded successfully
2025-02-24 10:50:49 ceph-node-1 osd[1378]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:33 ceph-node-1 mgr[3240]: WARNING: OSD 3.1 marked down due to heartbeat failure
2025-02-24 10:51:24 ceph-node-2 radosgw[2094]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:48 ceph-node-1 mgr[8936]: INFO: Monitor map has been updated
2025-02-24 10:51:29 ceph-node-4 mgr[2294]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:01 ceph-node-3 client[9404]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:50 ceph-node-3 mgr[1692]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:51:12 ceph-node-5 osd[7460]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:50:37 ceph-node-2 mgr[5240]: INFO: OSD disk space utilization: 70%
2025-02-24 10:51:11 ceph-node-3 mon[4553]: INFO: Monitor map has been updated
2025-02-24 10:50:57 ceph-node-3 radosgw[2763]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:51:18 ceph-node-3 mon[4291]: DEBUG: OSD rebalancing completed
2025-02-24 10:51:15 ceph-node-1 osd[6911]: CRITICAL: Cluster status: HEALTH_WARN
2025-02-24 10:51:12 ceph-node-2 mgr[5465]: INFO: MGR module loaded successfully
2025-02-24 10:50:46 ceph-node-3 mds[3865]: WARNING: Slow request detected on OSD 4.3
2025-02-24 10:51:14 ceph-node-3 client[8436]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:50 ceph-node-4 osd[3701]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:20 ceph-node-3 mds[6259]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:51:27 ceph-node-2 mon[4726]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:02 ceph-node-4 osd[5558]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:36 ceph-node-3 radosgw[7577]: INFO: OSD rebalancing completed
2025-02-24 10:51:16 ceph-node-4 mon[1291]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:50:36 ceph-node-1 client[5512]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:50:49 ceph-node-2 client[6921]: NOTICE: OSD rebalancing completed
2025-02-24 10:50:50 ceph-node-4 client[2866]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:18 ceph-node-4 client[5360]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:51:21 ceph-node-5 mgr[8093]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:51:30 ceph-node-1 osd[5407]: WARNING: Client connection timeout detected
2025-02-24 10:51:09 ceph-node-1 mds[3663]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:24 ceph-node-1 mds[3685]: INFO: Monitor map has been updated
2025-02-24 10:51:26 ceph-node-3 mgr[2172]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:47 ceph-node-5 mds[9586]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:48 ceph-node-4 mon[2107]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:58 ceph-node-1 mds[8303]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:28 ceph-node-4 mgr[1329]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:53 ceph-node-2 osd[2821]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:51:20 ceph-node-2 client[6754]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:51:29 ceph-node-1 client[8528]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:51:29 ceph-node-5 osd[9752]: DEBUG: Data replication completed for object pool
2025-02-24 10:50:48 ceph-node-2 mon[3201]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:51:25 ceph-node-4 mon[4548]: NOTICE: Data replication completed for object pool
2025-02-24 10:51:09 ceph-node-1 mon[4691]: INFO: OSD rebalancing completed
2025-02-24 10:50:45 ceph-node-1 radosgw[8434]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:00 ceph-node-1 radosgw[1825]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:56 ceph-node-3 mds[1474]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:21 ceph-node-3 radosgw[4586]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:10 ceph-node-1 mds[2873]: WARNING: Network partition detected between OSD nodes
2025-02-24 10:50:38 ceph-node-5 mgr[9003]: INFO: MGR module loaded successfully
2025-02-24 10:51:16 ceph-node-5 client[6501]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:36 ceph-node-3 osd[1255]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:45 ceph-node-3 osd[6934]: NOTICE: OSD rebalancing completed
2025-02-24 10:51:02 ceph-node-2 client[4183]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:50:48 ceph-node-2 osd[8745]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:16 ceph-node-5 radosgw[2498]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:27 ceph-node-5 mon[1174]: CRITICAL: CRITICAL: Data loss detected on pool 1
2025-02-24 10:50:55 ceph-node-3 mgr[2940]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:19 ceph-node-2 radosgw[9275]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:51:27 ceph-node-3 client[2840]: INFO: MGR module loaded successfully
2025-02-24 10:50:55 ceph-node-2 mon[2161]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:23 ceph-node-2 osd[1264]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:26 ceph-node-5 mds[9041]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:50:37 ceph-node-5 client[3031]: NOTICE: OSD rebalancing completed
2025-02-24 10:50:45 ceph-node-3 mgr[1921]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:13 ceph-node-4 mon[5235]: ERROR: Disk failure reported on ceph-node-4
2025-02-24 10:50:58 ceph-node-2 mgr[5796]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:50:46 ceph-node-1 mds[9152]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:11 ceph-node-5 mgr[1130]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:50:45 ceph-node-4 osd[1218]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:13 ceph-node-3 mgr[6109]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:01 ceph-node-2 mgr[3852]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:45 ceph-node-5 osd[5537]: WARNING: Journal write failure detected on OSD node
2025-02-24 10:51:17 ceph-node-2 client[7415]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:51:10 ceph-node-5 client[6639]: CRITICAL: Monitor mon1 failed to reach quorum
2025-02-24 10:50:40 ceph-node-1 client[8748]: CRITICAL: Monitor down, unable to connect to quorum
2025-02-24 10:50:56 ceph-node-4 mgr[8529]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:51:22 ceph-node-2 mds[7678]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:59 ceph-node-3 mgr[9457]: DEBUG: MGR module loaded successfully
2025-02-24 10:50:48 ceph-node-1 osd[5748]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:48 ceph-node-4 mgr[4889]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:50:44 ceph-node-1 osd[8479]: DEBUG: Monitor map has been updated
2025-02-24 10:51:23 ceph-node-4 mon[4368]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:51:10 ceph-node-4 mon[6248]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:51:14 ceph-node-2 mon[3959]: INFO: Data replication completed for object pool
2025-02-24 10:50:34 ceph-node-3 radosgw[2481]: ERROR: Slow request detected on OSD 4.3
2025-02-24 10:50:37 ceph-node-5 client[2722]: ERROR: CRITICAL: Data loss detected on pool 1
2025-02-24 10:50:57 ceph-node-1 mds[7656]: INFO: MGR module loaded successfully
2025-02-24 10:51:05 ceph-node-5 osd[6409]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:51:04 ceph-node-1 radosgw[1581]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:01 ceph-node-5 osd[5578]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:50:52 ceph-node-2 osd[9849]: NOTICE: Monitor map has been updated
2025-02-24 10:50:41 ceph-node-5 mds[5308]: INFO: Monitor map has been updated
2025-02-24 10:51:05 ceph-node-5 mgr[3524]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:46 ceph-node-3 mds[6175]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:50:53 ceph-node-2 client[8106]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:21 ceph-node-3 radosgw[5372]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:13 ceph-node-2 osd[8858]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:08 ceph-node-3 radosgw[1786]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:50:38 ceph-node-5 radosgw[6028]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:14 ceph-node-2 mgr[8297]: DEBUG: OSD rebalancing completed
2025-02-24 10:51:21 ceph-node-4 client[8049]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:51:15 ceph-node-5 osd[9365]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:51:20 ceph-node-4 mgr[5513]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:19 ceph-node-1 radosgw[5099]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:36 ceph-node-2 osd[3708]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:28 ceph-node-2 radosgw[1501]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:04 ceph-node-1 osd[5714]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:51:16 ceph-node-4 mds[3836]: ERROR: Disk failure reported on ceph-node-4
2025-02-24 10:51:22 ceph-node-3 mds[8065]: INFO: Monitor map has been updated
2025-02-24 10:50:43 ceph-node-1 mds[9134]: INFO: PG 1.2 replicated successfully
2025-02-24 10:51:27 ceph-node-3 mgr[4501]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:35 ceph-node-4 osd[7186]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:50:50 ceph-node-3 osd[8050]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:50:40 ceph-node-4 mgr[3440]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:50:36 ceph-node-4 mon[4300]: ERROR: Slow request detected on OSD 4.3
2025-02-24 10:51:30 ceph-node-2 mds[2578]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:59 ceph-node-2 mgr[1736]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:01 ceph-node-5 client[2351]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:50:33 ceph-node-5 osd[9243]: INFO: OSD rebalancing completed
2025-02-24 10:50:37 ceph-node-3 mon[7422]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:50:45 ceph-node-2 radosgw[4214]: WARNING: High I/O latency detected in radosgw service
2025-02-24 10:50:37 ceph-node-4 mds[9517]: INFO: Monitor map has been updated
2025-02-24 10:51:10 ceph-node-4 radosgw[9477]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:51:15 ceph-node-1 radosgw[5505]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:05 ceph-node-3 client[3625]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:51:07 ceph-node-4 mds[9390]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:13 ceph-node-1 radosgw[2639]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:07 ceph-node-1 mon[5065]: WARNING: Journal write failure detected on OSD node
2025-02-24 10:51:10 ceph-node-1 mds[4721]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:18 ceph-node-3 osd[5239]: NOTICE: Data replication completed for object pool
2025-02-24 10:51:30 ceph-node-5 mgr[8762]: NOTICE: Monitor map has been updated
2025-02-24 10:51:25 ceph-node-3 osd[5551]: INFO: OSD disk space utilization: 70%
2025-02-24 10:51:00 ceph-node-3 osd[6378]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:26 ceph-node-2 osd[5086]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:10 ceph-node-2 mgr[8700]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:51:16 ceph-node-2 radosgw[7634]: DEBUG: Data replication completed for object pool
2025-02-24 10:50:36 ceph-node-5 mgr[3901]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:13 ceph-node-4 osd[6963]: NOTICE: MGR module loaded successfully
2025-02-24 10:50:50 ceph-node-2 client[3007]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:50:49 ceph-node-3 osd[8859]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:50:34 ceph-node-4 mds[5584]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:51:08 ceph-node-4 mds[2749]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:13 ceph-node-5 mgr[3894]: ERROR: Authentication failure in RADOS Gateway
2025-02-24 10:51:18 ceph-node-4 radosgw[5286]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:50:49 ceph-node-2 mon[4330]: ERROR: Disk failure reported on ceph-node-4
2025-02-24 10:50:34 ceph-node-1 radosgw[7524]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:44 ceph-node-1 mon[2876]: INFO: Monitor map has been updated
2025-02-24 10:50:54 ceph-node-2 osd[6437]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:58 ceph-node-3 radosgw[3292]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:50:43 ceph-node-2 radosgw[2261]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:50:39 ceph-node-3 mds[7514]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:07 ceph-node-3 client[6005]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:11 ceph-node-4 mds[4287]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:11 ceph-node-3 osd[5433]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:04 ceph-node-2 mgr[1046]: DEBUG: MGR module loaded successfully
2025-02-24 10:50:53 ceph-node-3 mgr[8279]: ERROR: Data corruption detected in object pool
2025-02-24 10:51:31 ceph-node-1 mgr[3443]: WARNING: Slow request detected on OSD 4.3
2025-02-24 10:51:00 ceph-node-5 client[7777]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:45 ceph-node-3 osd[3035]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:25 ceph-node-5 mon[9018]: CRITICAL: OSD 3.1 marked down due to heartbeat failure
2025-02-24 10:51:00 ceph-node-2 client[3951]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:32 ceph-node-2 mgr[6284]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:50:42 ceph-node-1 osd[9669]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:50:45 ceph-node-1 mgr[9134]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:01 ceph-node-2 client[9873]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:20 ceph-node-2 mon[7107]: NOTICE: MGR module loaded successfully
2025-02-24 10:50:49 ceph-node-2 osd[5477]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:50:58 ceph-node-5 osd[1417]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:28 ceph-node-4 mgr[2564]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:08 ceph-node-1 mgr[8418]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:50:52 ceph-node-2 mon[1373]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:51:23 ceph-node-2 mds[5106]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:50:42 ceph-node-2 osd[7054]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:32 ceph-node-1 mon[3389]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:50:48 ceph-node-1 mon[3544]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:52 ceph-node-5 radosgw[7477]: NOTICE: MGR module loaded successfully
2025-02-24 10:51:01 ceph-node-4 mon[2449]: NOTICE: OSD rebalancing completed
2025-02-24 10:51:23 ceph-node-2 mgr[3348]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:51:15 ceph-node-3 mds[3726]: NOTICE: MGR module loaded successfully
2025-02-24 10:51:20 ceph-node-5 mon[8585]: NOTICE: MGR module loaded successfully
2025-02-24 10:50:51 ceph-node-2 client[5876]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:50:56 ceph-node-2 mgr[7815]: INFO: Data replication completed for object pool
2025-02-24 10:51:01 ceph-node-5 osd[2070]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:31 ceph-node-4 mgr[8816]: DEBUG: Monitor map has been updated
2025-02-24 10:51:01 ceph-node-3 mon[4300]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:50:50 ceph-node-2 client[7013]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:50:56 ceph-node-4 mds[4220]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:51:28 ceph-node-5 mds[7392]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:50:59 ceph-node-3 client[8121]: WARNING: CRITICAL: Data loss detected on pool 1
2025-02-24 10:51:02 ceph-node-1 mds[2864]: NOTICE: MGR module loaded successfully
2025-02-24 10:50:42 ceph-node-4 mds[4237]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:31 ceph-node-4 radosgw[5487]: INFO: OSD rebalancing completed
2025-02-24 10:51:10 ceph-node-4 osd[1126]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:44 ceph-node-4 client[3138]: NOTICE: Monitor map has been updated
2025-02-24 10:50:35 ceph-node-4 mgr[8520]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:50:46 ceph-node-1 client[3841]: INFO: OSD disk space utilization: 70%
2025-02-24 10:50:54 ceph-node-4 mon[3236]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:41 ceph-node-4 mds[2538]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:50:56 ceph-node-4 client[1421]: INFO: Monitor map has been updated
2025-02-24 10:51:09 ceph-node-1 radosgw[1091]: DEBUG: Monitor map has been updated
2025-02-24 10:51:00 ceph-node-3 mgr[8661]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:50:42 ceph-node-4 mds[8268]: CRITICAL: Client connection timeout detected
2025-02-24 10:50:44 ceph-node-3 mgr[4245]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:36 ceph-node-4 client[2168]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:06 ceph-node-2 mgr[2899]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:31 ceph-node-5 client[8238]: CRITICAL: Monitor down, unable to connect to quorum
2025-02-24 10:51:31 ceph-node-4 mgr[8038]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:51:22 ceph-node-5 mon[6485]: INFO: PG 1.2 replicated successfully
2025-02-24 10:50:33 ceph-node-5 mon[2264]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:50 ceph-node-5 client[9422]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:50:55 ceph-node-1 mds[3587]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:51:26 ceph-node-5 mon[8738]: ERROR: CRITICAL: PG 2.1 stuck in inconsistent state
2025-02-24 10:50:41 ceph-node-2 mds[2488]: NOTICE: Monitor map has been updated
2025-02-24 10:50:51 ceph-node-5 mgr[7368]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:07 ceph-node-3 osd[7338]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:53 ceph-node-4 mgr[1980]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:51:07 ceph-node-5 radosgw[1001]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:50:59 ceph-node-4 osd[2168]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:51:09 ceph-node-3 mds[8845]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:12 ceph-node-5 osd[2223]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:50:35 ceph-node-4 mds[9199]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:50:58 ceph-node-3 osd[5633]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:50:36 ceph-node-1 radosgw[9815]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:43 ceph-node-5 radosgw[8422]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:51 ceph-node-3 mgr[8027]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:50:38 ceph-node-5 mon[2215]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:51:12 ceph-node-2 mgr[3910]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:51:19 ceph-node-2 mgr[6003]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:00 ceph-node-1 osd[1068]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:51:08 ceph-node-5 osd[1500]: INFO: MGR module loaded successfully
2025-02-24 10:51:29 ceph-node-1 mgr[5046]: DEBUG: MGR module loaded successfully
2025-02-24 10:50:35 ceph-node-4 radosgw[9655]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:50:40 ceph-node-2 mgr[3292]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:50:37 ceph-node-4 mgr[1176]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:01 ceph-node-5 mon[8587]: NOTICE: OSD rebalancing completed
2025-02-24 10:50:37 ceph-node-3 osd[9940]: ERROR: Client connection timeout detected
2025-02-24 10:50:47 ceph-node-3 mon[2492]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:30 ceph-node-2 osd[9418]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:50:40 ceph-node-4 radosgw[1033]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:50:55 ceph-node-5 client[7399]: DEBUG: Monitor map has been updated
2025-02-24 10:51:15 ceph-node-2 osd[7874]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:50:40 ceph-node-5 osd[2135]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:50:40 ceph-node-3 mds[4949]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:08 ceph-node-3 mds[8196]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:30 ceph-node-3 mon[9257]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:51:16 ceph-node-3 osd[6121]: WARNING: MDS failure detected: metadata inconsistency
2025-02-24 10:51:19 ceph-node-1 osd[1447]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:15 ceph-node-2 radosgw[2164]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:23 ceph-node-2 client[7468]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:28 ceph-node-1 osd[7342]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:51:04 ceph-node-1 mgr[9675]: INFO: OSD disk space utilization: 70%
2025-02-24 10:50:39 ceph-node-2 osd[4470]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:51:08 ceph-node-3 osd[9904]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:50 ceph-node-1 mds[6307]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:50:59 ceph-node-2 mds[1138]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:00 ceph-node-1 mds[9906]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:50:34 ceph-node-1 mon[1696]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:51:07 ceph-node-1 radosgw[6245]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:01 ceph-node-3 mon[4053]: INFO: Monitor map has been updated
2025-02-24 10:51:23 ceph-node-2 mds[6097]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:18 ceph-node-2 mon[1482]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:50:54 ceph-node-2 radosgw[5707]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:51:18 ceph-node-4 mon[5067]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:51:10 ceph-node-5 radosgw[3543]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:50:34 ceph-node-5 mds[8895]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:04 ceph-node-5 osd[7058]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:50:54 ceph-node-1 mgr[1870]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:51:26 ceph-node-5 mgr[3167]: ERROR: Slow request detected on OSD 4.3
2025-02-24 10:51:22 ceph-node-2 radosgw[6837]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:15 ceph-node-2 radosgw[8850]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:46 ceph-node-4 mds[3301]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:10 ceph-node-1 osd[2715]: INFO: Monitor map has been updated
2025-02-24 10:50:44 ceph-node-3 radosgw[4305]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:38 ceph-node-2 osd[4356]: INFO: OSD rebalancing completed
2025-02-24 10:50:37 ceph-node-5 mds[4370]: CRITICAL: Monitor mon1 failed to reach quorum
2025-02-24 10:51:29 ceph-node-3 radosgw[3730]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:22 ceph-node-1 radosgw[2901]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:19 ceph-node-5 radosgw[4153]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:57 ceph-node-3 mgr[2583]: NOTICE: MGR module loaded successfully
2025-02-24 10:50:34 ceph-node-3 radosgw[9800]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:50:56 ceph-node-3 mgr[8808]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:51:00 ceph-node-5 mon[4405]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:51:07 ceph-node-5 osd[4028]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:15 ceph-node-5 client[7242]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:50:31 ceph-node-3 osd[9975]: DEBUG: Monitor map has been updated
2025-02-24 10:50:55 ceph-node-5 mon[6121]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:09 ceph-node-1 mgr[7312]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:23 ceph-node-4 mon[4418]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:50:34 ceph-node-3 mds[4628]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:50:52 ceph-node-3 mon[9108]: INFO: OSD rebalancing completed
2025-02-24 10:50:44 ceph-node-2 radosgw[2528]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:13 ceph-node-3 mon[5832]: CRITICAL: Client connection timeout detected
2025-02-24 10:50:36 ceph-node-3 osd[3724]: INFO: PG 1.2 replicated successfully
2025-02-24 10:50:44 ceph-node-1 mds[6596]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:50:56 ceph-node-5 osd[5872]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:12 ceph-node-5 client[4075]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:50:37 ceph-node-4 client[1074]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:51:10 ceph-node-4 osd[9590]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:51:19 ceph-node-2 osd[2401]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:12 ceph-node-4 mon[2874]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:26 ceph-node-4 radosgw[9338]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:50:40 ceph-node-2 mgr[5490]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:13 ceph-node-2 mon[9932]: NOTICE: Data replication completed for object pool
2025-02-24 10:51:08 ceph-node-5 mgr[6384]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:50:37 ceph-node-1 radosgw[6819]: INFO: Monitor map has been updated
2025-02-24 10:51:09 ceph-node-5 mon[2007]: DEBUG: OSD rebalancing completed
2025-02-24 10:51:05 ceph-node-5 mds[4649]: INFO: PG 1.2 replicated successfully
2025-02-24 10:50:59 ceph-node-2 radosgw[8604]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:16 ceph-node-2 mon[7810]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:50:43 ceph-node-5 osd[3074]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:23 ceph-node-2 mon[8499]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:50:56 ceph-node-1 radosgw[1633]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:51:21 ceph-node-1 mon[3989]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:50:33 ceph-node-4 mon[5489]: DEBUG: Monitor map has been updated
2025-02-24 10:51:05 ceph-node-5 mgr[9032]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:23 ceph-node-3 osd[4544]: NOTICE: Data replication completed for object pool
2025-02-24 10:50:48 ceph-node-5 mgr[5046]: CRITICAL: Cluster status: HEALTH_WARN
2025-02-24 10:50:41 ceph-node-4 mon[6916]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:50:53 ceph-node-3 radosgw[8751]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:51 ceph-node-4 radosgw[5178]: ERROR: CRITICAL: Data loss detected on pool 1
2025-02-24 10:50:41 ceph-node-1 mon[8939]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:48 ceph-node-1 mds[2737]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:50:42 ceph-node-1 mon[5660]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:50:54 ceph-node-3 client[3727]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:50:44 ceph-node-5 mon[2483]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:01 ceph-node-1 mgr[7334]: INFO: Data replication completed for object pool
2025-02-24 10:51:28 ceph-node-3 mon[1822]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:50:59 ceph-node-5 osd[9323]: DEBUG: MGR module loaded successfully
2025-02-24 10:50:33 ceph-node-3 radosgw[6198]: WARNING: Client connection timeout detected
2025-02-24 10:50:46 ceph-node-2 mon[5012]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:50:41 ceph-node-3 mds[7151]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:50:34 ceph-node-2 osd[3789]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:50:31 ceph-node-3 client[1081]: INFO: OSD disk space utilization: 70%
2025-02-24 10:51:08 ceph-node-4 mds[8711]: NOTICE: MGR module loaded successfully
2025-02-24 10:51:27 ceph-node-1 osd[7428]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:51:25 ceph-node-4 radosgw[6125]: NOTICE: Monitor map has been updated
2025-02-24 10:51:08 ceph-node-5 osd[1032]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:51:15 ceph-node-5 radosgw[7476]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:50:45 ceph-node-3 osd[6666]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:51:27 ceph-node-5 osd[7291]: INFO: OSD disk space utilization: 70%
2025-02-24 10:51:06 ceph-node-4 client[4050]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:50:51 ceph-node-3 client[9647]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:51:09 ceph-node-4 mds[7735]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:43 ceph-node-5 client[6488]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:22 ceph-node-5 mds[6511]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:51 ceph-node-5 mds[4861]: DEBUG: Monitor map has been updated
2025-02-24 10:51:30 ceph-node-4 osd[1281]: DEBUG: MGR module loaded successfully
2025-02-24 10:50:49 ceph-node-2 client[6385]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:51:11 ceph-node-1 osd[8451]: NOTICE: Data replication completed for object pool
2025-02-24 10:50:50 ceph-node-2 osd[8737]: NOTICE: MGR module loaded successfully
2025-02-24 10:51:00 ceph-node-4 osd[6537]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:51:03 ceph-node-4 client[7927]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:51:00 ceph-node-5 mon[7848]: CRITICAL: Cluster status: HEALTH_WARN
2025-02-24 10:51:02 ceph-node-4 mds[7803]: ERROR: OSD 3.1 marked down due to heartbeat failure
2025-02-24 10:51:29 ceph-node-1 client[6922]: NOTICE: OSD rebalancing completed
2025-02-24 10:50:45 ceph-node-1 osd[8800]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:50 ceph-node-2 radosgw[2922]: DEBUG: Data replication completed for object pool
2025-02-24 10:50:48 ceph-node-3 radosgw[8846]: INFO: MGR module loaded successfully
2025-02-24 10:51:04 ceph-node-1 radosgw[3251]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:51:25 ceph-node-1 mon[3636]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:40 ceph-node-3 osd[2548]: NOTICE: Monitor map has been updated
2025-02-24 10:50:58 ceph-node-4 osd[3695]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:50:35 ceph-node-3 mon[2849]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:51:05 ceph-node-4 radosgw[4905]: NOTICE: Monitor map has been updated
2025-02-24 10:51:12 ceph-node-4 mds[4242]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:57 ceph-node-2 radosgw[3951]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:10 ceph-node-1 mgr[7379]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:50:53 ceph-node-1 mon[2416]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:37 ceph-node-2 osd[5514]: INFO: OSD rebalancing completed
2025-02-24 10:50:59 ceph-node-3 osd[7970]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:50:31 ceph-node-3 mgr[7345]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:17 ceph-node-5 mds[2582]: INFO: Monitor map has been updated
2025-02-24 10:51:17 ceph-node-4 radosgw[8435]: ERROR: Journal write failure detected on OSD node
2025-02-24 10:50:45 ceph-node-3 mon[9915]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:50:39 ceph-node-4 osd[3588]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:50:54 ceph-node-4 osd[8430]: NOTICE: MGR module loaded successfully
2025-02-24 10:50:35 ceph-node-4 mds[8546]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:51:22 ceph-node-4 mds[2680]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:50:39 ceph-node-5 client[6040]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:46 ceph-node-4 mgr[9381]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:36 ceph-node-3 mds[2243]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:16 ceph-node-3 radosgw[2376]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:51:03 ceph-node-1 mon[4073]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:15 ceph-node-2 mgr[5774]: INFO: Monitor map has been updated
2025-02-24 10:51:16 ceph-node-3 osd[9494]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:51:20 ceph-node-5 mds[8089]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:52 ceph-node-1 osd[7735]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:50:52 ceph-node-3 osd[3085]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:35 ceph-node-4 client[5143]: INFO: Data replication completed for object pool
2025-02-24 10:50:47 ceph-node-2 mgr[3560]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:50:39 ceph-node-5 mon[7493]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:58 ceph-node-4 radosgw[4318]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:50:40 ceph-node-3 osd[8363]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:50:57 ceph-node-5 mgr[2478]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:27 ceph-node-2 osd[5951]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:51:03 ceph-node-4 mon[1982]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:50:41 ceph-node-3 client[7035]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:05 ceph-node-5 mds[6447]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:19 ceph-node-5 osd[3371]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:30 ceph-node-4 osd[4035]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:31 ceph-node-3 radosgw[7079]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:50:41 ceph-node-4 mds[2049]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:30 ceph-node-4 osd[8907]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:50:50 ceph-node-1 mon[1150]: DEBUG: Data replication completed for object pool
2025-02-24 10:50:37 ceph-node-2 osd[3214]: DEBUG: OSD rebalancing completed
2025-02-24 10:51:02 ceph-node-3 mgr[9137]: WARNING: CRITICAL: PG 2.1 stuck in inconsistent state
2025-02-24 10:50:54 ceph-node-1 osd[3023]: CRITICAL: Monitor down, unable to connect to quorum
2025-02-24 10:51:23 ceph-node-4 osd[9092]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:51:07 ceph-node-5 osd[8527]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:51:09 ceph-node-2 mgr[1367]: INFO: PG 1.2 replicated successfully
2025-02-24 10:51:17 ceph-node-1 mon[6084]: INFO: Monitor map has been updated
2025-02-24 10:51:12 ceph-node-4 radosgw[9010]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:12 ceph-node-5 mon[1253]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:05 ceph-node-4 radosgw[9031]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:50:57 ceph-node-2 osd[4286]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:30 ceph-node-4 mon[2172]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:11 ceph-node-5 mon[7427]: NOTICE: OSD rebalancing completed
2025-02-24 10:50:37 ceph-node-3 mds[7485]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:02 ceph-node-5 radosgw[5761]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:55 ceph-node-4 mon[1487]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:50:47 ceph-node-4 mgr[3154]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:17 ceph-node-4 mds[2661]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:23 ceph-node-4 mgr[8819]: ERROR: Client connection timeout detected
2025-02-24 10:51:11 ceph-node-4 client[1749]: NOTICE: OSD rebalancing completed
2025-02-24 10:51:03 ceph-node-1 osd[6057]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:01 ceph-node-2 mgr[3967]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:51:14 ceph-node-3 mon[2816]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:23 ceph-node-2 radosgw[1667]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:36 ceph-node-1 mds[5657]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:14 ceph-node-1 osd[1955]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:51:11 ceph-node-5 radosgw[2674]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:34 ceph-node-5 client[7691]: WARNING: CRITICAL: Data loss detected on pool 1
2025-02-24 10:51:09 ceph-node-2 client[9142]: NOTICE: OSD rebalancing completed
2025-02-24 10:50:53 ceph-node-4 osd[3462]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:51:28 ceph-node-5 radosgw[7277]: WARNING: Cluster status: HEALTH_WARN
2025-02-24 10:50:54 ceph-node-4 mon[3236]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:13 ceph-node-4 radosgw[6785]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:02 ceph-node-5 mgr[4474]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:51:28 ceph-node-2 mgr[1065]: CRITICAL: Network partition detected between OSD nodes
2025-02-24 10:51:08 ceph-node-3 mon[3229]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:50:48 ceph-node-1 osd[2597]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:51:03 ceph-node-5 mgr[9187]: NOTICE: OSD rebalancing completed
2025-02-24 10:51:06 ceph-node-3 radosgw[6101]: NOTICE: MGR module loaded successfully
2025-02-24 10:51:14 ceph-node-3 mds[7266]: NOTICE: OSD rebalancing completed
2025-02-24 10:50:41 ceph-node-2 mon[4512]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:50:32 ceph-node-3 osd[2420]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:50:59 ceph-node-5 radosgw[7717]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:18 ceph-node-5 radosgw[9629]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:51:02 ceph-node-1 radosgw[6592]: WARNING: Monitor mon1 failed to reach quorum
2025-02-24 10:50:50 ceph-node-3 mgr[5907]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:50:42 ceph-node-3 mon[4799]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:24 ceph-node-3 osd[3430]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:16 ceph-node-1 osd[5357]: DEBUG: Monitor map has been updated
2025-02-24 10:51:26 ceph-node-3 mon[5938]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:50:57 ceph-node-1 osd[6353]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:50:51 ceph-node-1 mds[2054]: WARNING: Slow request detected on OSD 4.3
2025-02-24 10:50:35 ceph-node-3 mds[2847]: CRITICAL: Journal write failure detected on OSD node
2025-02-24 10:50:32 ceph-node-4 mds[1931]: WARNING: Network partition detected between OSD nodes
2025-02-24 10:51:16 ceph-node-5 osd[8728]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:27 ceph-node-2 radosgw[3607]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:48 ceph-node-4 mgr[6154]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:23 ceph-node-1 radosgw[4636]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:24 ceph-node-5 osd[5084]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:51:31 ceph-node-4 osd[9752]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:51:25 ceph-node-4 mon[7483]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:51:02 ceph-node-5 mgr[6000]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:50:54 ceph-node-5 mgr[7019]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:15 ceph-node-4 mds[8338]: INFO: PG 1.2 replicated successfully
2025-02-24 10:50:54 ceph-node-2 mon[8622]: ERROR: Disk failure reported on ceph-node-4
2025-02-24 10:51:18 ceph-node-1 mon[1108]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:15 ceph-node-1 mds[8538]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:51:10 ceph-node-3 mds[4121]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:16 ceph-node-5 mon[3758]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:51:23 ceph-node-1 mon[3225]: INFO: OSD disk space utilization: 70%
2025-02-24 10:50:47 ceph-node-1 osd[2328]: INFO: OSD disk space utilization: 70%
2025-02-24 10:50:47 ceph-node-4 mon[1550]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:01 ceph-node-1 mgr[2817]: INFO: Monitor map has been updated
2025-02-24 10:51:26 ceph-node-3 osd[6750]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:42 ceph-node-3 mds[6820]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:51:11 ceph-node-1 mon[5240]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:30 ceph-node-4 osd[1336]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:23 ceph-node-4 client[7721]: CRITICAL: Uncorrectable ECC error detected in MDS cache
2025-02-24 10:51:10 ceph-node-4 radosgw[6705]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:50:55 ceph-node-5 radosgw[7721]: INFO: OSD disk space utilization: 70%
2025-02-24 10:51:12 ceph-node-1 mon[9562]: NOTICE: OSD rebalancing completed
2025-02-24 10:51:14 ceph-node-5 mon[9677]: NOTICE: MGR module loaded successfully
2025-02-24 10:50:44 ceph-node-3 mds[9158]: ERROR: Journal write failure detected on OSD node
2025-02-24 10:50:36 ceph-node-3 mds[6842]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:51:22 ceph-node-2 osd[5442]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:50:32 ceph-node-3 mds[8217]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:47 ceph-node-1 osd[3149]: NOTICE: MGR module loaded successfully
2025-02-24 10:50:41 ceph-node-3 radosgw[1223]: INFO: OSD rebalancing completed
2025-02-24 10:51:12 ceph-node-2 client[8313]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:07 ceph-node-3 mgr[4668]: ERROR: MDS failure detected: metadata inconsistency
2025-02-24 10:50:55 ceph-node-1 mon[4877]: NOTICE: MGR module loaded successfully
2025-02-24 10:50:43 ceph-node-2 client[8223]: DEBUG: OSD rebalancing completed
2025-02-24 10:51:05 ceph-node-5 mgr[6834]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:50:33 ceph-node-5 mds[3015]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:51:27 ceph-node-3 osd[9694]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:48 ceph-node-5 radosgw[9480]: NOTICE: MGR module loaded successfully
2025-02-24 10:50:43 ceph-node-1 mgr[2624]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:50:37 ceph-node-5 osd[5734]: CRITICAL: CRITICAL: PG 2.1 stuck in inconsistent state
2025-02-24 10:51:18 ceph-node-1 mon[8842]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:51:12 ceph-node-5 mds[4669]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:50:46 ceph-node-2 radosgw[7875]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:29 ceph-node-5 mds[2838]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:18 ceph-node-2 mgr[4974]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:51:23 ceph-node-4 client[4378]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:50:46 ceph-node-5 radosgw[9146]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:10 ceph-node-4 radosgw[2839]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:26 ceph-node-5 osd[2652]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:50:55 ceph-node-1 mgr[5379]: CRITICAL: CRITICAL: Data loss detected on pool 1
2025-02-24 10:50:44 ceph-node-4 mon[5895]: INFO: OSD rebalancing completed
2025-02-24 10:51:24 ceph-node-5 client[8155]: CRITICAL: Slow request detected on OSD 4.3
2025-02-24 10:51:03 ceph-node-2 client[8940]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:28 ceph-node-3 osd[2657]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:51:12 ceph-node-1 mgr[7012]: NOTICE: MGR module loaded successfully
2025-02-24 10:50:53 ceph-node-2 client[3527]: NOTICE: OSD rebalancing completed
2025-02-24 10:50:59 ceph-node-5 mds[6842]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:50:45 ceph-node-2 client[8299]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:50:47 ceph-node-3 client[7596]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:51:15 ceph-node-3 mds[9556]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:50:56 ceph-node-2 radosgw[1947]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:50:36 ceph-node-2 mon[2581]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:50:31 ceph-node-2 osd[5205]: INFO: Data replication completed for object pool
2025-02-24 10:50:56 ceph-node-3 client[1375]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:51:21 ceph-node-4 radosgw[9622]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:00 ceph-node-2 mds[3378]: CRITICAL: Slow request detected on OSD 4.3
2025-02-24 10:50:41 ceph-node-5 mds[9925]: INFO: Data replication completed for object pool
2025-02-24 10:51:12 ceph-node-3 osd[3854]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:51:19 ceph-node-2 client[6937]: INFO: Data replication completed for object pool
2025-02-24 10:50:33 ceph-node-5 mgr[8896]: ERROR: High I/O latency detected in radosgw service
2025-02-24 10:51:08 ceph-node-1 client[5893]: DEBUG: Data replication completed for object pool
2025-02-24 10:50:36 ceph-node-1 client[3553]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:50:43 ceph-node-5 mon[3989]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:50:58 ceph-node-2 client[8942]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:51:27 ceph-node-5 client[3201]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:50:39 ceph-node-5 osd[9661]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:12 ceph-node-5 osd[8623]: INFO: PG 1.2 replicated successfully
2025-02-24 10:50:55 ceph-node-5 mon[4964]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:51:11 ceph-node-5 radosgw[1061]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:29 ceph-node-4 radosgw[3228]: INFO: Data replication completed for object pool
2025-02-24 10:50:50 ceph-node-3 mon[1627]: DEBUG: Data replication completed for object pool
2025-02-24 10:50:37 ceph-node-5 mds[8817]: INFO: PG 1.2 replicated successfully
2025-02-24 10:51:11 ceph-node-2 radosgw[4608]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:50:36 ceph-node-4 osd[7812]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:01 ceph-node-5 osd[9066]: INFO: OSD rebalancing completed
2025-02-24 10:51:31 ceph-node-1 mon[2872]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:39 ceph-node-2 osd[3033]: NOTICE: OSD rebalancing completed
2025-02-24 10:50:37 ceph-node-4 mds[1424]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:50:56 ceph-node-4 radosgw[1554]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:22 ceph-node-3 osd[5597]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:50:51 ceph-node-2 mgr[7678]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:33 ceph-node-3 radosgw[5988]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:37 ceph-node-2 osd[1327]: INFO: PG 1.2 replicated successfully
2025-02-24 10:51:06 ceph-node-1 client[3935]: NOTICE: Data replication completed for object pool
2025-02-24 10:50:43 ceph-node-4 client[7243]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:56 ceph-node-1 radosgw[8199]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:51:02 ceph-node-5 radosgw[9916]: NOTICE: Monitor map has been updated
2025-02-24 10:50:54 ceph-node-4 client[5983]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:08 ceph-node-2 mds[6278]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:50:42 ceph-node-5 osd[3606]: INFO: Monitor map has been updated
2025-02-24 10:51:14 ceph-node-1 client[4577]: INFO: Data replication completed for object pool
2025-02-24 10:51:09 ceph-node-2 radosgw[4924]: WARNING: Monitor mon1 failed to reach quorum
2025-02-24 10:51:13 ceph-node-2 osd[8363]: INFO: OSD rebalancing completed
2025-02-24 10:51:02 ceph-node-2 mon[7357]: INFO: PG 1.2 replicated successfully
2025-02-24 10:50:59 ceph-node-1 mgr[1893]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:50:56 ceph-node-2 mon[1374]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:11 ceph-node-4 radosgw[7188]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:50:41 ceph-node-3 mds[8313]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:05 ceph-node-2 mon[8557]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:48 ceph-node-5 mon[3039]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:50:59 ceph-node-1 radosgw[2983]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:56 ceph-node-2 mon[9335]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:51:08 ceph-node-1 mgr[6310]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:50:35 ceph-node-3 mds[4580]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:50:40 ceph-node-2 mgr[4511]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:51:13 ceph-node-4 mon[3969]: DEBUG: Data replication completed for object pool
2025-02-24 10:50:35 ceph-node-5 mon[2662]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:51:07 ceph-node-2 mds[9056]: NOTICE: Monitor map has been updated
2025-02-24 10:51:17 ceph-node-2 mon[2348]: DEBUG: OSD rebalancing completed
2025-02-24 10:50:58 ceph-node-5 client[6796]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:50:42 ceph-node-1 client[9811]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:51:16 ceph-node-3 radosgw[5983]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:38 ceph-node-5 osd[4800]: ERROR: Monitor mon1 failed to reach quorum
2025-02-24 10:50:42 ceph-node-2 mgr[1866]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:50:41 ceph-node-1 mds[3564]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:50:37 ceph-node-3 client[3100]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:50:47 ceph-node-4 radosgw[4878]: NOTICE: Data replication completed for object pool
2025-02-24 10:50:53 ceph-node-5 mon[5792]: DEBUG: Monitor map has been updated
2025-02-24 10:50:47 ceph-node-5 mon[6263]: INFO: PG 1.2 replicated successfully
2025-02-24 10:51:09 ceph-node-1 client[9796]: WARNING: High I/O latency detected in radosgw service
2025-02-24 10:51:21 ceph-node-4 mon[7293]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:18 ceph-node-4 client[2786]: ERROR: Slow request detected on OSD 4.3
2025-02-24 10:50:42 ceph-node-3 mon[4386]: INFO: PG 1.2 replicated successfully
2025-02-24 10:50:56 ceph-node-3 client[9802]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:50:57 ceph-node-2 radosgw[8469]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:49 ceph-node-2 mon[5502]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:29 ceph-node-5 mgr[4595]: INFO: OSD disk space utilization: 70%
2025-02-24 10:51:11 ceph-node-5 mds[4291]: CRITICAL: Authentication failure in RADOS Gateway
2025-02-24 10:50:47 ceph-node-4 client[6812]: WARNING: Uncorrectable ECC error detected in MDS cache
2025-02-24 10:50:32 ceph-node-1 client[8866]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:22 ceph-node-5 mon[4983]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:51:14 ceph-node-1 osd[3539]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:56 ceph-node-2 mon[9141]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:34 ceph-node-5 mon[7323]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:26 ceph-node-2 mon[2479]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:43 ceph-node-4 mds[9096]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:45 ceph-node-5 radosgw[2440]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:23 ceph-node-2 radosgw[8277]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:51:31 ceph-node-4 radosgw[7459]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:24 ceph-node-5 client[2931]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:50:32 ceph-node-5 radosgw[2205]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:54 ceph-node-5 radosgw[3910]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:03 ceph-node-3 mgr[5137]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:50:52 ceph-node-2 client[8981]: DEBUG: MGR module loaded successfully
2025-02-24 10:50:34 ceph-node-4 osd[4743]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:51:28 ceph-node-1 mgr[3955]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:55 ceph-node-5 mon[9744]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:50:45 ceph-node-2 client[4272]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:50:56 ceph-node-5 client[7959]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:51:22 ceph-node-2 mds[1378]: WARNING: Uncorrectable ECC error detected in MDS cache
2025-02-24 10:51:06 ceph-node-2 mds[3106]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:51:05 ceph-node-4 client[7408]: NOTICE: MGR module loaded successfully
2025-02-24 10:51:12 ceph-node-1 mon[4681]: INFO: Monitor map has been updated
2025-02-24 10:51:16 ceph-node-2 mds[6999]: WARNING: MDS failure detected: metadata inconsistency
2025-02-24 10:51:06 ceph-node-2 mon[7864]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:23 ceph-node-2 mds[5376]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:29 ceph-node-3 client[4483]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:34 ceph-node-4 mgr[4271]: NOTICE: MGR module loaded successfully
2025-02-24 10:51:27 ceph-node-3 mgr[3941]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:00 ceph-node-3 radosgw[8208]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:50:44 ceph-node-2 mon[4346]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:51:29 ceph-node-2 mds[9484]: WARNING: Cluster status: HEALTH_WARN
2025-02-24 10:50:47 ceph-node-5 client[8854]: DEBUG: OSD rebalancing completed
2025-02-24 10:50:37 ceph-node-2 mon[2618]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:50:52 ceph-node-5 mon[7738]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:50:58 ceph-node-2 mgr[2572]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:20 ceph-node-4 mgr[2336]: INFO: PG 1.2 replicated successfully
2025-02-24 10:50:39 ceph-node-5 mgr[2211]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:28 ceph-node-5 client[1315]: NOTICE: Monitor map has been updated
2025-02-24 10:50:35 ceph-node-1 mds[1642]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:16 ceph-node-2 osd[1673]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:27 ceph-node-2 osd[2685]: NOTICE: MGR module loaded successfully
2025-02-24 10:51:18 ceph-node-4 client[1692]: INFO: OSD disk space utilization: 70%
2025-02-24 10:50:37 ceph-node-4 client[6951]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:42 ceph-node-4 radosgw[3312]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:51:17 ceph-node-3 radosgw[8073]: ERROR: Monitor down, unable to connect to quorum
2025-02-24 10:51:15 ceph-node-4 radosgw[6779]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:50:48 ceph-node-4 mon[1819]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:50:38 ceph-node-3 mon[6897]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:50:55 ceph-node-2 mon[6690]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:50:35 ceph-node-4 mon[3229]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:51:17 ceph-node-2 mgr[5631]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:51:00 ceph-node-5 mon[9775]: NOTICE: Monitor map has been updated
2025-02-24 10:51:13 ceph-node-1 client[2403]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:50:31 ceph-node-4 mds[2897]: CRITICAL: Uncorrectable ECC error detected in MDS cache
2025-02-24 10:51:30 ceph-node-5 mds[5934]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:50:53 ceph-node-4 mon[2934]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:50:59 ceph-node-1 osd[4429]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:50:46 ceph-node-1 client[1293]: NOTICE: MGR module loaded successfully
2025-02-24 10:51:02 ceph-node-2 osd[8717]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:50:36 ceph-node-1 mds[3553]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:51:11 ceph-node-3 mgr[1303]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:30 ceph-node-3 mds[8969]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:51:25 ceph-node-5 radosgw[2505]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:56 ceph-node-1 radosgw[6496]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:50:51 ceph-node-3 mon[2763]: INFO: PG 1.2 replicated successfully
2025-02-24 10:50:48 ceph-node-2 mgr[7203]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:51:25 ceph-node-2 mds[4699]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:50:39 ceph-node-3 radosgw[6666]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:51:14 ceph-node-5 client[2900]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:50:35 ceph-node-1 mgr[7184]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:51:17 ceph-node-5 radosgw[9483]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:02 ceph-node-5 client[6302]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:10 ceph-node-2 osd[6204]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:43 ceph-node-3 mgr[9371]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:31 ceph-node-5 mgr[4149]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:23 ceph-node-2 client[9565]: INFO: PG 1.2 replicated successfully
2025-02-24 10:51:27 ceph-node-2 radosgw[8920]: CRITICAL: Journal write failure detected on OSD node
2025-02-24 10:50:45 ceph-node-4 mgr[1578]: WARNING: Data corruption detected in object pool
2025-02-24 10:50:34 ceph-node-1 client[4170]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:51:18 ceph-node-3 mds[7918]: NOTICE: Data replication completed for object pool
2025-02-24 10:51:23 ceph-node-4 osd[5202]: INFO: PG 1.2 replicated successfully
2025-02-24 10:51:24 ceph-node-4 mgr[9032]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:11 ceph-node-5 mds[6319]: CRITICAL: OSD 3.1 marked down due to heartbeat failure
2025-02-24 10:51:06 ceph-node-1 osd[6713]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:20 ceph-node-3 mds[3625]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:05 ceph-node-5 mgr[4662]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:03 ceph-node-5 client[2508]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:51:22 ceph-node-4 mon[1747]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:25 ceph-node-4 mgr[8074]: DEBUG: MGR module loaded successfully
2025-02-24 10:50:58 ceph-node-2 mon[6425]: NOTICE: Monitor map has been updated
2025-02-24 10:50:35 ceph-node-1 mon[1340]: WARNING: Uncorrectable ECC error detected in MDS cache
2025-02-24 10:51:14 ceph-node-4 mon[1103]: WARNING: MDS failure detected: metadata inconsistency
2025-02-24 10:50:59 ceph-node-5 radosgw[6004]: INFO: Monitor map has been updated
2025-02-24 10:51:28 ceph-node-4 radosgw[4076]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:15 ceph-node-1 radosgw[2304]: WARNING: Uncorrectable ECC error detected in MDS cache
2025-02-24 10:51:27 ceph-node-2 radosgw[5546]: INFO: Monitor map has been updated
2025-02-24 10:51:25 ceph-node-1 mgr[9933]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:01 ceph-node-5 radosgw[5562]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:07 ceph-node-5 osd[5039]: INFO: OSD rebalancing completed
2025-02-24 10:50:34 ceph-node-4 client[3526]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:51:17 ceph-node-2 mds[3449]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:50:40 ceph-node-5 mgr[7952]: NOTICE: OSD rebalancing completed
2025-02-24 10:50:59 ceph-node-3 mds[1656]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:50:43 ceph-node-1 radosgw[8383]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:50:44 ceph-node-1 client[1296]: WARNING: CRITICAL: Data loss detected on pool 1
2025-02-24 10:50:59 ceph-node-1 client[8301]: DEBUG: Monitor map has been updated
2025-02-24 10:50:39 ceph-node-4 mgr[4113]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:30 ceph-node-1 radosgw[7069]: DEBUG: Data replication completed for object pool
2025-02-24 10:50:53 ceph-node-4 mon[2908]: CRITICAL: Cluster status: HEALTH_WARN
2025-02-24 10:51:03 ceph-node-1 radosgw[4273]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:31 ceph-node-4 mds[5745]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:51:09 ceph-node-3 mds[7781]: DEBUG: Monitor map has been updated
2025-02-24 10:50:48 ceph-node-1 client[9908]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:51:15 ceph-node-4 mds[3262]: INFO: Data replication completed for object pool
2025-02-24 10:50:57 ceph-node-4 client[1202]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:34 ceph-node-2 mon[1946]: CRITICAL: Authentication failure in RADOS Gateway
2025-02-24 10:51:19 ceph-node-3 osd[9966]: INFO: MGR module loaded successfully
2025-02-24 10:51:11 ceph-node-2 mgr[2205]: INFO: OSD disk space utilization: 70%
2025-02-24 10:50:35 ceph-node-3 radosgw[7668]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:49 ceph-node-2 mon[6364]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:42 ceph-node-4 radosgw[3804]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:51:23 ceph-node-1 mgr[2851]: NOTICE: OSD rebalancing completed
2025-02-24 10:50:31 ceph-node-1 osd[1801]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:16 ceph-node-3 mgr[2452]: INFO: Monitor map has been updated
2025-02-24 10:51:04 ceph-node-4 radosgw[1529]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:58 ceph-node-1 osd[2075]: INFO: OSD disk space utilization: 70%
2025-02-24 10:51:13 ceph-node-2 mds[3482]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:40 ceph-node-1 radosgw[5368]: INFO: Monitor map has been updated
2025-02-24 10:51:02 ceph-node-5 osd[5604]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:51:26 ceph-node-1 radosgw[4671]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:11 ceph-node-3 mon[9202]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:51:07 ceph-node-2 mgr[9113]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:50:59 ceph-node-2 mgr[4226]: NOTICE: OSD rebalancing completed
2025-02-24 10:50:44 ceph-node-2 client[2425]: CRITICAL: Cluster status: HEALTH_WARN
2025-02-24 10:51:23 ceph-node-2 mgr[8220]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:50:43 ceph-node-1 radosgw[2266]: NOTICE: Monitor map has been updated
2025-02-24 10:50:44 ceph-node-1 radosgw[9708]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:05 ceph-node-1 client[1257]: CRITICAL: OSD disk space utilization exceeded 90%
2025-02-24 10:50:37 ceph-node-1 mgr[2550]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:50:52 ceph-node-2 radosgw[8913]: INFO: PG 1.2 replicated successfully
2025-02-24 10:50:36 ceph-node-3 radosgw[7090]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:59 ceph-node-4 osd[2025]: NOTICE: MGR module loaded successfully
2025-02-24 10:51:12 ceph-node-1 osd[4552]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:50:31 ceph-node-1 client[6755]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:46 ceph-node-1 radosgw[2381]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:51:13 ceph-node-5 osd[2435]: ERROR: Slow request detected on OSD 4.3
2025-02-24 10:50:51 ceph-node-4 client[4785]: NOTICE: Monitor map has been updated
2025-02-24 10:51:04 ceph-node-3 mon[1347]: INFO: MGR module loaded successfully
2025-02-24 10:50:38 ceph-node-4 mds[9250]: INFO: OSD rebalancing completed
2025-02-24 10:51:21 ceph-node-2 radosgw[5543]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:51:23 ceph-node-4 mds[2980]: DEBUG: Monitor map has been updated
2025-02-24 10:51:20 ceph-node-2 osd[7733]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:13 ceph-node-5 mds[1982]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:35 ceph-node-3 radosgw[1507]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:12 ceph-node-1 mds[4615]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:50:54 ceph-node-5 mgr[7899]: CRITICAL: Disk failure reported on ceph-node-4
2025-02-24 10:51:15 ceph-node-3 mds[8788]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:18 ceph-node-2 radosgw[5056]: INFO: Data replication completed for object pool
2025-02-24 10:51:06 ceph-node-5 client[6395]: ERROR: Cluster status: HEALTH_WARN
2025-02-24 10:50:36 ceph-node-5 client[9646]: INFO: Monitor map has been updated
2025-02-24 10:51:23 ceph-node-2 osd[6024]: DEBUG: Data replication completed for object pool
2025-02-24 10:50:49 ceph-node-1 mon[5667]: NOTICE: MGR module loaded successfully
2025-02-24 10:51:21 ceph-node-2 mon[1301]: ERROR: Cluster status: HEALTH_WARN
2025-02-24 10:51:09 ceph-node-3 client[1014]: INFO: OSD disk space utilization: 70%
2025-02-24 10:51:06 ceph-node-2 radosgw[5440]: DEBUG: OSD rebalancing completed
2025-02-24 10:51:28 ceph-node-4 mon[8945]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:00 ceph-node-1 mon[1585]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:50:36 ceph-node-2 osd[3312]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:14 ceph-node-4 client[6263]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:34 ceph-node-3 radosgw[7113]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:49 ceph-node-4 mds[6087]: INFO: OSD rebalancing completed
2025-02-24 10:51:03 ceph-node-2 mgr[2597]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:33 ceph-node-1 radosgw[4807]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:27 ceph-node-5 mds[8342]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:50:56 ceph-node-2 radosgw[6147]: WARNING: Uncorrectable ECC error detected in MDS cache
2025-02-24 10:51:06 ceph-node-2 client[5574]: DEBUG: Data replication completed for object pool
2025-02-24 10:50:43 ceph-node-3 osd[8325]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:51:24 ceph-node-4 mon[3378]: ERROR: Client connection timeout detected
2025-02-24 10:50:50 ceph-node-4 mgr[6274]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:51:23 ceph-node-1 mds[8628]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:44 ceph-node-3 mds[5252]: WARNING: Client connection timeout detected
2025-02-24 10:50:54 ceph-node-3 mds[1186]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:12 ceph-node-1 client[6075]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:03 ceph-node-1 client[7638]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:50:43 ceph-node-2 radosgw[4753]: NOTICE: MGR module loaded successfully
2025-02-24 10:50:53 ceph-node-3 radosgw[7569]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:10 ceph-node-5 osd[4746]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:51:24 ceph-node-4 osd[4739]: WARNING: Cluster status: HEALTH_WARN
2025-02-24 10:50:48 ceph-node-5 mon[8949]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:50:31 ceph-node-5 radosgw[3516]: NOTICE: Data replication completed for object pool
2025-02-24 10:51:24 ceph-node-4 osd[5961]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:23 ceph-node-3 mon[7659]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:50:36 ceph-node-3 mon[8341]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:50:41 ceph-node-5 mgr[6076]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:13 ceph-node-4 radosgw[7839]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:51:02 ceph-node-4 osd[1547]: ERROR: CRITICAL: PG 2.1 stuck in inconsistent state
2025-02-24 10:50:50 ceph-node-1 mgr[9067]: NOTICE: MGR module loaded successfully
2025-02-24 10:50:49 ceph-node-2 radosgw[2174]: NOTICE: MGR module loaded successfully
2025-02-24 10:51:10 ceph-node-4 osd[7530]: NOTICE: Data replication completed for object pool
2025-02-24 10:51:08 ceph-node-1 mon[1047]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:51:23 ceph-node-1 mon[8694]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:50:39 ceph-node-4 client[2367]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:35 ceph-node-3 mds[9268]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:58 ceph-node-4 osd[4183]: INFO: OSD rebalancing completed
2025-02-24 10:50:49 ceph-node-4 radosgw[8666]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:02 ceph-node-4 mgr[5296]: CRITICAL: Slow request detected on OSD 4.3
2025-02-24 10:51:01 ceph-node-4 mds[4805]: INFO: Monitor map has been updated
2025-02-24 10:51:03 ceph-node-1 mgr[9509]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:50:59 ceph-node-1 radosgw[5660]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:42 ceph-node-1 client[4461]: ERROR: Monitor down, unable to connect to quorum
2025-02-24 10:50:40 ceph-node-2 mon[1315]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:39 ceph-node-1 osd[7335]: NOTICE: Data replication completed for object pool
2025-02-24 10:51:24 ceph-node-5 osd[4723]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:32 ceph-node-4 mgr[1422]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:51:24 ceph-node-3 radosgw[8594]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:50:48 ceph-node-4 radosgw[3836]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:31 ceph-node-4 mgr[3307]: NOTICE: Monitor map has been updated
2025-02-24 10:51:08 ceph-node-4 mds[5224]: WARNING: High I/O latency detected in radosgw service
2025-02-24 10:51:19 ceph-node-1 mgr[5243]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:02 ceph-node-1 osd[8450]: INFO: Data replication completed for object pool
2025-02-24 10:51:10 ceph-node-1 client[7478]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:12 ceph-node-5 client[4439]: INFO: Data replication completed for object pool
2025-02-24 10:50:49 ceph-node-5 mon[1024]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:32 ceph-node-2 mon[3558]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:50:59 ceph-node-2 mon[3693]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:50:31 ceph-node-4 mgr[9170]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:51:12 ceph-node-3 osd[1411]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:36 ceph-node-3 client[2644]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:50:32 ceph-node-1 mds[8833]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:22 ceph-node-5 radosgw[2896]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:07 ceph-node-1 mgr[3786]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:51:17 ceph-node-3 mon[7987]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:08 ceph-node-3 mds[2863]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:51:10 ceph-node-3 osd[8927]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:50:36 ceph-node-4 client[7017]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:50:54 ceph-node-2 mon[4607]: INFO: OSD disk space utilization: 70%
2025-02-24 10:50:55 ceph-node-1 mon[9928]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:53 ceph-node-1 mds[8101]: INFO: OSD rebalancing completed
2025-02-24 10:50:59 ceph-node-5 mds[1019]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:10 ceph-node-1 client[9896]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:50:40 ceph-node-1 radosgw[4365]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:51:20 ceph-node-4 client[7854]: CRITICAL: Data corruption detected in object pool
2025-02-24 10:50:48 ceph-node-1 osd[1432]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:18 ceph-node-4 mon[9072]: INFO: MGR module loaded successfully
2025-02-24 10:51:13 ceph-node-3 mon[9295]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:53 ceph-node-5 mds[7361]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:12 ceph-node-3 radosgw[3015]: INFO: OSD disk space utilization: 70%
2025-02-24 10:51:24 ceph-node-2 mds[1524]: INFO: Data replication completed for object pool
2025-02-24 10:50:33 ceph-node-1 osd[1876]: CRITICAL: Network partition detected between OSD nodes
2025-02-24 10:51:20 ceph-node-3 osd[1023]: INFO: OSD rebalancing completed
2025-02-24 10:50:41 ceph-node-1 mgr[4371]: INFO: OSD disk space utilization: 70%
2025-02-24 10:51:06 ceph-node-4 mgr[8806]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:16 ceph-node-2 osd[7808]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:50:32 ceph-node-1 mds[2656]: NOTICE: Monitor map has been updated
2025-02-24 10:50:59 ceph-node-5 mgr[7687]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:50:39 ceph-node-1 client[3050]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:59 ceph-node-2 mon[1762]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:57 ceph-node-1 client[6285]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:30 ceph-node-4 client[6249]: NOTICE: Data replication completed for object pool
2025-02-24 10:51:30 ceph-node-2 radosgw[7747]: DEBUG: Monitor map has been updated
2025-02-24 10:51:26 ceph-node-1 mgr[9807]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:51:10 ceph-node-3 radosgw[4163]: WARNING: Slow request detected on OSD 4.3
2025-02-24 10:51:03 ceph-node-5 osd[9733]: CRITICAL: Cluster status: HEALTH_WARN
2025-02-24 10:50:38 ceph-node-2 osd[4702]: INFO: PG 1.2 replicated successfully
2025-02-24 10:50:55 ceph-node-1 client[5860]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:51:26 ceph-node-4 mds[8225]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:51:19 ceph-node-1 client[5155]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:47 ceph-node-2 radosgw[4917]: DEBUG: Data replication completed for object pool
2025-02-24 10:50:36 ceph-node-4 radosgw[2507]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:50:56 ceph-node-2 mds[1481]: INFO: Monitor map has been updated
2025-02-24 10:51:08 ceph-node-5 mgr[6778]: ERROR: Data corruption detected in object pool
2025-02-24 10:51:09 ceph-node-4 client[4972]: NOTICE: MGR module loaded successfully
2025-02-24 10:51:22 ceph-node-2 osd[6913]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:51:06 ceph-node-3 mgr[8556]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:27 ceph-node-4 osd[3646]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:50:39 ceph-node-5 mon[2332]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:38 ceph-node-2 mds[4240]: INFO: MGR module loaded successfully
2025-02-24 10:50:58 ceph-node-3 mon[6848]: NOTICE: OSD rebalancing completed
2025-02-24 10:51:04 ceph-node-4 mds[4385]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:51:21 ceph-node-5 osd[4107]: NOTICE: MGR module loaded successfully
2025-02-24 10:51:25 ceph-node-4 mon[1298]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:30 ceph-node-1 mon[7202]: NOTICE: Monitor map has been updated
2025-02-24 10:50:52 ceph-node-3 mon[7779]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:16 ceph-node-5 mon[2663]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:10 ceph-node-4 mds[3767]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:51:30 ceph-node-4 radosgw[9324]: DEBUG: Monitor map has been updated
2025-02-24 10:51:01 ceph-node-5 mon[5974]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:51 ceph-node-1 mgr[9249]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:04 ceph-node-2 mon[1334]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:50:47 ceph-node-4 mon[7797]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:48 ceph-node-2 mon[9148]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:15 ceph-node-5 radosgw[3074]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:50:50 ceph-node-1 mon[4714]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:32 ceph-node-4 mon[7778]: DEBUG: Monitor map has been updated
2025-02-24 10:50:54 ceph-node-1 mon[5993]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:50:53 ceph-node-5 mgr[8222]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:57 ceph-node-3 radosgw[7844]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:21 ceph-node-3 mon[6174]: DEBUG: Monitor map has been updated
2025-02-24 10:50:58 ceph-node-2 osd[3377]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:08 ceph-node-5 osd[1531]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:50:53 ceph-node-4 radosgw[6329]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:51:13 ceph-node-2 osd[3507]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:05 ceph-node-4 mds[4431]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:09 ceph-node-1 mgr[7340]: DEBUG: OSD rebalancing completed
2025-02-24 10:50:39 ceph-node-5 mgr[1835]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:51:25 ceph-node-1 client[2650]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:51:07 ceph-node-3 mon[4209]: DEBUG: Monitor map has been updated
2025-02-24 10:51:00 ceph-node-2 mgr[7673]: WARNING: CRITICAL: PG 2.1 stuck in inconsistent state
2025-02-24 10:50:50 ceph-node-5 radosgw[6945]: CRITICAL: High I/O latency detected in radosgw service
2025-02-24 10:51:00 ceph-node-1 mon[1351]: DEBUG: Monitor map has been updated
2025-02-24 10:51:19 ceph-node-4 client[6281]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:50:50 ceph-node-4 radosgw[9321]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:50:37 ceph-node-5 mon[9556]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:12 ceph-node-5 osd[8268]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:51:31 ceph-node-4 osd[9467]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:50:35 ceph-node-4 mon[5270]: DEBUG: Monitor map has been updated
2025-02-24 10:51:14 ceph-node-4 radosgw[5122]: INFO: OSD disk space utilization: 70%
2025-02-24 10:50:57 ceph-node-4 osd[3544]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:50:48 ceph-node-5 mgr[5313]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:43 ceph-node-4 mgr[7262]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:33 ceph-node-2 radosgw[8639]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:50:45 ceph-node-2 osd[9923]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:50:52 ceph-node-4 mds[6494]: WARNING: Client connection timeout detected
2025-02-24 10:51:06 ceph-node-2 mon[1172]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:57 ceph-node-4 mgr[8253]: DEBUG: Monitor map has been updated
2025-02-24 10:50:48 ceph-node-1 mds[2102]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:50:51 ceph-node-1 radosgw[9732]: NOTICE: Monitor map has been updated
2025-02-24 10:50:32 ceph-node-2 radosgw[3106]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:19 ceph-node-5 osd[3347]: NOTICE: MGR module loaded successfully
2025-02-24 10:51:24 ceph-node-1 radosgw[5413]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:06 ceph-node-3 osd[2142]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:15 ceph-node-4 mon[7244]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:50:39 ceph-node-4 mon[2165]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:51:13 ceph-node-5 mgr[8494]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:02 ceph-node-3 mon[6943]: INFO: Data replication completed for object pool
2025-02-24 10:50:49 ceph-node-4 client[5796]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:50:42 ceph-node-5 osd[1583]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:50:47 ceph-node-4 mgr[7859]: CRITICAL: Cluster status: HEALTH_WARN
2025-02-24 10:51:17 ceph-node-5 mon[1775]: INFO: OSD rebalancing completed
2025-02-24 10:51:04 ceph-node-1 mgr[1510]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:30 ceph-node-4 mon[2856]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:51:01 ceph-node-1 mon[8942]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:50:43 ceph-node-5 mon[7906]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:51:21 ceph-node-3 mon[7725]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:19 ceph-node-1 radosgw[2513]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:50:54 ceph-node-4 client[3494]: NOTICE: Monitor map has been updated
2025-02-24 10:51:30 ceph-node-1 client[5994]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:32 ceph-node-5 client[3052]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:26 ceph-node-5 mds[2299]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:30 ceph-node-3 mgr[9389]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:51:07 ceph-node-5 client[8737]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:20 ceph-node-2 radosgw[2686]: WARNING: OSD disk space utilization exceeded 90%
2025-02-24 10:50:39 ceph-node-4 mon[5299]: CRITICAL: OSD 3.1 marked down due to heartbeat failure
2025-02-24 10:50:35 ceph-node-3 mon[6815]: DEBUG: OSD rebalancing completed
2025-02-24 10:50:46 ceph-node-3 client[3018]: INFO: OSD rebalancing completed
2025-02-24 10:51:27 ceph-node-4 osd[3558]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:51:20 ceph-node-1 radosgw[9437]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:23 ceph-node-1 mds[7625]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:51:10 ceph-node-4 radosgw[2237]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:50:36 ceph-node-4 mgr[2001]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:59 ceph-node-3 mon[7382]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:51:08 ceph-node-4 client[8370]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:51:20 ceph-node-2 mds[9425]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:21 ceph-node-1 mon[5946]: CRITICAL: Disk failure reported on ceph-node-4
2025-02-24 10:51:02 ceph-node-2 radosgw[9606]: INFO: OSD disk space utilization: 70%
2025-02-24 10:51:08 ceph-node-5 mgr[3725]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:10 ceph-node-1 mgr[4332]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:32 ceph-node-4 mon[6107]: ERROR: Data corruption detected in object pool
2025-02-24 10:51:13 ceph-node-4 mgr[3258]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:50:32 ceph-node-5 radosgw[9138]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:00 ceph-node-3 mgr[7891]: CRITICAL: High I/O latency detected in radosgw service
2025-02-24 10:50:50 ceph-node-1 mgr[9093]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:29 ceph-node-2 osd[3110]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:50:49 ceph-node-1 radosgw[2309]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:55 ceph-node-2 osd[4791]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:01 ceph-node-1 mds[8255]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:50 ceph-node-4 mon[5564]: CRITICAL: Slow request detected on OSD 4.3
2025-02-24 10:50:53 ceph-node-4 mon[3869]: INFO: PG 1.2 replicated successfully
2025-02-24 10:51:19 ceph-node-3 mds[6041]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:51:31 ceph-node-1 mgr[4070]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:19 ceph-node-4 osd[9963]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:18 ceph-node-5 client[5726]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:08 ceph-node-2 osd[3581]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:05 ceph-node-4 radosgw[3854]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:50:35 ceph-node-4 client[4436]: DEBUG: MGR module loaded successfully
2025-02-24 10:50:34 ceph-node-4 mds[7709]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:50:36 ceph-node-2 osd[1764]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:51:25 ceph-node-3 osd[7319]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:50:51 ceph-node-4 radosgw[1493]: CRITICAL: CRITICAL: Data loss detected on pool 1
2025-02-24 10:51:10 ceph-node-3 mgr[3310]: INFO: Monitor map has been updated
2025-02-24 10:50:36 ceph-node-1 client[2586]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:50:58 ceph-node-1 mds[3815]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:51:04 ceph-node-2 mds[2687]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:31 ceph-node-1 radosgw[8127]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:51:09 ceph-node-4 mgr[5041]: DEBUG: OSD rebalancing completed
2025-02-24 10:51:22 ceph-node-4 mon[5263]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:42 ceph-node-2 osd[2213]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:51:19 ceph-node-5 osd[8270]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:50:41 ceph-node-3 mgr[8851]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:07 ceph-node-4 mon[5193]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:13 ceph-node-3 osd[9380]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:51:25 ceph-node-3 osd[4281]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:24 ceph-node-5 mgr[7082]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:32 ceph-node-5 radosgw[3412]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:51:19 ceph-node-4 client[6246]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:49 ceph-node-2 mon[2261]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:51 ceph-node-2 mgr[7555]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:50:59 ceph-node-1 client[1310]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:23 ceph-node-3 mds[4634]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:50:41 ceph-node-1 client[8328]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:50:58 ceph-node-1 osd[7791]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:50:46 ceph-node-1 osd[5134]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:22 ceph-node-1 client[6548]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:50:41 ceph-node-1 mds[1131]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:07 ceph-node-3 radosgw[5818]: NOTICE: Data replication completed for object pool
2025-02-24 10:51:00 ceph-node-2 mon[8712]: INFO: PG 1.2 replicated successfully
2025-02-24 10:51:30 ceph-node-1 mgr[1109]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:51:30 ceph-node-2 radosgw[1175]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:28 ceph-node-5 mgr[3986]: INFO: MGR module loaded successfully
2025-02-24 10:51:30 ceph-node-2 osd[3353]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:02 ceph-node-5 mgr[3105]: NOTICE: MGR module loaded successfully
2025-02-24 10:50:37 ceph-node-4 radosgw[1571]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:16 ceph-node-3 mon[6153]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:50:58 ceph-node-4 mgr[2974]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:50:44 ceph-node-4 radosgw[7950]: DEBUG: OSD rebalancing completed
2025-02-24 10:50:53 ceph-node-2 radosgw[8652]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:46 ceph-node-1 mon[6056]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:16 ceph-node-2 mds[1160]: INFO: Data replication completed for object pool
2025-02-24 10:50:55 ceph-node-2 osd[4874]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:51:24 ceph-node-4 mds[4968]: DEBUG: Monitor map has been updated
2025-02-24 10:51:30 ceph-node-4 radosgw[3485]: INFO: OSD rebalancing completed
2025-02-24 10:51:11 ceph-node-4 client[9327]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:51:11 ceph-node-5 client[8040]: INFO: OSD rebalancing completed
2025-02-24 10:51:30 ceph-node-5 mon[2465]: NOTICE: OSD rebalancing completed
2025-02-24 10:51:25 ceph-node-4 mds[2188]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:51:11 ceph-node-4 mds[5940]: DEBUG: MGR module loaded successfully
2025-02-24 10:50:43 ceph-node-2 client[4789]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:49 ceph-node-5 client[7203]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:51:10 ceph-node-3 mon[4722]: NOTICE: Data replication completed for object pool
2025-02-24 10:51:02 ceph-node-3 client[9663]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:51:10 ceph-node-3 mgr[2636]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:30 ceph-node-2 mgr[7112]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:00 ceph-node-3 client[8961]: INFO: Data replication completed for object pool
2025-02-24 10:50:59 ceph-node-4 osd[2913]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:44 ceph-node-4 mon[2845]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:50:48 ceph-node-4 osd[2548]: CRITICAL: Uncorrectable ECC error detected in MDS cache
2025-02-24 10:51:26 ceph-node-1 mds[2350]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:50:39 ceph-node-3 osd[8795]: ERROR: Journal write failure detected on OSD node
2025-02-24 10:51:22 ceph-node-4 mds[3939]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:45 ceph-node-2 mgr[1175]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:50:55 ceph-node-4 mgr[5140]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:50:40 ceph-node-5 client[1242]: WARNING: Monitor mon1 failed to reach quorum
2025-02-24 10:50:33 ceph-node-2 radosgw[4894]: WARNING: Journal write failure detected on OSD node
2025-02-24 10:50:34 ceph-node-5 osd[3063]: NOTICE: Data replication completed for object pool
2025-02-24 10:50:45 ceph-node-3 osd[9952]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:50:49 ceph-node-2 mds[4139]: NOTICE: Monitor map has been updated
2025-02-24 10:50:32 ceph-node-2 radosgw[9348]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:51:22 ceph-node-4 radosgw[5073]: INFO: PG 1.2 replicated successfully
2025-02-24 10:50:41 ceph-node-4 client[7734]: INFO: OSD rebalancing completed
2025-02-24 10:51:11 ceph-node-3 osd[8052]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:19 ceph-node-2 osd[3040]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:51:19 ceph-node-4 mon[2303]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:50:47 ceph-node-1 osd[6885]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:25 ceph-node-4 radosgw[2771]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:28 ceph-node-1 client[7723]: INFO: OSD rebalancing completed
2025-02-24 10:51:12 ceph-node-5 mgr[4388]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:50:39 ceph-node-3 mds[1415]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:50:34 ceph-node-2 radosgw[7502]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:51:19 ceph-node-5 osd[6511]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:51 ceph-node-4 osd[1598]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:51:01 ceph-node-1 mds[2142]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:07 ceph-node-4 mon[8081]: CRITICAL: CRITICAL: Data loss detected on pool 1
2025-02-24 10:50:35 ceph-node-5 osd[8989]: WARNING: Network partition detected between OSD nodes
2025-02-24 10:51:19 ceph-node-1 mgr[1543]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:28 ceph-node-5 mds[4414]: INFO: MGR module loaded successfully
2025-02-24 10:50:46 ceph-node-5 osd[1319]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:25 ceph-node-3 mon[5622]: DEBUG: OSD rebalancing completed
2025-02-24 10:51:08 ceph-node-3 mds[4903]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:50:55 ceph-node-3 mds[7792]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:50:53 ceph-node-1 mds[6348]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:50:56 ceph-node-3 mon[6438]: INFO: OSD disk space utilization: 70%
2025-02-24 10:51:20 ceph-node-5 radosgw[4930]: WARNING: Monitor mon1 failed to reach quorum
2025-02-24 10:50:38 ceph-node-3 mgr[1508]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:14 ceph-node-4 mgr[1094]: INFO: OSD disk space utilization: 70%
2025-02-24 10:51:25 ceph-node-4 client[9250]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:20 ceph-node-2 mgr[1736]: WARNING: OSD 3.1 marked down due to heartbeat failure
2025-02-24 10:50:34 ceph-node-1 mds[9117]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:09 ceph-node-1 client[1397]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:01 ceph-node-4 mds[3789]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:27 ceph-node-4 mgr[8229]: ERROR: Client connection timeout detected
2025-02-24 10:50:54 ceph-node-1 mon[4104]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:01 ceph-node-4 osd[3247]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:50:43 ceph-node-2 radosgw[1970]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:21 ceph-node-5 mds[5663]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:50:37 ceph-node-5 mgr[3973]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:16 ceph-node-1 client[5032]: INFO: PG 1.2 replicated successfully
2025-02-24 10:51:27 ceph-node-3 osd[6870]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:50:47 ceph-node-2 radosgw[7615]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:38 ceph-node-2 radosgw[9240]: INFO: Data replication completed for object pool
2025-02-24 10:51:00 ceph-node-5 radosgw[7886]: CRITICAL: Network partition detected between OSD nodes
2025-02-24 10:50:38 ceph-node-3 mgr[3677]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:51:13 ceph-node-4 radosgw[9463]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:06 ceph-node-4 mon[2684]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:18 ceph-node-1 osd[1155]: INFO: OSD disk space utilization: 70%
2025-02-24 10:51:24 ceph-node-4 mgr[3198]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:50:48 ceph-node-4 osd[6339]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:32 ceph-node-5 mon[8039]: ERROR: Slow request detected on OSD 4.3
2025-02-24 10:50:44 ceph-node-1 osd[3288]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:33 ceph-node-2 osd[6853]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:50:37 ceph-node-4 mon[1139]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:51:19 ceph-node-3 mgr[4088]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:51 ceph-node-1 mds[3516]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:17 ceph-node-3 client[1436]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:29 ceph-node-5 client[2099]: NOTICE: OSD rebalancing completed
2025-02-24 10:51:21 ceph-node-4 client[2586]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:51:08 ceph-node-2 mgr[5665]: NOTICE: Monitor map has been updated
2025-02-24 10:50:54 ceph-node-5 mgr[3731]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:51:12 ceph-node-1 mds[9738]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:27 ceph-node-4 mds[6512]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:31 ceph-node-3 mgr[1121]: INFO: MGR module loaded successfully
2025-02-24 10:50:40 ceph-node-5 mgr[5722]: WARNING: Monitor mon1 failed to reach quorum
2025-02-24 10:50:31 ceph-node-5 client[8715]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:50:58 ceph-node-3 client[1626]: INFO: Monitor map has been updated
2025-02-24 10:50:53 ceph-node-4 mgr[5429]: INFO: Monitor map has been updated
2025-02-24 10:50:50 ceph-node-3 mgr[5476]: INFO: PG 1.2 replicated successfully
2025-02-24 10:50:36 ceph-node-2 mgr[7656]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:59 ceph-node-3 mon[6934]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:51:06 ceph-node-5 mds[7403]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:44 ceph-node-1 mds[4171]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:50:41 ceph-node-5 mon[6707]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:24 ceph-node-2 radosgw[7481]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:27 ceph-node-2 client[1299]: NOTICE: MGR module loaded successfully
2025-02-24 10:50:47 ceph-node-1 mgr[6014]: CRITICAL: OSD 3.1 marked down due to heartbeat failure
2025-02-24 10:51:10 ceph-node-3 mgr[8455]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:50:54 ceph-node-4 radosgw[3678]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:50:50 ceph-node-1 osd[9562]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:50:42 ceph-node-2 osd[5415]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:50:41 ceph-node-3 osd[5237]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:11 ceph-node-4 mgr[7633]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:50:47 ceph-node-1 client[6624]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:50 ceph-node-4 mgr[3846]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:50:31 ceph-node-2 mds[8369]: DEBUG: Data replication completed for object pool
2025-02-24 10:50:51 ceph-node-5 mds[3147]: NOTICE: MGR module loaded successfully
2025-02-24 10:50:50 ceph-node-5 mds[3391]: INFO: MGR module loaded successfully
2025-02-24 10:51:25 ceph-node-4 osd[1606]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:08 ceph-node-1 client[3565]: ERROR: Authentication failure in RADOS Gateway
2025-02-24 10:51:09 ceph-node-3 mgr[7896]: WARNING: High I/O latency detected in radosgw service
2025-02-24 10:51:24 ceph-node-5 mgr[9816]: NOTICE: OSD rebalancing completed
2025-02-24 10:51:23 ceph-node-1 radosgw[2937]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:05 ceph-node-5 mds[8707]: NOTICE: Monitor map has been updated
2025-02-24 10:50:57 ceph-node-5 client[3449]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:50:39 ceph-node-2 osd[8808]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:50:35 ceph-node-4 osd[7251]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:50:57 ceph-node-1 osd[2257]: INFO: OSD disk space utilization: 70%
2025-02-24 10:51:14 ceph-node-3 mgr[7574]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:51:17 ceph-node-5 mds[9728]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:24 ceph-node-4 osd[5311]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:50 ceph-node-1 mon[6237]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:50:56 ceph-node-3 mgr[8543]: DEBUG: Monitor map has been updated
2025-02-24 10:50:45 ceph-node-1 osd[1489]: DEBUG: Monitor map has been updated
2025-02-24 10:51:22 ceph-node-3 mgr[4763]: NOTICE: Monitor map has been updated
2025-02-24 10:50:56 ceph-node-2 mds[2548]: INFO: OSD disk space utilization: 70%
2025-02-24 10:51:09 ceph-node-3 radosgw[7470]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:50:36 ceph-node-2 mon[1738]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:51:17 ceph-node-4 radosgw[1782]: NOTICE: Monitor map has been updated
2025-02-24 10:50:47 ceph-node-1 mds[7221]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:27 ceph-node-5 client[1207]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:51:28 ceph-node-5 mon[6202]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:51:06 ceph-node-2 radosgw[9067]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:26 ceph-node-4 radosgw[5709]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:50:35 ceph-node-3 mon[2997]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:50:43 ceph-node-4 mds[1472]: DEBUG: Data replication completed for object pool
2025-02-24 10:50:45 ceph-node-5 mon[9820]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:50:39 ceph-node-3 osd[3709]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:50:34 ceph-node-5 mgr[4962]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:50:33 ceph-node-5 osd[9797]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:15 ceph-node-2 client[7046]: INFO: PG 1.2 replicated successfully
2025-02-24 10:51:24 ceph-node-5 mgr[2904]: INFO: MGR module loaded successfully
2025-02-24 10:51:27 ceph-node-5 radosgw[2055]: NOTICE: Monitor map has been updated
2025-02-24 10:50:31 ceph-node-1 radosgw[7090]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:50:57 ceph-node-5 mds[9153]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:51:17 ceph-node-1 client[1798]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:13 ceph-node-2 radosgw[7353]: DEBUG: Monitor map has been updated
2025-02-24 10:50:38 ceph-node-4 mgr[6932]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:51:22 ceph-node-1 radosgw[4359]: INFO: MGR module loaded successfully
2025-02-24 10:51:19 ceph-node-4 mon[8761]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:23 ceph-node-1 radosgw[1068]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:50:57 ceph-node-5 mds[2147]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:34 ceph-node-2 mon[4140]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:50:32 ceph-node-4 mgr[7680]: INFO: Monitor map has been updated
2025-02-24 10:51:09 ceph-node-2 mon[8857]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:50:34 ceph-node-1 mgr[2314]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:17 ceph-node-4 radosgw[3704]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:31 ceph-node-3 mds[6300]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:50:54 ceph-node-4 mon[4309]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:50:45 ceph-node-3 radosgw[9068]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:50:49 ceph-node-1 radosgw[4878]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:11 ceph-node-1 client[7800]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:50:46 ceph-node-1 mds[8855]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:01 ceph-node-5 client[8117]: NOTICE: OSD rebalancing completed
2025-02-24 10:50:47 ceph-node-5 mgr[6647]: NOTICE: MGR module loaded successfully
2025-02-24 10:50:47 ceph-node-3 radosgw[9153]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:04 ceph-node-4 mgr[2526]: NOTICE: MGR module loaded successfully
2025-02-24 10:51:02 ceph-node-1 osd[6671]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:35 ceph-node-2 mds[7006]: INFO: MGR module loaded successfully
2025-02-24 10:50:58 ceph-node-5 radosgw[9671]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:01 ceph-node-3 osd[3464]: NOTICE: OSD rebalancing completed
2025-02-24 10:50:50 ceph-node-1 radosgw[2349]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:27 ceph-node-1 client[9026]: DEBUG: Data replication completed for object pool
2025-02-24 10:50:53 ceph-node-3 osd[5999]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:50:34 ceph-node-3 mgr[6865]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:50:50 ceph-node-5 mon[2548]: NOTICE: OSD rebalancing completed
2025-02-24 10:51:09 ceph-node-5 mgr[3208]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:01 ceph-node-1 radosgw[3439]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:30 ceph-node-4 mds[6542]: ERROR: Client connection timeout detected
2025-02-24 10:51:26 ceph-node-3 radosgw[3218]: DEBUG: OSD rebalancing completed
2025-02-24 10:51:07 ceph-node-3 mon[4907]: ERROR: Monitor mon1 failed to reach quorum
2025-02-24 10:51:20 ceph-node-1 mgr[1619]: INFO: Data replication completed for object pool
2025-02-24 10:50:43 ceph-node-1 mds[4813]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:03 ceph-node-1 radosgw[2013]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:54 ceph-node-1 radosgw[5990]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:51:03 ceph-node-2 mon[5425]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:25 ceph-node-3 client[3109]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:42 ceph-node-3 client[1956]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:03 ceph-node-1 client[2085]: NOTICE: Data replication completed for object pool
2025-02-24 10:51:03 ceph-node-5 osd[5591]: NOTICE: OSD rebalancing completed
2025-02-24 10:50:42 ceph-node-1 osd[8879]: CRITICAL: CRITICAL: PG 2.1 stuck in inconsistent state
2025-02-24 10:51:17 ceph-node-2 mon[2167]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:51:27 ceph-node-5 mds[9247]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:51:20 ceph-node-4 osd[8444]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:56 ceph-node-5 mgr[2364]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:50:53 ceph-node-1 mgr[1422]: WARNING: High I/O latency detected in radosgw service
2025-02-24 10:50:32 ceph-node-4 client[9383]: INFO: PG 1.2 replicated successfully
2025-02-24 10:51:24 ceph-node-1 radosgw[3237]: WARNING: Monitor down, unable to connect to quorum
2025-02-24 10:51:13 ceph-node-3 radosgw[5221]: NOTICE: OSD rebalancing completed
2025-02-24 10:51:27 ceph-node-2 osd[6416]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:29 ceph-node-2 client[7368]: ERROR: OSD 3.1 marked down due to heartbeat failure
2025-02-24 10:51:29 ceph-node-3 client[1121]: INFO: OSD disk space utilization: 70%
2025-02-24 10:50:34 ceph-node-1 radosgw[8952]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:15 ceph-node-5 osd[9440]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:41 ceph-node-2 radosgw[2739]: INFO: OSD rebalancing completed
2025-02-24 10:51:17 ceph-node-5 mon[1440]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:49 ceph-node-2 osd[1432]: INFO: Monitor map has been updated
2025-02-24 10:51:17 ceph-node-4 osd[3502]: NOTICE: OSD rebalancing completed
2025-02-24 10:50:59 ceph-node-4 mds[7577]: INFO: Data replication completed for object pool
2025-02-24 10:51:26 ceph-node-5 mds[3364]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:31 ceph-node-2 client[4046]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:21 ceph-node-3 mon[8487]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:09 ceph-node-2 osd[2027]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:50:40 ceph-node-2 mon[7255]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:50:48 ceph-node-5 osd[7199]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:50:53 ceph-node-3 client[5011]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:21 ceph-node-3 mds[4516]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:50:42 ceph-node-3 mds[2215]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:12 ceph-node-2 osd[8519]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:50:55 ceph-node-3 client[5179]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:50:55 ceph-node-1 radosgw[3015]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:06 ceph-node-2 mgr[8549]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:51:13 ceph-node-4 mds[4418]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:50:46 ceph-node-3 client[6123]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:51:20 ceph-node-4 mds[2286]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:58 ceph-node-4 osd[2426]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:08 ceph-node-4 mon[5563]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:45 ceph-node-3 osd[8911]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:50 ceph-node-1 osd[5253]: DEBUG: MGR module loaded successfully
2025-02-24 10:50:41 ceph-node-4 mgr[4289]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:28 ceph-node-1 radosgw[6240]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:50:49 ceph-node-2 mds[3128]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:20 ceph-node-1 client[6330]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:09 ceph-node-1 osd[6690]: DEBUG: Monitor map has been updated
2025-02-24 10:51:18 ceph-node-5 client[7790]: NOTICE: OSD rebalancing completed
2025-02-24 10:51:09 ceph-node-2 mds[1086]: NOTICE: Monitor map has been updated
2025-02-24 10:51:18 ceph-node-3 mds[9051]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:51:09 ceph-node-3 mon[5964]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:03 ceph-node-2 mds[8711]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:51:30 ceph-node-2 mon[5352]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:20 ceph-node-3 mds[7146]: NOTICE: Monitor map has been updated
2025-02-24 10:51:21 ceph-node-2 osd[7189]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:50:41 ceph-node-2 client[4837]: DEBUG: Monitor map has been updated
2025-02-24 10:50:56 ceph-node-4 mgr[9276]: INFO: Monitor map has been updated
2025-02-24 10:51:02 ceph-node-4 radosgw[6375]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:50:48 ceph-node-5 mgr[3803]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:17 ceph-node-1 osd[2631]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:50:35 ceph-node-1 mon[8697]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:02 ceph-node-5 client[6161]: INFO: Data replication completed for object pool
2025-02-24 10:51:30 ceph-node-5 mds[7655]: NOTICE: MGR module loaded successfully
2025-02-24 10:50:36 ceph-node-3 mon[5639]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:50:45 ceph-node-5 radosgw[1836]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:51:12 ceph-node-5 mgr[6944]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:50:48 ceph-node-1 osd[5472]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:50:49 ceph-node-5 osd[8732]: WARNING: MDS failure detected: metadata inconsistency
2025-02-24 10:51:25 ceph-node-4 client[2029]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:14 ceph-node-3 radosgw[8590]: CRITICAL: Uncorrectable ECC error detected in MDS cache
2025-02-24 10:51:23 ceph-node-4 client[2582]: INFO: OSD disk space utilization: 70%
2025-02-24 10:50:37 ceph-node-3 mds[7479]: ERROR: Monitor down, unable to connect to quorum
2025-02-24 10:50:39 ceph-node-1 osd[9898]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:13 ceph-node-2 osd[4949]: DEBUG: OSD rebalancing completed
2025-02-24 10:50:53 ceph-node-2 mon[6768]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:19 ceph-node-3 radosgw[5980]: ERROR: OSD disk space utilization exceeded 90%
2025-02-24 10:50:54 ceph-node-1 mon[2314]: INFO: OSD rebalancing completed
2025-02-24 10:51:29 ceph-node-2 client[3750]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:00 ceph-node-1 osd[1243]: INFO: OSD disk space utilization: 70%
2025-02-24 10:50:59 ceph-node-1 mds[4382]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:44 ceph-node-2 mds[6577]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:12 ceph-node-2 osd[8397]: WARNING: Monitor mon1 failed to reach quorum
2025-02-24 10:51:19 ceph-node-2 client[1001]: NOTICE: Monitor map has been updated
2025-02-24 10:50:50 ceph-node-1 radosgw[3382]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:50:43 ceph-node-2 mon[4555]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:30 ceph-node-5 mds[5145]: DEBUG: OSD rebalancing completed
2025-02-24 10:51:29 ceph-node-4 mon[1893]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:27 ceph-node-4 client[2795]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:59 ceph-node-5 osd[9475]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:51:27 ceph-node-3 mon[6271]: NOTICE: Data replication completed for object pool
2025-02-24 10:51:12 ceph-node-5 mon[3928]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:51:00 ceph-node-4 radosgw[4462]: DEBUG: Monitor map has been updated
2025-02-24 10:51:04 ceph-node-1 osd[6372]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:51 ceph-node-2 mon[2012]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:14 ceph-node-1 osd[5748]: INFO: OSD disk space utilization: 70%
2025-02-24 10:51:05 ceph-node-3 mds[4712]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:02 ceph-node-3 client[4148]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:20 ceph-node-2 mgr[2891]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:20 ceph-node-4 mon[7633]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:22 ceph-node-2 osd[9845]: ERROR: Network partition detected between OSD nodes
2025-02-24 10:50:39 ceph-node-5 client[4697]: NOTICE: Data replication completed for object pool
2025-02-24 10:50:50 ceph-node-1 mgr[5789]: NOTICE: MGR module loaded successfully
2025-02-24 10:51:06 ceph-node-2 client[7717]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:51:11 ceph-node-4 client[4851]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:50:31 ceph-node-3 radosgw[5487]: CRITICAL: Data corruption detected in object pool
2025-02-24 10:50:39 ceph-node-2 mon[3954]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:50:54 ceph-node-3 osd[7525]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:50:54 ceph-node-1 client[6734]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:18 ceph-node-1 mon[8786]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:55 ceph-node-3 osd[8317]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:04 ceph-node-4 osd[1540]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:14 ceph-node-1 client[4345]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:51:22 ceph-node-4 radosgw[7206]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:50:40 ceph-node-2 mds[4129]: INFO: PG 1.2 replicated successfully
2025-02-24 10:51:24 ceph-node-2 radosgw[8229]: NOTICE: MGR module loaded successfully
2025-02-24 10:51:10 ceph-node-2 client[6232]: INFO: Monitor map has been updated
2025-02-24 10:51:03 ceph-node-3 client[3999]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:50:36 ceph-node-3 mgr[1041]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:51 ceph-node-2 osd[4956]: NOTICE: OSD rebalancing completed
2025-02-24 10:51:02 ceph-node-1 mgr[8984]: INFO: OSD disk space utilization: 70%
2025-02-24 10:50:42 ceph-node-3 client[9012]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:50:55 ceph-node-1 radosgw[5846]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:50:59 ceph-node-4 osd[2435]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:51:23 ceph-node-1 mgr[9678]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:51:11 ceph-node-3 mgr[6598]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:39 ceph-node-2 mds[3452]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:06 ceph-node-1 radosgw[9371]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:31 ceph-node-4 mon[2551]: DEBUG: Data replication completed for object pool
2025-02-24 10:50:33 ceph-node-5 osd[7041]: INFO: Data replication completed for object pool
2025-02-24 10:51:11 ceph-node-4 mgr[7962]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:07 ceph-node-4 mgr[8447]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:10 ceph-node-1 mgr[7346]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:32 ceph-node-4 client[4282]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:50:57 ceph-node-4 radosgw[1904]: INFO: Monitor map has been updated
2025-02-24 10:50:59 ceph-node-3 mds[7063]: INFO: Monitor map has been updated
2025-02-24 10:51:15 ceph-node-4 mds[2664]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:50:48 ceph-node-2 mgr[6675]: NOTICE: Monitor map has been updated
2025-02-24 10:50:36 ceph-node-3 osd[6626]: INFO: MGR module loaded successfully
2025-02-24 10:50:31 ceph-node-4 mgr[3608]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:50:47 ceph-node-3 mon[3240]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:33 ceph-node-2 client[7793]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:50:54 ceph-node-4 mds[3422]: INFO: OSD disk space utilization: 70%
2025-02-24 10:50:32 ceph-node-3 radosgw[5013]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:50:38 ceph-node-2 radosgw[1525]: NOTICE: Data replication completed for object pool
2025-02-24 10:51:02 ceph-node-4 mgr[9539]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:36 ceph-node-1 client[1543]: NOTICE: Data replication completed for object pool
2025-02-24 10:50:37 ceph-node-4 radosgw[7167]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:20 ceph-node-5 mgr[5505]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:43 ceph-node-5 radosgw[8079]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:10 ceph-node-1 radosgw[6666]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:51:18 ceph-node-1 osd[9431]: NOTICE: Data replication completed for object pool
2025-02-24 10:50:38 ceph-node-4 mon[5142]: INFO: Data replication completed for object pool
2025-02-24 10:50:37 ceph-node-2 client[7655]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:51:28 ceph-node-5 radosgw[7889]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:50:31 ceph-node-2 mon[3141]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:33 ceph-node-3 osd[9779]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:50:36 ceph-node-2 osd[6939]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:51:16 ceph-node-1 mgr[3441]: WARNING: Data corruption detected in object pool
2025-02-24 10:50:39 ceph-node-3 mon[6503]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:11 ceph-node-5 radosgw[8092]: DEBUG: MGR module loaded successfully
2025-02-24 10:50:40 ceph-node-1 mon[6193]: CRITICAL: Journal write failure detected on OSD node
2025-02-24 10:50:55 ceph-node-5 osd[5225]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:50 ceph-node-2 mon[4497]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:12 ceph-node-3 mgr[7708]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:51:11 ceph-node-2 radosgw[8006]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:25 ceph-node-2 radosgw[3655]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:52 ceph-node-2 client[7341]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:29 ceph-node-4 mgr[6650]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:50:44 ceph-node-4 osd[9928]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:51:04 ceph-node-4 mgr[4708]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:51:17 ceph-node-4 radosgw[4486]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:14 ceph-node-5 client[3413]: INFO: OSD rebalancing completed
2025-02-24 10:51:19 ceph-node-5 mgr[9465]: WARNING: Client connection timeout detected
2025-02-24 10:51:20 ceph-node-2 client[1440]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:50:42 ceph-node-3 osd[9008]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:11 ceph-node-2 osd[9556]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:03 ceph-node-3 osd[6315]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:51:08 ceph-node-4 radosgw[3020]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:24 ceph-node-2 osd[4186]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:41 ceph-node-3 mon[3593]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:50:50 ceph-node-2 radosgw[6816]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:35 ceph-node-4 mds[8008]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:50:37 ceph-node-4 mds[9724]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:51:28 ceph-node-5 client[7530]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:50:54 ceph-node-4 mgr[4016]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:39 ceph-node-1 mon[2766]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:50:32 ceph-node-1 mds[4216]: ERROR: Client connection timeout detected
2025-02-24 10:50:49 ceph-node-1 mon[2627]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:00 ceph-node-2 client[8777]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:50:58 ceph-node-5 radosgw[9027]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:08 ceph-node-3 radosgw[1573]: INFO: Monitor map has been updated
2025-02-24 10:51:28 ceph-node-3 client[4550]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:50:55 ceph-node-3 client[2938]: NOTICE: Monitor map has been updated
2025-02-24 10:51:14 ceph-node-5 client[4521]: WARNING: High I/O latency detected in radosgw service
2025-02-24 10:51:30 ceph-node-1 radosgw[9662]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:01 ceph-node-3 client[9811]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:51:19 ceph-node-4 mgr[8396]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:20 ceph-node-4 radosgw[3472]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:53 ceph-node-1 mds[3452]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:50:31 ceph-node-5 mds[7085]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:23 ceph-node-2 osd[9673]: INFO: OSD disk space utilization: 70%
2025-02-24 10:51:19 ceph-node-1 mgr[7554]: NOTICE: MGR module loaded successfully
2025-02-24 10:51:31 ceph-node-3 mon[4782]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:51:20 ceph-node-5 client[7351]: DEBUG: OSD rebalancing completed
2025-02-24 10:50:45 ceph-node-1 mgr[8841]: ERROR: Network partition detected between OSD nodes
2025-02-24 10:50:38 ceph-node-1 mds[3000]: DEBUG: Monitor map has been updated
2025-02-24 10:51:23 ceph-node-5 client[7023]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:57 ceph-node-2 radosgw[1402]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:51:25 ceph-node-4 mgr[5014]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:51:31 ceph-node-3 osd[7920]: NOTICE: Monitor map has been updated
2025-02-24 10:50:48 ceph-node-3 radosgw[6588]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:51:13 ceph-node-4 mds[8837]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:50:31 ceph-node-5 mon[1816]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:50:58 ceph-node-1 client[1255]: INFO: PG 1.2 replicated successfully
2025-02-24 10:50:31 ceph-node-2 radosgw[8010]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:11 ceph-node-2 radosgw[3906]: ERROR: Monitor mon1 failed to reach quorum
2025-02-24 10:50:39 ceph-node-4 mgr[9577]: NOTICE: Data replication completed for object pool
2025-02-24 10:50:38 ceph-node-5 mon[6829]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:50:47 ceph-node-2 client[9466]: INFO: MGR module loaded successfully
2025-02-24 10:51:19 ceph-node-4 mon[2095]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:03 ceph-node-3 radosgw[7691]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:50:49 ceph-node-1 radosgw[5500]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:50:47 ceph-node-1 osd[5250]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:30 ceph-node-2 radosgw[5358]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:08 ceph-node-3 mon[1665]: INFO: PG 1.2 replicated successfully
2025-02-24 10:51:18 ceph-node-2 client[6008]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:28 ceph-node-3 client[3871]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:45 ceph-node-4 radosgw[2467]: WARNING: Journal write failure detected on OSD node
2025-02-24 10:50:40 ceph-node-5 osd[8171]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:50:44 ceph-node-3 mon[7174]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:02 ceph-node-4 mds[9952]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:51:27 ceph-node-1 mds[6216]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:12 ceph-node-5 radosgw[1843]: INFO: OSD disk space utilization: 70%
2025-02-24 10:50:47 ceph-node-4 radosgw[5748]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:09 ceph-node-2 mds[7034]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:50:38 ceph-node-5 mon[8231]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:41 ceph-node-3 osd[4324]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:31 ceph-node-3 radosgw[2148]: DEBUG: MGR module loaded successfully
2025-02-24 10:50:33 ceph-node-1 osd[4920]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:24 ceph-node-4 mon[3858]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:55 ceph-node-4 osd[7733]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:58 ceph-node-4 mon[6443]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:15 ceph-node-4 mds[4047]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:50:44 ceph-node-2 mgr[2311]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:51 ceph-node-5 mon[3650]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:39 ceph-node-1 osd[7143]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:50:37 ceph-node-2 radosgw[7799]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:50:34 ceph-node-1 mds[3288]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:03 ceph-node-2 osd[5401]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:50:34 ceph-node-4 client[4060]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:31 ceph-node-5 client[7038]: INFO: MGR module loaded successfully
2025-02-24 10:51:00 ceph-node-1 radosgw[9833]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:20 ceph-node-1 mgr[9588]: ERROR: Slow request detected on OSD 4.3
2025-02-24 10:50:57 ceph-node-1 mds[5583]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:50:48 ceph-node-3 osd[4746]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:51:09 ceph-node-1 client[2682]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:55 ceph-node-1 mgr[6100]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:15 ceph-node-1 radosgw[8209]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:50:58 ceph-node-4 client[5663]: WARNING: Client connection timeout detected
2025-02-24 10:51:23 ceph-node-5 osd[2602]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:51:19 ceph-node-3 radosgw[9500]: ERROR: OSD 3.1 marked down due to heartbeat failure
2025-02-24 10:50:32 ceph-node-1 mds[3978]: CRITICAL: Network partition detected between OSD nodes
2025-02-24 10:51:31 ceph-node-3 osd[1863]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:50:57 ceph-node-2 mon[9465]: NOTICE: Data replication completed for object pool
2025-02-24 10:50:33 ceph-node-4 mon[2545]: DEBUG: Data replication completed for object pool
2025-02-24 10:50:50 ceph-node-1 osd[2823]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:50:56 ceph-node-3 mgr[6924]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:50:54 ceph-node-3 mds[8387]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:05 ceph-node-5 mds[9112]: DEBUG: Monitor map has been updated
2025-02-24 10:50:45 ceph-node-4 client[9235]: NOTICE: OSD rebalancing completed
2025-02-24 10:51:12 ceph-node-1 mds[9595]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:50:46 ceph-node-1 mon[9843]: INFO: PG 1.2 replicated successfully
2025-02-24 10:50:37 ceph-node-1 radosgw[7321]: INFO: PG 1.2 replicated successfully
2025-02-24 10:50:52 ceph-node-1 mgr[2426]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:50:36 ceph-node-3 mds[8505]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:50:52 ceph-node-4 mon[1461]: WARNING: Monitor down, unable to connect to quorum
2025-02-24 10:51:25 ceph-node-2 mgr[7202]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:50:51 ceph-node-4 mds[6108]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:50:35 ceph-node-2 mds[5308]: NOTICE: Monitor map has been updated
2025-02-24 10:51:30 ceph-node-4 mgr[9875]: NOTICE: Data replication completed for object pool
2025-02-24 10:50:37 ceph-node-2 client[5535]: NOTICE: Data replication completed for object pool
2025-02-24 10:50:43 ceph-node-4 mon[9878]: NOTICE: Monitor map has been updated
2025-02-24 10:50:56 ceph-node-2 mds[1270]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:50:49 ceph-node-3 radosgw[5254]: NOTICE: Data replication completed for object pool
2025-02-24 10:51:15 ceph-node-5 mds[8781]: NOTICE: Data replication completed for object pool
2025-02-24 10:51:11 ceph-node-4 client[2568]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:07 ceph-node-5 radosgw[5606]: NOTICE: MGR module loaded successfully
2025-02-24 10:50:51 ceph-node-2 client[8841]: DEBUG: OSD rebalancing completed
2025-02-24 10:50:58 ceph-node-1 radosgw[4795]: NOTICE: OSD rebalancing completed
2025-02-24 10:50:46 ceph-node-1 mon[7132]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:02 ceph-node-5 client[4176]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:51:17 ceph-node-1 client[2730]: INFO: PG 1.2 replicated successfully
2025-02-24 10:51:04 ceph-node-1 radosgw[8356]: WARNING: Slow request detected on OSD 4.3
2025-02-24 10:50:31 ceph-node-5 radosgw[4414]: WARNING: MDS failure detected: metadata inconsistency
2025-02-24 10:50:59 ceph-node-1 mon[5027]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:26 ceph-node-3 osd[3172]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:54 ceph-node-5 osd[4805]: ERROR: Client connection timeout detected
2025-02-24 10:50:56 ceph-node-3 radosgw[1098]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:51:31 ceph-node-4 mds[8737]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:51:03 ceph-node-2 radosgw[9508]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:31 ceph-node-5 mds[2465]: INFO: Data replication completed for object pool
2025-02-24 10:50:45 ceph-node-4 osd[8182]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:50:31 ceph-node-3 radosgw[8687]: INFO: Monitor map has been updated
2025-02-24 10:50:55 ceph-node-2 osd[4495]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:21 ceph-node-1 radosgw[6726]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:26 ceph-node-5 mon[5481]: NOTICE: Data replication completed for object pool
2025-02-24 10:51:16 ceph-node-1 client[6714]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:00 ceph-node-5 radosgw[9667]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:51:01 ceph-node-1 osd[8436]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:51:00 ceph-node-4 mon[7798]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:50:46 ceph-node-1 mon[5291]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:51:00 ceph-node-2 osd[8663]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:50:46 ceph-node-1 osd[7307]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:52 ceph-node-2 mon[6422]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:50:31 ceph-node-5 mgr[5555]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:50:47 ceph-node-4 mon[5346]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:50:44 ceph-node-3 mon[9256]: INFO: Monitor map has been updated
2025-02-24 10:51:20 ceph-node-5 mon[3148]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:50:53 ceph-node-4 mds[7624]: INFO: OSD rebalancing completed
2025-02-24 10:50:35 ceph-node-1 mds[1091]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:50 ceph-node-3 mon[1765]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:28 ceph-node-1 mon[1587]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:50:49 ceph-node-2 mgr[6147]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:51:20 ceph-node-2 mgr[1158]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:50:39 ceph-node-3 mgr[4467]: CRITICAL: Data corruption detected in object pool
2025-02-24 10:51:12 ceph-node-4 mon[1580]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:20 ceph-node-4 mds[4636]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:50:56 ceph-node-2 osd[5668]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:51:31 ceph-node-3 mon[6376]: CRITICAL: Monitor mon1 failed to reach quorum
2025-02-24 10:50:32 ceph-node-1 mgr[7283]: INFO: Monitor map has been updated
2025-02-24 10:50:40 ceph-node-3 mon[5847]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:50:48 ceph-node-1 client[4274]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:51:15 ceph-node-1 mon[7723]: INFO: Data replication completed for object pool
2025-02-24 10:51:02 ceph-node-2 client[1913]: INFO: OSD rebalancing completed
2025-02-24 10:51:29 ceph-node-5 osd[5895]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:50:36 ceph-node-1 mon[5204]: NOTICE: Data replication completed for object pool
2025-02-24 10:50:44 ceph-node-1 radosgw[6367]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:50:46 ceph-node-1 osd[2586]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:44 ceph-node-5 radosgw[9752]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:29 ceph-node-1 client[1248]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:14 ceph-node-1 mon[6668]: ERROR: Disk failure reported on ceph-node-4
2025-02-24 10:50:44 ceph-node-1 mgr[5665]: NOTICE: Data replication completed for object pool
2025-02-24 10:51:07 ceph-node-1 mon[9495]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:34 ceph-node-4 mon[5725]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:18 ceph-node-4 radosgw[4799]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:27 ceph-node-3 radosgw[5970]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:51:07 ceph-node-3 client[1243]: CRITICAL: Journal write failure detected on OSD node
2025-02-24 10:51:16 ceph-node-5 mds[7925]: DEBUG: MGR module loaded successfully
2025-02-24 10:50:45 ceph-node-4 radosgw[3592]: WARNING: OSD disk space utilization exceeded 90%
2025-02-24 10:50:55 ceph-node-1 mgr[1867]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:50:48 ceph-node-4 osd[3851]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:50:51 ceph-node-3 osd[9698]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:29 ceph-node-5 radosgw[3806]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:50:47 ceph-node-2 mgr[1149]: NOTICE: Data replication completed for object pool
2025-02-24 10:50:37 ceph-node-1 mds[9491]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:27 ceph-node-3 mgr[5063]: INFO: OSD disk space utilization: 70%
2025-02-24 10:50:36 ceph-node-5 mon[1745]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:50:36 ceph-node-3 osd[5100]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:07 ceph-node-3 osd[3586]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:51:09 ceph-node-3 mgr[8818]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:25 ceph-node-1 mds[4613]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:50:49 ceph-node-4 mds[8759]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:51:14 ceph-node-1 mon[6149]: NOTICE: MGR module loaded successfully
2025-02-24 10:50:45 ceph-node-3 mds[8068]: INFO: MGR module loaded successfully
2025-02-24 10:50:53 ceph-node-5 mds[2794]: WARNING: MDS failure detected: metadata inconsistency
2025-02-24 10:50:45 ceph-node-2 client[9056]: INFO: MGR module loaded successfully
2025-02-24 10:50:39 ceph-node-1 mon[8886]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:50:43 ceph-node-2 osd[8190]: INFO: OSD rebalancing completed
2025-02-24 10:51:18 ceph-node-3 mon[2109]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:59 ceph-node-4 mds[5875]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:19 ceph-node-4 mon[6504]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:51:03 ceph-node-4 mds[8016]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:30 ceph-node-3 radosgw[4734]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:51:23 ceph-node-3 mon[2834]: INFO: MGR module loaded successfully
2025-02-24 10:51:07 ceph-node-2 mgr[4271]: INFO: MGR module loaded successfully
2025-02-24 10:51:31 ceph-node-1 client[4990]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:51:08 ceph-node-4 mgr[8047]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:29 ceph-node-4 osd[2890]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:30 ceph-node-5 client[2064]: INFO: OSD rebalancing completed
2025-02-24 10:50:38 ceph-node-2 mds[4133]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:51:11 ceph-node-1 mon[1944]: DEBUG: Data replication completed for object pool
2025-02-24 10:50:49 ceph-node-1 mgr[3416]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:51:28 ceph-node-5 radosgw[9017]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:18 ceph-node-3 client[7347]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:50:56 ceph-node-5 client[6858]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:44 ceph-node-3 osd[3940]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:50:58 ceph-node-2 mgr[6993]: INFO: Monitor map has been updated
2025-02-24 10:51:00 ceph-node-3 mon[9886]: NOTICE: MGR module loaded successfully
2025-02-24 10:50:57 ceph-node-3 mds[9244]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:50:52 ceph-node-4 client[2522]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:29 ceph-node-4 mds[8605]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:50:59 ceph-node-4 client[4456]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:10 ceph-node-1 mds[9131]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:35 ceph-node-2 mds[1603]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:09 ceph-node-4 mgr[7978]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:47 ceph-node-1 mon[7754]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:41 ceph-node-2 mgr[8718]: DEBUG: MGR module loaded successfully
2025-02-24 10:50:37 ceph-node-4 mds[9600]: ERROR: CRITICAL: PG 2.1 stuck in inconsistent state
2025-02-24 10:50:50 ceph-node-1 mds[7330]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:49 ceph-node-3 mds[8482]: ERROR: Uncorrectable ECC error detected in MDS cache
2025-02-24 10:50:53 ceph-node-2 mon[1165]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:50:35 ceph-node-2 radosgw[7176]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:32 ceph-node-1 client[2499]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:51:20 ceph-node-4 osd[6467]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:50:56 ceph-node-4 client[2595]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:50:54 ceph-node-4 mon[1075]: NOTICE: Monitor map has been updated
2025-02-24 10:50:55 ceph-node-2 radosgw[8353]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:50:36 ceph-node-3 mds[5851]: INFO: OSD rebalancing completed
2025-02-24 10:51:28 ceph-node-1 mds[6391]: INFO: PG 1.2 replicated successfully
2025-02-24 10:50:46 ceph-node-2 mds[8570]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:02 ceph-node-4 mds[2938]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:13 ceph-node-1 mgr[1786]: INFO: PG 1.2 replicated successfully
2025-02-24 10:50:35 ceph-node-5 radosgw[5754]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:50:37 ceph-node-1 mon[7537]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:36 ceph-node-2 client[7616]: INFO: Monitor map has been updated
2025-02-24 10:50:43 ceph-node-5 mgr[4466]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:51:06 ceph-node-4 mds[4971]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:50:56 ceph-node-2 mds[4147]: DEBUG: MGR module loaded successfully
2025-02-24 10:50:42 ceph-node-5 radosgw[6181]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:11 ceph-node-1 client[3685]: NOTICE: OSD rebalancing completed
2025-02-24 10:51:08 ceph-node-3 radosgw[2921]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:31 ceph-node-2 radosgw[7795]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:50:42 ceph-node-2 client[3485]: DEBUG: MGR module loaded successfully
2025-02-24 10:50:50 ceph-node-1 mgr[2888]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:50:57 ceph-node-4 osd[1957]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:51 ceph-node-1 mgr[1404]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:44 ceph-node-4 mds[2748]: NOTICE: MGR module loaded successfully
2025-02-24 10:50:41 ceph-node-5 mds[7502]: NOTICE: OSD rebalancing completed
2025-02-24 10:51:24 ceph-node-4 mgr[1304]: NOTICE: Monitor map has been updated
2025-02-24 10:51:10 ceph-node-2 client[2912]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:50:58 ceph-node-2 client[9375]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:51:07 ceph-node-2 mon[6238]: NOTICE: Monitor map has been updated
2025-02-24 10:51:06 ceph-node-5 mon[5544]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:50:56 ceph-node-3 mgr[2219]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:16 ceph-node-2 radosgw[5526]: NOTICE: Monitor map has been updated
2025-02-24 10:51:31 ceph-node-3 mon[6350]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:17 ceph-node-4 client[2099]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:50:36 ceph-node-3 radosgw[3536]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:01 ceph-node-3 osd[9575]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:20 ceph-node-4 mon[8830]: NOTICE: Data replication completed for object pool
2025-02-24 10:50:59 ceph-node-3 radosgw[4266]: WARNING: MDS failure detected: metadata inconsistency
2025-02-24 10:51:16 ceph-node-5 client[8595]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:50:39 ceph-node-5 radosgw[3126]: WARNING: Journal write failure detected on OSD node
2025-02-24 10:50:47 ceph-node-1 mds[7752]: DEBUG: Monitor map has been updated
2025-02-24 10:50:43 ceph-node-5 osd[5203]: CRITICAL: Monitor mon1 failed to reach quorum
2025-02-24 10:50:53 ceph-node-3 radosgw[3720]: NOTICE: MGR module loaded successfully
2025-02-24 10:50:46 ceph-node-2 client[2674]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:51:04 ceph-node-3 mon[6386]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:04 ceph-node-4 mgr[1796]: INFO: OSD rebalancing completed
2025-02-24 10:51:03 ceph-node-1 mgr[4246]: NOTICE: Data replication completed for object pool
2025-02-24 10:51:01 ceph-node-2 client[7441]: INFO: OSD rebalancing completed
2025-02-24 10:50:33 ceph-node-1 mds[3917]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:18 ceph-node-5 mgr[6926]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:23 ceph-node-1 mgr[9107]: INFO: MGR module loaded successfully
2025-02-24 10:51:06 ceph-node-5 mon[5196]: WARNING: CRITICAL: PG 2.1 stuck in inconsistent state
2025-02-24 10:50:56 ceph-node-5 mgr[1423]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:50:41 ceph-node-5 mds[5506]: INFO: OSD disk space utilization: 70%
2025-02-24 10:51:28 ceph-node-1 mon[7925]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:50:34 ceph-node-1 radosgw[6412]: INFO: Data replication completed for object pool
2025-02-24 10:50:31 ceph-node-1 mon[6862]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:50:37 ceph-node-4 mgr[8230]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:33 ceph-node-1 mds[2834]: WARNING: Client connection timeout detected
2025-02-24 10:51:01 ceph-node-1 mgr[4383]: DEBUG: Monitor map has been updated
2025-02-24 10:50:38 ceph-node-4 radosgw[3988]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:41 ceph-node-2 mds[7064]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:50:45 ceph-node-5 osd[5924]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:50:47 ceph-node-2 mds[1521]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:50:57 ceph-node-3 mgr[1479]: WARNING: OSD disk space utilization exceeded 90%
2025-02-24 10:51:26 ceph-node-4 radosgw[7769]: INFO: Data replication completed for object pool
2025-02-24 10:51:21 ceph-node-5 osd[9094]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:27 ceph-node-4 radosgw[2826]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:25 ceph-node-1 mon[5934]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:41 ceph-node-1 client[4100]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:49 ceph-node-1 osd[4747]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:51:29 ceph-node-3 client[6501]: INFO: PG 1.2 replicated successfully
2025-02-24 10:51:02 ceph-node-4 radosgw[1938]: NOTICE: MGR module loaded successfully
2025-02-24 10:51:08 ceph-node-1 osd[5336]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:51:27 ceph-node-4 mgr[4425]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:51:03 ceph-node-2 mon[3709]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:51:22 ceph-node-3 mds[5652]: CRITICAL: OSD disk space utilization exceeded 90%
2025-02-24 10:50:52 ceph-node-1 mds[6728]: ERROR: Slow request detected on OSD 4.3
2025-02-24 10:51:14 ceph-node-1 mon[7387]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:51:04 ceph-node-4 mon[7303]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:15 ceph-node-2 mon[1739]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:51:01 ceph-node-5 mon[3761]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:51:10 ceph-node-5 mon[8897]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:51:07 ceph-node-4 mgr[7532]: NOTICE: Data replication completed for object pool
2025-02-24 10:50:36 ceph-node-4 radosgw[7269]: ERROR: OSD disk space utilization exceeded 90%
2025-02-24 10:51:05 ceph-node-5 mon[9158]: INFO: OSD rebalancing completed
2025-02-24 10:51:03 ceph-node-4 mds[6590]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:47 ceph-node-1 radosgw[7379]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:51:04 ceph-node-1 mgr[3299]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:51:13 ceph-node-5 osd[4281]: DEBUG: Data replication completed for object pool
2025-02-24 10:50:51 ceph-node-5 mon[3861]: INFO: Data replication completed for object pool
2025-02-24 10:50:37 ceph-node-3 mgr[3745]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:07 ceph-node-1 client[5493]: WARNING: Authentication failure in RADOS Gateway
2025-02-24 10:50:58 ceph-node-4 mds[2803]: INFO: MGR module loaded successfully
2025-02-24 10:50:40 ceph-node-1 mds[6663]: NOTICE: Monitor map has been updated
2025-02-24 10:50:38 ceph-node-2 mon[1794]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:48 ceph-node-5 mon[4514]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:50:32 ceph-node-4 mds[9742]: CRITICAL: OSD disk space utilization exceeded 90%
2025-02-24 10:50:48 ceph-node-1 radosgw[7101]: INFO: OSD rebalancing completed
2025-02-24 10:51:16 ceph-node-1 mgr[2636]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:09 ceph-node-2 mgr[7727]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:31 ceph-node-1 radosgw[4771]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:51:27 ceph-node-4 radosgw[5107]: ERROR: CRITICAL: Data loss detected on pool 1
2025-02-24 10:51:00 ceph-node-4 mon[3326]: WARNING: Journal write failure detected on OSD node
2025-02-24 10:51:23 ceph-node-3 mgr[6865]: NOTICE: MGR module loaded successfully
2025-02-24 10:51:31 ceph-node-2 mon[6786]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:13 ceph-node-5 mds[4111]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:51:07 ceph-node-1 mgr[5559]: WARNING: CRITICAL: PG 2.1 stuck in inconsistent state
2025-02-24 10:51:13 ceph-node-1 client[1277]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:51:00 ceph-node-5 mds[8367]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:50:40 ceph-node-4 radosgw[1009]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:21 ceph-node-4 client[6868]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:26 ceph-node-3 osd[3590]: NOTICE: MGR module loaded successfully
2025-02-24 10:50:38 ceph-node-3 mds[3572]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:50:43 ceph-node-1 radosgw[3148]: NOTICE: OSD rebalancing completed
2025-02-24 10:51:19 ceph-node-2 osd[5547]: INFO: OSD disk space utilization: 70%
2025-02-24 10:50:31 ceph-node-3 mds[1418]: INFO: OSD disk space utilization: 70%
2025-02-24 10:51:14 ceph-node-2 mds[1618]: CRITICAL: Monitor mon1 failed to reach quorum
2025-02-24 10:50:59 ceph-node-2 client[2941]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:51:10 ceph-node-3 osd[4839]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:50:48 ceph-node-1 mds[6851]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:50:57 ceph-node-2 client[7981]: WARNING: OSD disk space utilization exceeded 90%
2025-02-24 10:51:23 ceph-node-4 mon[4060]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:30 ceph-node-1 client[5842]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:11 ceph-node-1 mds[9168]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:02 ceph-node-1 mgr[5120]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:50:46 ceph-node-5 client[5773]: ERROR: OSD 3.1 marked down due to heartbeat failure
2025-02-24 10:50:43 ceph-node-3 radosgw[7644]: WARNING: Authentication failure in RADOS Gateway
2025-02-24 10:51:29 ceph-node-1 mds[4211]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:11 ceph-node-1 mon[2066]: NOTICE: Monitor map has been updated
2025-02-24 10:50:57 ceph-node-4 mgr[7939]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:34 ceph-node-3 mon[7872]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:59 ceph-node-2 mgr[8261]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:07 ceph-node-2 radosgw[9656]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:51:11 ceph-node-1 client[8298]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:51:15 ceph-node-2 mds[6043]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:00 ceph-node-2 mgr[8215]: INFO: OSD rebalancing completed
2025-02-24 10:50:45 ceph-node-4 radosgw[8376]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:27 ceph-node-5 mgr[7015]: CRITICAL: Journal write failure detected on OSD node
2025-02-24 10:50:56 ceph-node-4 radosgw[9630]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:23 ceph-node-3 mgr[1606]: DEBUG: OSD rebalancing completed
2025-02-24 10:50:58 ceph-node-5 radosgw[3516]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:51:04 ceph-node-3 osd[1131]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:20 ceph-node-1 mgr[3137]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:37 ceph-node-3 radosgw[9217]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:02 ceph-node-4 radosgw[5240]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:34 ceph-node-3 mgr[1982]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:48 ceph-node-2 client[8783]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:50:32 ceph-node-1 radosgw[6517]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:15 ceph-node-3 client[1357]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:51:11 ceph-node-1 osd[7863]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:14 ceph-node-5 osd[6407]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:13 ceph-node-5 mds[4675]: ERROR: Monitor mon1 failed to reach quorum
2025-02-24 10:50:35 ceph-node-5 client[1461]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:47 ceph-node-4 mgr[4268]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:10 ceph-node-5 osd[8731]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:12 ceph-node-5 client[8112]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:15 ceph-node-1 radosgw[8819]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:11 ceph-node-2 radosgw[6480]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:07 ceph-node-1 mon[4154]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:09 ceph-node-2 osd[8315]: ERROR: Disk failure reported on ceph-node-4
2025-02-24 10:51:31 ceph-node-3 client[7288]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:14 ceph-node-3 client[5227]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:00 ceph-node-1 radosgw[2406]: DEBUG: Monitor map has been updated
2025-02-24 10:50:58 ceph-node-1 mds[9447]: NOTICE: Data replication completed for object pool
2025-02-24 10:50:50 ceph-node-3 osd[8028]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:07 ceph-node-5 mon[4269]: NOTICE: MGR module loaded successfully
2025-02-24 10:50:32 ceph-node-2 mgr[2743]: INFO: OSD disk space utilization: 70%
2025-02-24 10:50:52 ceph-node-4 osd[9595]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:27 ceph-node-1 mon[6694]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:50:33 ceph-node-4 osd[4670]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:50:36 ceph-node-2 mon[3080]: DEBUG: OSD rebalancing completed
2025-02-24 10:51:23 ceph-node-4 mgr[8905]: WARNING: Slow request detected on OSD 4.3
2025-02-24 10:51:19 ceph-node-5 radosgw[5516]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:51:01 ceph-node-2 osd[8693]: DEBUG: MGR module loaded successfully
2025-02-24 10:50:49 ceph-node-1 mon[2595]: INFO: Data replication completed for object pool
2025-02-24 10:50:39 ceph-node-1 mgr[5049]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:53 ceph-node-3 osd[5762]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:35 ceph-node-2 radosgw[8060]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:50:45 ceph-node-5 mds[2876]: ERROR: High I/O latency detected in radosgw service
2025-02-24 10:50:53 ceph-node-1 mgr[2508]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:50:53 ceph-node-1 mgr[9238]: CRITICAL: CRITICAL: PG 2.1 stuck in inconsistent state
2025-02-24 10:50:49 ceph-node-5 radosgw[6904]: DEBUG: OSD rebalancing completed
2025-02-24 10:50:59 ceph-node-3 radosgw[4974]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:51:31 ceph-node-2 client[9783]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:47 ceph-node-5 osd[5623]: INFO: OSD rebalancing completed
2025-02-24 10:50:34 ceph-node-1 mds[8910]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:50:53 ceph-node-5 client[5872]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:51:14 ceph-node-5 radosgw[4074]: INFO: OSD disk space utilization: 70%
2025-02-24 10:51:07 ceph-node-4 mgr[4360]: INFO: MGR module loaded successfully
2025-02-24 10:50:51 ceph-node-1 mon[2058]: INFO: OSD disk space utilization: 70%
2025-02-24 10:51:09 ceph-node-1 radosgw[3403]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:51:25 ceph-node-2 mgr[1382]: DEBUG: OSD rebalancing completed
2025-02-24 10:51:22 ceph-node-5 mds[4001]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:50:34 ceph-node-3 mgr[7438]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:04 ceph-node-4 radosgw[4744]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:51:21 ceph-node-2 mds[8845]: INFO: Data replication completed for object pool
2025-02-24 10:51:25 ceph-node-2 radosgw[7226]: INFO: MGR module loaded successfully
2025-02-24 10:50:40 ceph-node-5 osd[6342]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:50:47 ceph-node-4 mon[8240]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:46 ceph-node-1 radosgw[7132]: WARNING: Disk failure reported on ceph-node-4
2025-02-24 10:51:09 ceph-node-5 osd[4933]: DEBUG: Monitor map has been updated
2025-02-24 10:50:56 ceph-node-2 osd[5277]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:50:40 ceph-node-3 osd[4888]: NOTICE: MGR module loaded successfully
2025-02-24 10:51:29 ceph-node-3 mgr[7712]: NOTICE: Data replication completed for object pool
2025-02-24 10:51:20 ceph-node-1 client[3908]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:51:28 ceph-node-1 mds[4866]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:29 ceph-node-2 mon[2314]: WARNING: CRITICAL: PG 2.1 stuck in inconsistent state
2025-02-24 10:50:31 ceph-node-4 radosgw[2893]: INFO: Data replication completed for object pool
2025-02-24 10:51:12 ceph-node-1 osd[8896]: INFO: Data replication completed for object pool
2025-02-24 10:51:12 ceph-node-2 mgr[9813]: ERROR: Disk failure reported on ceph-node-4
2025-02-24 10:50:46 ceph-node-3 mon[9933]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:23 ceph-node-1 client[7217]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:51:06 ceph-node-3 mon[6037]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:55 ceph-node-2 radosgw[9667]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:12 ceph-node-2 mon[3183]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:50:52 ceph-node-1 osd[4152]: NOTICE: MGR module loaded successfully
2025-02-24 10:51:14 ceph-node-1 mds[7793]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:25 ceph-node-5 osd[7793]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:46 ceph-node-1 mgr[2816]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:50:54 ceph-node-1 mgr[4709]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:00 ceph-node-3 radosgw[1747]: INFO: MGR module loaded successfully
2025-02-24 10:50:43 ceph-node-1 mgr[5110]: DEBUG: OSD rebalancing completed
2025-02-24 10:50:37 ceph-node-2 radosgw[6416]: INFO: Data replication completed for object pool
2025-02-24 10:51:01 ceph-node-5 radosgw[3925]: ERROR: Client connection timeout detected
2025-02-24 10:50:41 ceph-node-5 mds[3785]: INFO: OSD disk space utilization: 70%
2025-02-24 10:51:28 ceph-node-5 radosgw[2554]: DEBUG: Data replication completed for object pool
2025-02-24 10:50:45 ceph-node-2 mgr[8444]: INFO: MGR module loaded successfully
2025-02-24 10:51:23 ceph-node-3 mds[1437]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:23 ceph-node-5 mgr[3406]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:51:06 ceph-node-1 mon[9994]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:51:03 ceph-node-1 mds[2683]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:50:52 ceph-node-4 radosgw[8060]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:28 ceph-node-1 mgr[3122]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:51:07 ceph-node-1 mon[9010]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:52 ceph-node-1 client[4996]: DEBUG: Data replication completed for object pool
2025-02-24 10:50:55 ceph-node-2 osd[8812]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:07 ceph-node-5 radosgw[3554]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:51:07 ceph-node-3 mgr[1327]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:50:33 ceph-node-2 radosgw[3302]: NOTICE: Data replication completed for object pool
2025-02-24 10:51:01 ceph-node-2 radosgw[1130]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:05 ceph-node-2 client[7385]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:51:20 ceph-node-3 mgr[9605]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:50:32 ceph-node-5 client[6267]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:50:55 ceph-node-2 client[5370]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:50:55 ceph-node-3 radosgw[5171]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:36 ceph-node-5 client[1810]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:10 ceph-node-4 client[9142]: NOTICE: Data replication completed for object pool
2025-02-24 10:51:04 ceph-node-2 mds[6391]: NOTICE: Data replication completed for object pool
2025-02-24 10:50:57 ceph-node-1 mon[7496]: WARNING: MDS failure detected: metadata inconsistency
2025-02-24 10:50:41 ceph-node-2 mgr[6709]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:00 ceph-node-3 radosgw[3898]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:55 ceph-node-4 mgr[6134]: ERROR: Slow request detected on OSD 4.3
2025-02-24 10:51:01 ceph-node-1 client[1724]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:50:38 ceph-node-2 mds[4864]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:50:48 ceph-node-2 client[8270]: INFO: PG 1.2 replicated successfully
2025-02-24 10:51:24 ceph-node-1 radosgw[9831]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:19 ceph-node-5 radosgw[9107]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:18 ceph-node-1 mon[2595]: DEBUG: OSD rebalancing completed
2025-02-24 10:50:37 ceph-node-5 osd[3934]: DEBUG: Monitor map has been updated
2025-02-24 10:51:02 ceph-node-1 client[2116]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:09 ceph-node-3 mon[9565]: NOTICE: Data replication completed for object pool
2025-02-24 10:50:43 ceph-node-5 radosgw[6134]: INFO: OSD rebalancing completed
2025-02-24 10:51:23 ceph-node-5 client[7263]: ERROR: OSD disk space utilization exceeded 90%
2025-02-24 10:50:38 ceph-node-5 radosgw[4782]: DEBUG: OSD rebalancing completed
2025-02-24 10:50:52 ceph-node-1 osd[3750]: DEBUG: Monitor map has been updated
2025-02-24 10:50:38 ceph-node-4 mgr[9598]: NOTICE: OSD rebalancing completed
2025-02-24 10:51:17 ceph-node-2 mgr[2144]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:51:29 ceph-node-1 mgr[3855]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:50:37 ceph-node-3 client[4081]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:50:49 ceph-node-5 radosgw[3932]: NOTICE: OSD rebalancing completed
2025-02-24 10:50:43 ceph-node-1 radosgw[7676]: INFO: OSD disk space utilization: 70%
2025-02-24 10:50:32 ceph-node-4 mon[2633]: INFO: Data replication completed for object pool
2025-02-24 10:51:26 ceph-node-2 mgr[3246]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:51:26 ceph-node-1 mgr[1421]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:19 ceph-node-1 mds[2817]: INFO: MGR module loaded successfully
2025-02-24 10:50:37 ceph-node-5 mgr[2984]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:00 ceph-node-1 mgr[6495]: INFO: OSD rebalancing completed
2025-02-24 10:50:34 ceph-node-1 client[1952]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:51:01 ceph-node-2 mds[7708]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:50:50 ceph-node-1 radosgw[8267]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:33 ceph-node-2 mon[5713]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:51:21 ceph-node-3 radosgw[4821]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:51:18 ceph-node-2 mon[4037]: NOTICE: OSD rebalancing completed
2025-02-24 10:51:23 ceph-node-5 mon[6974]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:50:47 ceph-node-2 osd[8155]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:23 ceph-node-5 radosgw[9563]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:51:23 ceph-node-3 osd[2405]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:39 ceph-node-4 client[2095]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:19 ceph-node-1 radosgw[6579]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:50:32 ceph-node-1 mgr[8287]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:05 ceph-node-5 mds[7806]: CRITICAL: Monitor down, unable to connect to quorum
2025-02-24 10:51:12 ceph-node-4 mon[9294]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:50:39 ceph-node-4 mgr[1233]: INFO: MGR module loaded successfully
2025-02-24 10:50:39 ceph-node-3 radosgw[9011]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:22 ceph-node-4 mgr[2612]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:28 ceph-node-1 mgr[8744]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:50:56 ceph-node-1 client[1315]: INFO: MGR module loaded successfully
2025-02-24 10:51:30 ceph-node-1 osd[7028]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:18 ceph-node-3 radosgw[8913]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:02 ceph-node-5 mds[2728]: DEBUG: Monitor map has been updated
2025-02-24 10:50:35 ceph-node-1 mds[5192]: INFO: MGR module loaded successfully
2025-02-24 10:50:56 ceph-node-4 mds[6053]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:50:44 ceph-node-2 mds[9126]: INFO: OSD disk space utilization: 70%
2025-02-24 10:50:35 ceph-node-1 osd[7265]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:50:51 ceph-node-4 mds[9913]: DEBUG: MGR module loaded successfully
2025-02-24 10:50:45 ceph-node-5 client[4485]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:24 ceph-node-2 mds[4669]: DEBUG: Monitor map has been updated
2025-02-24 10:51:17 ceph-node-4 radosgw[7038]: CRITICAL: Cluster status: HEALTH_WARN
2025-02-24 10:51:01 ceph-node-4 mds[1681]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:50:40 ceph-node-2 radosgw[1403]: WARNING: Uncorrectable ECC error detected in MDS cache
2025-02-24 10:51:26 ceph-node-3 mon[4162]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:51:06 ceph-node-4 mon[1517]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:50 ceph-node-5 osd[8192]: CRITICAL: Network partition detected between OSD nodes
2025-02-24 10:51:17 ceph-node-5 mon[3320]: NOTICE: MGR module loaded successfully
2025-02-24 10:51:21 ceph-node-5 mgr[5645]: ERROR: Client connection timeout detected
2025-02-24 10:51:18 ceph-node-5 client[8215]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:00 ceph-node-4 client[8557]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:51:13 ceph-node-1 mon[9475]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:40 ceph-node-5 mds[3812]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:53 ceph-node-1 mds[3671]: INFO: PG 1.2 replicated successfully
2025-02-24 10:51:03 ceph-node-2 client[7582]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:50:47 ceph-node-4 osd[4321]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:50:49 ceph-node-3 osd[7018]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:51:12 ceph-node-2 mon[8231]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:13 ceph-node-3 mgr[3125]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:21 ceph-node-5 mgr[7330]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:51:26 ceph-node-1 mon[3520]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:01 ceph-node-4 radosgw[8455]: NOTICE: Data replication completed for object pool
2025-02-24 10:50:40 ceph-node-5 mgr[5945]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:31 ceph-node-2 client[2131]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:54 ceph-node-5 client[3498]: NOTICE: MGR module loaded successfully
2025-02-24 10:51:26 ceph-node-4 mds[1272]: INFO: Data replication completed for object pool
2025-02-24 10:51:10 ceph-node-5 client[7191]: NOTICE: OSD rebalancing completed
2025-02-24 10:50:44 ceph-node-3 client[8428]: ERROR: Slow request detected on OSD 4.3
2025-02-24 10:50:58 ceph-node-3 mds[9555]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:50:56 ceph-node-3 mgr[2305]: NOTICE: Monitor map has been updated
2025-02-24 10:51:07 ceph-node-4 mds[2840]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:50:52 ceph-node-5 radosgw[1151]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:04 ceph-node-4 client[4442]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:50:33 ceph-node-4 mon[9250]: INFO: OSD disk space utilization: 70%
2025-02-24 10:51:05 ceph-node-4 mgr[1325]: NOTICE: Data replication completed for object pool
2025-02-24 10:51:19 ceph-node-5 mgr[2107]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:03 ceph-node-2 osd[2067]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:50:48 ceph-node-3 mds[4150]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:00 ceph-node-4 mon[3514]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:20 ceph-node-1 osd[7551]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:13 ceph-node-2 client[7319]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:50:31 ceph-node-1 mds[6353]: INFO: OSD disk space utilization: 70%
2025-02-24 10:51:14 ceph-node-3 client[3980]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:06 ceph-node-4 mgr[8440]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:32 ceph-node-1 mon[9978]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:50:44 ceph-node-3 mds[1965]: WARNING: Monitor down, unable to connect to quorum
2025-02-24 10:51:23 ceph-node-3 mon[2806]: NOTICE: OSD rebalancing completed
2025-02-24 10:51:06 ceph-node-5 radosgw[4926]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:08 ceph-node-5 mon[9444]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:02 ceph-node-1 mgr[9792]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:51:06 ceph-node-5 mds[8540]: DEBUG: Monitor map has been updated
2025-02-24 10:50:33 ceph-node-1 osd[3473]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:24 ceph-node-5 mds[3336]: INFO: Monitor map has been updated
2025-02-24 10:51:05 ceph-node-5 client[7107]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:55 ceph-node-1 mgr[2498]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:50:31 ceph-node-1 osd[9445]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:12 ceph-node-1 mon[7271]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:51:26 ceph-node-4 client[3484]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:23 ceph-node-3 radosgw[6288]: CRITICAL: Journal write failure detected on OSD node
2025-02-24 10:50:41 ceph-node-1 mgr[8954]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:03 ceph-node-3 client[7114]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:50:34 ceph-node-1 client[1937]: NOTICE: Monitor map has been updated
2025-02-24 10:50:31 ceph-node-5 client[6134]: WARNING: OSD disk space utilization exceeded 90%
2025-02-24 10:50:44 ceph-node-5 mgr[6736]: DEBUG: Monitor map has been updated
2025-02-24 10:51:26 ceph-node-4 mon[1290]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:50:56 ceph-node-2 mon[8495]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:22 ceph-node-2 mgr[6012]: DEBUG: OSD rebalancing completed
2025-02-24 10:51:31 ceph-node-1 radosgw[5021]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:50:56 ceph-node-4 radosgw[5154]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:09 ceph-node-2 radosgw[1703]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:51:13 ceph-node-4 radosgw[3907]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:23 ceph-node-4 mon[3846]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:36 ceph-node-1 osd[4095]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:50:49 ceph-node-4 osd[7555]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:51:02 ceph-node-5 client[9219]: INFO: OSD disk space utilization: 70%
2025-02-24 10:50:53 ceph-node-1 mon[4719]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:01 ceph-node-1 mon[4338]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:50:50 ceph-node-2 osd[7585]: NOTICE: MGR module loaded successfully
2025-02-24 10:50:58 ceph-node-5 mgr[8822]: NOTICE: MGR module loaded successfully
2025-02-24 10:51:08 ceph-node-5 osd[6051]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:50:41 ceph-node-1 mgr[9055]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:58 ceph-node-5 mgr[7020]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:04 ceph-node-3 mon[5512]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:49 ceph-node-5 radosgw[4579]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:46 ceph-node-3 mds[1206]: INFO: Data replication completed for object pool
2025-02-24 10:51:11 ceph-node-4 radosgw[8824]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:51:24 ceph-node-3 mgr[9364]: INFO: OSD rebalancing completed
2025-02-24 10:50:40 ceph-node-3 mds[2152]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:33 ceph-node-3 mgr[4857]: INFO: MGR module loaded successfully
2025-02-24 10:51:00 ceph-node-5 client[9454]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:25 ceph-node-5 mgr[9291]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:44 ceph-node-5 mgr[1508]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:53 ceph-node-4 client[7041]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:51:27 ceph-node-2 client[1345]: CRITICAL: MDS failure detected: metadata inconsistency
2025-02-24 10:50:31 ceph-node-2 mgr[4752]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:51:22 ceph-node-2 mds[1772]: INFO: PG 1.2 replicated successfully
2025-02-24 10:50:42 ceph-node-1 osd[4045]: INFO: PG 1.2 replicated successfully
2025-02-24 10:50:42 ceph-node-1 osd[1985]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:59 ceph-node-3 mon[5484]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:05 ceph-node-3 radosgw[8397]: DEBUG: Monitor map has been updated
2025-02-24 10:50:34 ceph-node-3 mon[1349]: DEBUG: Monitor map has been updated
2025-02-24 10:51:27 ceph-node-3 mon[9333]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:50:41 ceph-node-5 mon[3807]: DEBUG: OSD rebalancing completed
2025-02-24 10:50:52 ceph-node-2 radosgw[7228]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:51:04 ceph-node-2 radosgw[6250]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:06 ceph-node-5 client[8508]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:02 ceph-node-3 osd[5615]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:50:59 ceph-node-2 mon[8740]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:35 ceph-node-5 mon[9167]: NOTICE: Monitor map has been updated
2025-02-24 10:50:38 ceph-node-3 client[5569]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:50:36 ceph-node-2 mgr[2120]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:23 ceph-node-4 mon[8669]: NOTICE: MGR module loaded successfully
2025-02-24 10:51:01 ceph-node-2 radosgw[8321]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:51:14 ceph-node-3 mon[5405]: NOTICE: OSD rebalancing completed
2025-02-24 10:50:47 ceph-node-4 osd[2778]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:50:54 ceph-node-4 mds[5821]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:50:39 ceph-node-2 radosgw[5925]: INFO: OSD rebalancing completed
2025-02-24 10:51:16 ceph-node-4 mgr[7500]: NOTICE: MGR module loaded successfully
2025-02-24 10:50:32 ceph-node-2 mds[8710]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:16 ceph-node-3 mon[1891]: INFO: Data replication completed for object pool
2025-02-24 10:50:39 ceph-node-4 mon[1007]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:50:49 ceph-node-2 mds[4771]: WARNING: Journal write failure detected on OSD node
2025-02-24 10:51:21 ceph-node-4 mgr[6646]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:04 ceph-node-3 mon[8710]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:51:21 ceph-node-3 mon[4559]: DEBUG: OSD rebalancing completed
2025-02-24 10:51:14 ceph-node-5 osd[5227]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:49 ceph-node-1 mon[1758]: NOTICE: Data replication completed for object pool
2025-02-24 10:51:25 ceph-node-4 mgr[6981]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:50:38 ceph-node-4 mds[3227]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:16 ceph-node-1 mgr[7425]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:03 ceph-node-2 client[3100]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:50:59 ceph-node-1 mon[8606]: INFO: OSD rebalancing completed
2025-02-24 10:51:14 ceph-node-1 osd[3834]: NOTICE: Data replication completed for object pool
2025-02-24 10:51:19 ceph-node-4 radosgw[7500]: DEBUG: OSD rebalancing completed
2025-02-24 10:51:06 ceph-node-1 osd[6238]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:31 ceph-node-1 osd[6303]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:51:11 ceph-node-5 client[8999]: CRITICAL: Uncorrectable ECC error detected in MDS cache
2025-02-24 10:51:24 ceph-node-2 mon[7946]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:46 ceph-node-2 mgr[3149]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:50:56 ceph-node-3 radosgw[5259]: NOTICE: MGR module loaded successfully
2025-02-24 10:51:11 ceph-node-2 mon[4019]: INFO: Data replication completed for object pool
2025-02-24 10:50:35 ceph-node-1 radosgw[6505]: ERROR: CRITICAL: Data loss detected on pool 1
2025-02-24 10:50:47 ceph-node-3 client[7780]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:50:49 ceph-node-1 mgr[5278]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:42 ceph-node-5 mds[7431]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:50:39 ceph-node-1 mds[1798]: DEBUG: OSD rebalancing completed
2025-02-24 10:51:06 ceph-node-2 mds[8794]: NOTICE: OSD rebalancing completed
2025-02-24 10:51:03 ceph-node-5 osd[1224]: INFO: Data replication completed for object pool
2025-02-24 10:50:59 ceph-node-2 mgr[5818]: DEBUG: Monitor map has been updated
2025-02-24 10:50:48 ceph-node-3 radosgw[7627]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:10 ceph-node-3 radosgw[8568]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:22 ceph-node-3 mds[1082]: INFO: Monitor map has been updated
2025-02-24 10:51:26 ceph-node-1 radosgw[2401]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:53 ceph-node-2 radosgw[6177]: DEBUG: Monitor map has been updated
2025-02-24 10:51:23 ceph-node-2 client[2693]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:51:17 ceph-node-1 client[7609]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:21 ceph-node-4 client[3493]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:47 ceph-node-3 osd[9529]: ERROR: MDS failure detected: metadata inconsistency
2025-02-24 10:51:31 ceph-node-3 mon[7833]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:23 ceph-node-3 mds[2265]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:50:53 ceph-node-5 client[5652]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:50:54 ceph-node-5 mds[5960]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:30 ceph-node-3 client[2144]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:50:40 ceph-node-3 mds[2490]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:50:32 ceph-node-1 mds[3039]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:50:39 ceph-node-3 mds[2227]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:02 ceph-node-3 mds[3708]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:50:51 ceph-node-1 client[3324]: CRITICAL: Journal write failure detected on OSD node
2025-02-24 10:50:55 ceph-node-3 client[4803]: ERROR: CRITICAL: Data loss detected on pool 1
2025-02-24 10:50:42 ceph-node-4 client[5127]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:20 ceph-node-1 mds[5567]: ERROR: Monitor down, unable to connect to quorum
2025-02-24 10:50:53 ceph-node-5 mds[1075]: NOTICE: OSD rebalancing completed
2025-02-24 10:50:47 ceph-node-2 mgr[9622]: INFO: OSD disk space utilization: 70%
2025-02-24 10:50:51 ceph-node-1 mon[8397]: INFO: MGR module loaded successfully
2025-02-24 10:50:36 ceph-node-1 osd[4064]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:28 ceph-node-5 mon[2816]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:25 ceph-node-3 osd[4648]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:51:23 ceph-node-4 radosgw[1830]: CRITICAL: Monitor mon1 failed to reach quorum
2025-02-24 10:51:26 ceph-node-3 mgr[1995]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:25 ceph-node-3 client[3093]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:14 ceph-node-5 radosgw[3265]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:50:54 ceph-node-2 mgr[6356]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:50:47 ceph-node-1 mds[9146]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:12 ceph-node-3 radosgw[4550]: INFO: Monitor map has been updated
2025-02-24 10:50:49 ceph-node-2 mds[4465]: INFO: MGR module loaded successfully
2025-02-24 10:51:01 ceph-node-1 osd[4509]: WARNING: Slow request detected on OSD 4.3
2025-02-24 10:50:36 ceph-node-1 client[7525]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:50:56 ceph-node-5 mds[3596]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:04 ceph-node-3 osd[2279]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:51:19 ceph-node-4 mgr[4434]: CRITICAL: CRITICAL: PG 2.1 stuck in inconsistent state
2025-02-24 10:50:36 ceph-node-3 client[2987]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:02 ceph-node-5 osd[5562]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:27 ceph-node-1 mon[8719]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:50:46 ceph-node-4 mgr[7988]: ERROR: Uncorrectable ECC error detected in MDS cache
2025-02-24 10:50:49 ceph-node-1 osd[6388]: CRITICAL: MDS failure detected: metadata inconsistency
2025-02-24 10:51:04 ceph-node-5 osd[3668]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:12 ceph-node-1 mgr[5270]: WARNING: Monitor mon1 failed to reach quorum
2025-02-24 10:51:21 ceph-node-1 mds[1416]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:50:48 ceph-node-2 mds[4605]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:50:49 ceph-node-4 mds[7383]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:57 ceph-node-5 osd[9218]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:50 ceph-node-4 radosgw[8479]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:25 ceph-node-1 mgr[9682]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:33 ceph-node-3 radosgw[6024]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:02 ceph-node-2 mon[7556]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:50:56 ceph-node-4 radosgw[8003]: DEBUG: Data replication completed for object pool
2025-02-24 10:50:50 ceph-node-3 mon[9510]: DEBUG: Monitor map has been updated
2025-02-24 10:50:44 ceph-node-3 mds[4321]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:50:41 ceph-node-1 mgr[8225]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:51:06 ceph-node-3 radosgw[2832]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:50:36 ceph-node-3 client[1944]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:23 ceph-node-5 mds[7632]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:34 ceph-node-1 mds[1358]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:51:06 ceph-node-1 mds[7035]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:51:11 ceph-node-3 mon[8249]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:50:40 ceph-node-3 client[9934]: INFO: Monitor map has been updated
2025-02-24 10:50:46 ceph-node-2 mon[8037]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:50:32 ceph-node-1 client[3786]: INFO: MGR module loaded successfully
2025-02-24 10:51:18 ceph-node-3 client[5717]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:51:01 ceph-node-5 radosgw[2516]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:51:26 ceph-node-1 mon[2229]: ERROR: Authentication failure in RADOS Gateway
2025-02-24 10:50:42 ceph-node-3 radosgw[8185]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:13 ceph-node-2 client[7018]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:48 ceph-node-4 osd[9900]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:25 ceph-node-3 radosgw[1795]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:51:24 ceph-node-5 client[4297]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:22 ceph-node-5 mon[1628]: DEBUG: Monitor map has been updated
2025-02-24 10:51:13 ceph-node-4 osd[9706]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:50:56 ceph-node-4 osd[7780]: INFO: OSD rebalancing completed
2025-02-24 10:50:33 ceph-node-5 osd[5641]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:51:31 ceph-node-1 osd[7944]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:51 ceph-node-4 osd[8194]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:51:07 ceph-node-4 mgr[7726]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:24 ceph-node-2 mds[9798]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:01 ceph-node-3 mgr[3237]: ERROR: Network partition detected between OSD nodes
2025-02-24 10:50:45 ceph-node-4 osd[2562]: DEBUG: OSD rebalancing completed
2025-02-24 10:50:43 ceph-node-3 client[9573]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:20 ceph-node-2 mon[6938]: DEBUG: MGR module loaded successfully
2025-02-24 10:50:55 ceph-node-3 mds[7542]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:51:01 ceph-node-3 radosgw[4192]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:51:17 ceph-node-3 client[6639]: DEBUG: OSD rebalancing completed
2025-02-24 10:50:37 ceph-node-5 mgr[9150]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:17 ceph-node-3 osd[3151]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:25 ceph-node-4 client[6280]: INFO: MGR module loaded successfully
2025-02-24 10:50:44 ceph-node-5 mon[6235]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:51:07 ceph-node-3 osd[1426]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:51:25 ceph-node-4 radosgw[4119]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:20 ceph-node-4 client[4681]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:51:08 ceph-node-5 mon[6221]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:12 ceph-node-1 mon[5945]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:32 ceph-node-4 client[2458]: ERROR: Disk failure reported on ceph-node-4
2025-02-24 10:51:15 ceph-node-1 mds[1068]: WARNING: Journal write failure detected on OSD node
2025-02-24 10:50:40 ceph-node-3 radosgw[7643]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:29 ceph-node-1 mon[3532]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:11 ceph-node-4 radosgw[4356]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:01 ceph-node-3 osd[9236]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:42 ceph-node-1 mds[3111]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:51:00 ceph-node-4 osd[1820]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:57 ceph-node-3 mds[2155]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:34 ceph-node-4 mds[4981]: DEBUG: OSD rebalancing completed
2025-02-24 10:51:30 ceph-node-1 mds[5617]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:05 ceph-node-2 mgr[5676]: INFO: PG 1.2 replicated successfully
2025-02-24 10:50:57 ceph-node-3 mon[1777]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:50:43 ceph-node-2 radosgw[1710]: NOTICE: MGR module loaded successfully
2025-02-24 10:50:49 ceph-node-2 radosgw[2708]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:13 ceph-node-5 client[2266]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:13 ceph-node-3 client[8054]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:24 ceph-node-1 radosgw[3187]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:51:23 ceph-node-5 mgr[2789]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:50:56 ceph-node-1 client[3177]: CRITICAL: OSD 3.1 marked down due to heartbeat failure
2025-02-24 10:51:23 ceph-node-2 osd[4215]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:25 ceph-node-1 mon[1837]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:51:29 ceph-node-1 mgr[8467]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:50:59 ceph-node-3 osd[1909]: INFO: Monitor map has been updated
2025-02-24 10:51:20 ceph-node-4 radosgw[9394]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:50:54 ceph-node-1 mgr[3008]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:50:59 ceph-node-4 client[1138]: DEBUG: Monitor map has been updated
2025-02-24 10:51:09 ceph-node-2 client[2515]: INFO: MGR module loaded successfully
2025-02-24 10:50:50 ceph-node-1 mon[1725]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:14 ceph-node-4 client[3018]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:50:53 ceph-node-4 mon[2336]: CRITICAL: CRITICAL: PG 2.1 stuck in inconsistent state
2025-02-24 10:51:25 ceph-node-5 mon[2253]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:08 ceph-node-3 osd[3856]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:17 ceph-node-1 mgr[5139]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:16 ceph-node-3 radosgw[8429]: INFO: PG 1.2 replicated successfully
2025-02-24 10:51:11 ceph-node-1 mds[7507]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:05 ceph-node-5 mgr[4899]: WARNING: High I/O latency detected in radosgw service
2025-02-24 10:50:37 ceph-node-4 mon[9941]: ERROR: Cluster status: HEALTH_WARN
2025-02-24 10:50:53 ceph-node-5 radosgw[8596]: INFO: PG 1.2 replicated successfully
2025-02-24 10:50:46 ceph-node-2 mon[4460]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:50:31 ceph-node-1 mon[9573]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:21 ceph-node-2 mgr[3545]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:51:13 ceph-node-3 mgr[8612]: NOTICE: Monitor map has been updated
2025-02-24 10:51:10 ceph-node-3 mon[6035]: INFO: OSD disk space utilization: 70%
2025-02-24 10:50:41 ceph-node-1 mon[9002]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:05 ceph-node-2 mgr[5046]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:11 ceph-node-5 mds[3702]: DEBUG: Monitor map has been updated
2025-02-24 10:50:40 ceph-node-5 mds[3583]: DEBUG: Data replication completed for object pool
2025-02-24 10:50:56 ceph-node-3 osd[8436]: WARNING: OSD disk space utilization exceeded 90%
2025-02-24 10:50:57 ceph-node-4 osd[7982]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:01 ceph-node-2 mds[6246]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:50:56 ceph-node-2 mds[3299]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:50:47 ceph-node-5 mgr[8389]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:51:05 ceph-node-4 radosgw[4751]: DEBUG: Monitor map has been updated
2025-02-24 10:50:42 ceph-node-5 radosgw[9446]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:50:57 ceph-node-4 osd[2814]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:16 ceph-node-3 mgr[8598]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:41 ceph-node-5 mon[2997]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:50:59 ceph-node-1 osd[7148]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:50:50 ceph-node-3 radosgw[6145]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:50:51 ceph-node-3 mgr[3041]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:45 ceph-node-2 mon[3937]: INFO: OSD rebalancing completed
2025-02-24 10:50:54 ceph-node-2 client[7230]: NOTICE: Data replication completed for object pool
2025-02-24 10:51:06 ceph-node-3 mds[3458]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:50:59 ceph-node-3 radosgw[6140]: INFO: OSD disk space utilization: 70%
2025-02-24 10:50:44 ceph-node-3 mgr[7569]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:02 ceph-node-2 osd[9824]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:17 ceph-node-4 radosgw[8515]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:22 ceph-node-2 mon[1468]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:51:25 ceph-node-5 osd[2902]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:03 ceph-node-4 osd[1020]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:28 ceph-node-3 mgr[3867]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:05 ceph-node-3 osd[9385]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:27 ceph-node-4 radosgw[1407]: INFO: OSD rebalancing completed
2025-02-24 10:51:30 ceph-node-1 radosgw[3374]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:51:06 ceph-node-1 osd[1021]: ERROR: CRITICAL: Data loss detected on pool 1
2025-02-24 10:51:31 ceph-node-1 mon[4423]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:12 ceph-node-1 client[9396]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:08 ceph-node-5 osd[8080]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:01 ceph-node-1 mgr[6206]: INFO: OSD rebalancing completed
2025-02-24 10:50:47 ceph-node-5 osd[4345]: INFO: Data replication completed for object pool
2025-02-24 10:50:38 ceph-node-2 osd[8230]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:50 ceph-node-3 mgr[8143]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:51:16 ceph-node-4 mgr[4796]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:51:00 ceph-node-3 radosgw[5813]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:14 ceph-node-3 client[2043]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:50:32 ceph-node-1 mds[8232]: WARNING: Data corruption detected in object pool
2025-02-24 10:51:12 ceph-node-3 mds[9584]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:42 ceph-node-1 osd[8183]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:38 ceph-node-5 mds[6970]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:51:04 ceph-node-1 osd[5863]: WARNING: Monitor down, unable to connect to quorum
2025-02-24 10:50:51 ceph-node-5 client[3910]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:15 ceph-node-1 client[1265]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:21 ceph-node-1 radosgw[6630]: INFO: OSD rebalancing completed
2025-02-24 10:50:52 ceph-node-3 mgr[1013]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:50:45 ceph-node-5 client[7411]: ERROR: MDS failure detected: metadata inconsistency
2025-02-24 10:51:18 ceph-node-5 radosgw[6360]: INFO: PG 1.2 replicated successfully
2025-02-24 10:50:37 ceph-node-1 mgr[2409]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:08 ceph-node-4 mgr[4575]: INFO: MGR module loaded successfully
2025-02-24 10:51:22 ceph-node-3 mds[2819]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:06 ceph-node-1 mon[2788]: WARNING: OSD disk space utilization exceeded 90%
2025-02-24 10:51:25 ceph-node-3 mgr[1759]: WARNING: Network partition detected between OSD nodes
2025-02-24 10:51:30 ceph-node-2 mon[1676]: INFO: OSD rebalancing completed
2025-02-24 10:50:34 ceph-node-3 osd[4349]: NOTICE: Monitor map has been updated
2025-02-24 10:50:31 ceph-node-3 mds[3780]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:01 ceph-node-2 osd[4476]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:51:00 ceph-node-5 mds[7263]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:51:04 ceph-node-1 client[5089]: INFO: OSD disk space utilization: 70%
2025-02-24 10:50:42 ceph-node-2 radosgw[7152]: INFO: OSD disk space utilization: 70%
2025-02-24 10:50:41 ceph-node-2 mon[6555]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:13 ceph-node-4 client[3892]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:38 ceph-node-1 client[7494]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:50:32 ceph-node-4 mon[7349]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:53 ceph-node-2 mon[4918]: ERROR: Disk failure reported on ceph-node-4
2025-02-24 10:51:31 ceph-node-2 mgr[4249]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:34 ceph-node-2 mgr[6023]: DEBUG: Data replication completed for object pool
2025-02-24 10:50:44 ceph-node-3 mds[4357]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:22 ceph-node-3 mon[3932]: DEBUG: Monitor map has been updated
2025-02-24 10:50:51 ceph-node-2 mgr[3871]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:45 ceph-node-4 mgr[2961]: DEBUG: Monitor map has been updated
2025-02-24 10:51:13 ceph-node-5 mgr[8261]: NOTICE: Monitor map has been updated
2025-02-24 10:51:04 ceph-node-2 osd[4841]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:50:42 ceph-node-2 client[6561]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:50:39 ceph-node-3 osd[8923]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:21 ceph-node-4 mon[2371]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:51:12 ceph-node-4 osd[2993]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:16 ceph-node-1 radosgw[2975]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:12 ceph-node-5 mon[3989]: INFO: OSD rebalancing completed
2025-02-24 10:50:31 ceph-node-4 radosgw[2436]: NOTICE: Data replication completed for object pool
2025-02-24 10:50:46 ceph-node-4 client[2562]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:26 ceph-node-4 client[5499]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:50:54 ceph-node-1 radosgw[6989]: DEBUG: Data replication completed for object pool
2025-02-24 10:50:31 ceph-node-1 mgr[5526]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:50:35 ceph-node-1 client[8650]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:22 ceph-node-4 mgr[5465]: CRITICAL: CRITICAL: Data loss detected on pool 1
2025-02-24 10:51:12 ceph-node-4 mgr[5270]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:51:29 ceph-node-3 mon[6996]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:30 ceph-node-2 radosgw[2914]: INFO: Data replication completed for object pool
2025-02-24 10:51:07 ceph-node-2 mgr[6137]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:50:44 ceph-node-5 osd[7224]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:51:19 ceph-node-4 radosgw[2824]: NOTICE: MGR module loaded successfully
2025-02-24 10:50:33 ceph-node-4 client[6235]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:50:58 ceph-node-4 mgr[9891]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:44 ceph-node-3 client[5830]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:51:05 ceph-node-1 radosgw[6284]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:16 ceph-node-4 radosgw[6429]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:59 ceph-node-3 mgr[3044]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:23 ceph-node-5 mon[9618]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:50:55 ceph-node-1 mon[1139]: NOTICE: OSD rebalancing completed
2025-02-24 10:51:04 ceph-node-1 mon[7278]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:50:44 ceph-node-1 radosgw[4202]: CRITICAL: OSD disk space utilization exceeded 90%
2025-02-24 10:50:37 ceph-node-2 radosgw[2388]: CRITICAL: OSD 3.1 marked down due to heartbeat failure
2025-02-24 10:51:09 ceph-node-1 mgr[5285]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:50:51 ceph-node-4 mon[8962]: INFO: Data replication completed for object pool
2025-02-24 10:50:45 ceph-node-3 radosgw[9522]: NOTICE: Monitor map has been updated
2025-02-24 10:50:36 ceph-node-3 client[9405]: NOTICE: MGR module loaded successfully
2025-02-24 10:51:05 ceph-node-3 osd[7559]: ERROR: Disk failure reported on ceph-node-4
2025-02-24 10:51:02 ceph-node-3 mon[7078]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:20 ceph-node-4 radosgw[9746]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:21 ceph-node-5 client[1773]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:12 ceph-node-5 client[4900]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:51:18 ceph-node-3 mon[4339]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:51:16 ceph-node-1 mon[8204]: NOTICE: OSD rebalancing completed
2025-02-24 10:51:03 ceph-node-3 radosgw[7432]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:51:22 ceph-node-1 mgr[5815]: INFO: PG 1.2 replicated successfully
2025-02-24 10:50:47 ceph-node-3 mds[7503]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:51:25 ceph-node-1 client[7561]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:31 ceph-node-1 mon[8210]: DEBUG: MGR module loaded successfully
2025-02-24 10:50:32 ceph-node-4 radosgw[6477]: INFO: PG 1.2 replicated successfully
2025-02-24 10:50:37 ceph-node-1 client[7478]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:43 ceph-node-3 osd[7205]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:43 ceph-node-3 radosgw[4663]: NOTICE: MGR module loaded successfully
2025-02-24 10:51:03 ceph-node-4 mgr[7195]: NOTICE: MGR module loaded successfully
2025-02-24 10:50:47 ceph-node-5 mgr[6246]: NOTICE: OSD rebalancing completed
2025-02-24 10:50:58 ceph-node-1 mon[4126]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:04 ceph-node-5 osd[1544]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:51:06 ceph-node-2 radosgw[1225]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:10 ceph-node-5 mds[3855]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:51:29 ceph-node-2 mon[3374]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:50:36 ceph-node-5 mgr[7320]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:51:03 ceph-node-3 mds[9787]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:44 ceph-node-3 client[9170]: DEBUG: Monitor map has been updated
2025-02-24 10:51:00 ceph-node-3 radosgw[3603]: INFO: OSD disk space utilization: 70%
2025-02-24 10:50:36 ceph-node-1 mon[1297]: INFO: Monitor map has been updated
2025-02-24 10:51:13 ceph-node-1 osd[7699]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:50:39 ceph-node-5 mon[7818]: ERROR: Slow request detected on OSD 4.3
2025-02-24 10:50:52 ceph-node-1 mon[7776]: DEBUG: OSD rebalancing completed
2025-02-24 10:50:50 ceph-node-4 osd[7299]: INFO: OSD disk space utilization: 70%
2025-02-24 10:50:33 ceph-node-5 mds[5313]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:26 ceph-node-2 osd[9057]: DEBUG: Monitor map has been updated
2025-02-24 10:50:59 ceph-node-2 mgr[9537]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:44 ceph-node-5 radosgw[5496]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:11 ceph-node-1 client[8956]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:51:07 ceph-node-2 mon[1726]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:50:36 ceph-node-2 radosgw[1723]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:50:34 ceph-node-5 osd[3765]: DEBUG: OSD rebalancing completed
2025-02-24 10:51:21 ceph-node-3 mon[4471]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:50:35 ceph-node-5 osd[6187]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:28 ceph-node-3 mds[9756]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:51:14 ceph-node-4 radosgw[2037]: INFO: OSD rebalancing completed
2025-02-24 10:51:25 ceph-node-5 mgr[7994]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:50:59 ceph-node-1 mds[6630]: DEBUG: OSD rebalancing completed
2025-02-24 10:50:47 ceph-node-2 client[2673]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:11 ceph-node-3 mon[2222]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:06 ceph-node-2 mds[9876]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:31 ceph-node-2 mgr[9389]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:50:33 ceph-node-3 radosgw[2585]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:01 ceph-node-4 mds[6170]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:50:37 ceph-node-2 osd[3362]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:29 ceph-node-4 osd[8856]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:25 ceph-node-5 client[9973]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:50:39 ceph-node-4 client[3737]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:51:22 ceph-node-1 client[2127]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:50:57 ceph-node-2 client[4252]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:15 ceph-node-1 client[2495]: NOTICE: MGR module loaded successfully
2025-02-24 10:50:41 ceph-node-1 mgr[3244]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:50:44 ceph-node-3 mds[1595]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:50:35 ceph-node-2 client[5870]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:50:41 ceph-node-2 mon[4126]: ERROR: OSD disk space utilization exceeded 90%
2025-02-24 10:50:33 ceph-node-5 osd[4872]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:30 ceph-node-4 osd[5001]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:50:55 ceph-node-5 osd[9934]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:20 ceph-node-4 osd[1172]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:51:04 ceph-node-2 mgr[3054]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:31 ceph-node-5 mon[6266]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:22 ceph-node-2 osd[2265]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:50:51 ceph-node-5 mgr[1033]: NOTICE: MGR module loaded successfully
2025-02-24 10:51:01 ceph-node-5 radosgw[4828]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:11 ceph-node-3 radosgw[4384]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:51:07 ceph-node-4 radosgw[5292]: CRITICAL: Authentication failure in RADOS Gateway
2025-02-24 10:50:47 ceph-node-1 osd[2736]: WARNING: Slow request detected on OSD 4.3
2025-02-24 10:51:18 ceph-node-5 client[1283]: ERROR: OSD 3.1 marked down due to heartbeat failure
2025-02-24 10:51:31 ceph-node-5 mgr[4631]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:24 ceph-node-4 client[6537]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:33 ceph-node-2 mgr[8918]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:51:27 ceph-node-3 radosgw[4834]: DEBUG: OSD rebalancing completed
2025-02-24 10:50:52 ceph-node-2 client[6043]: DEBUG: OSD rebalancing completed
2025-02-24 10:51:17 ceph-node-1 mgr[5507]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:51:30 ceph-node-3 mds[1717]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:30 ceph-node-2 mgr[2969]: NOTICE: MGR module loaded successfully
2025-02-24 10:50:39 ceph-node-1 mon[3375]: CRITICAL: Cluster status: HEALTH_WARN
2025-02-24 10:50:42 ceph-node-1 mon[8334]: INFO: Monitor map has been updated
2025-02-24 10:51:08 ceph-node-4 radosgw[3388]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:50:41 ceph-node-4 mon[4765]: NOTICE: MGR module loaded successfully
2025-02-24 10:50:50 ceph-node-4 mds[5617]: INFO: OSD disk space utilization: 70%
2025-02-24 10:51:15 ceph-node-1 client[3004]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:18 ceph-node-2 mgr[4295]: DEBUG: OSD rebalancing completed
2025-02-24 10:51:07 ceph-node-5 mgr[8212]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:50:39 ceph-node-1 mds[5847]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:50:41 ceph-node-5 client[8256]: INFO: OSD rebalancing completed
2025-02-24 10:51:19 ceph-node-2 mds[9850]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:50:38 ceph-node-4 client[1179]: DEBUG: OSD rebalancing completed
2025-02-24 10:51:22 ceph-node-4 radosgw[6857]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:19 ceph-node-5 radosgw[3949]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:16 ceph-node-4 mon[6799]: WARNING: Monitor mon1 failed to reach quorum
2025-02-24 10:51:17 ceph-node-5 mgr[5136]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:51:16 ceph-node-4 osd[4203]: NOTICE: Monitor map has been updated
2025-02-24 10:51:26 ceph-node-3 mgr[5223]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:07 ceph-node-1 osd[6855]: WARNING: OSD 3.1 marked down due to heartbeat failure
2025-02-24 10:51:22 ceph-node-4 osd[5671]: INFO: Monitor map has been updated
2025-02-24 10:50:31 ceph-node-1 mgr[8887]: ERROR: CRITICAL: PG 2.1 stuck in inconsistent state
2025-02-24 10:50:57 ceph-node-2 radosgw[2203]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:51:15 ceph-node-1 osd[3880]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:51:09 ceph-node-4 mon[3217]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:51:17 ceph-node-2 mgr[6386]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:51:24 ceph-node-2 mds[4164]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:51:23 ceph-node-4 mon[2947]: CRITICAL: Monitor mon1 failed to reach quorum
2025-02-24 10:50:46 ceph-node-3 mds[5043]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:59 ceph-node-3 client[6728]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:28 ceph-node-4 mgr[5714]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:13 ceph-node-4 client[9635]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:44 ceph-node-2 osd[2534]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:35 ceph-node-1 radosgw[6799]: DEBUG: Monitor map has been updated
2025-02-24 10:50:57 ceph-node-2 mds[4369]: DEBUG: Monitor map has been updated
2025-02-24 10:50:37 ceph-node-1 osd[3233]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:51:09 ceph-node-3 mon[4826]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:57 ceph-node-5 osd[6280]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:51:06 ceph-node-2 client[2280]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:51:25 ceph-node-3 radosgw[8523]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:51:16 ceph-node-2 mgr[3574]: CRITICAL: Slow request detected on OSD 4.3
2025-02-24 10:50:59 ceph-node-5 mgr[4562]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:39 ceph-node-1 mgr[2372]: WARNING: High I/O latency detected in radosgw service
2025-02-24 10:51:28 ceph-node-2 mon[4706]: WARNING: Slow request detected on OSD 4.3
2025-02-24 10:51:10 ceph-node-3 mon[5911]: INFO: PG 1.2 replicated successfully
2025-02-24 10:50:48 ceph-node-5 mon[2750]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:02 ceph-node-2 osd[3263]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:31 ceph-node-5 mgr[2457]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:39 ceph-node-4 mgr[6508]: INFO: PG 1.2 replicated successfully
2025-02-24 10:51:23 ceph-node-5 radosgw[7403]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:50:39 ceph-node-4 radosgw[2453]: ERROR: Client connection timeout detected
2025-02-24 10:50:31 ceph-node-2 mds[8820]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:50:57 ceph-node-2 mgr[5600]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:51:07 ceph-node-2 radosgw[3170]: CRITICAL: Monitor mon1 failed to reach quorum
2025-02-24 10:51:29 ceph-node-4 client[2081]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:20 ceph-node-3 mds[5207]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:50:57 ceph-node-1 osd[3372]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:50:48 ceph-node-1 mds[3451]: ERROR: Data corruption detected in object pool
2025-02-24 10:51:20 ceph-node-2 client[5420]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:51:13 ceph-node-4 client[3656]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:06 ceph-node-2 osd[5939]: ERROR: Cluster status: HEALTH_WARN
2025-02-24 10:50:47 ceph-node-3 client[9342]: NOTICE: OSD rebalancing completed
2025-02-24 10:50:41 ceph-node-4 mgr[9347]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:02 ceph-node-1 mgr[5934]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:20 ceph-node-5 mon[3436]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:27 ceph-node-1 radosgw[9622]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:50:36 ceph-node-4 client[7698]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:50:53 ceph-node-2 mon[5183]: NOTICE: Monitor map has been updated
2025-02-24 10:51:29 ceph-node-5 mgr[8494]: DEBUG: OSD rebalancing completed
2025-02-24 10:50:45 ceph-node-4 mds[1792]: WARNING: Cluster status: HEALTH_WARN
2025-02-24 10:51:00 ceph-node-2 osd[5813]: INFO: PG 1.2 replicated successfully
2025-02-24 10:50:37 ceph-node-2 radosgw[2262]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:51:09 ceph-node-3 osd[4331]: WARNING: Slow request detected on OSD 4.3
2025-02-24 10:50:33 ceph-node-3 client[6978]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:12 ceph-node-1 osd[5423]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:41 ceph-node-1 radosgw[5212]: ERROR: Disk failure reported on ceph-node-4
2025-02-24 10:51:24 ceph-node-4 mon[1123]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:02 ceph-node-3 osd[6069]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:51:01 ceph-node-1 radosgw[2410]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:13 ceph-node-4 radosgw[2652]: INFO: Data replication completed for object pool
2025-02-24 10:51:18 ceph-node-4 mon[1835]: ERROR: Slow request detected on OSD 4.3
2025-02-24 10:51:26 ceph-node-1 radosgw[7702]: CRITICAL: OSD disk space utilization exceeded 90%
2025-02-24 10:50:42 ceph-node-5 mgr[8791]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:50:34 ceph-node-5 mon[8548]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:26 ceph-node-3 radosgw[3475]: INFO: Data replication completed for object pool
2025-02-24 10:50:35 ceph-node-5 mds[4695]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:10 ceph-node-3 client[9466]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:31 ceph-node-3 mgr[4009]: ERROR: Authentication failure in RADOS Gateway
2025-02-24 10:51:07 ceph-node-2 client[9400]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:10 ceph-node-5 radosgw[4546]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:50:58 ceph-node-4 mgr[8077]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:28 ceph-node-1 radosgw[2736]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:51:00 ceph-node-2 mds[2820]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:51:24 ceph-node-5 client[9017]: NOTICE: Data replication completed for object pool
2025-02-24 10:50:47 ceph-node-3 osd[6262]: DEBUG: Monitor map has been updated
2025-02-24 10:50:44 ceph-node-3 mgr[9374]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:29 ceph-node-5 radosgw[6514]: INFO: PG 1.2 replicated successfully
2025-02-24 10:51:07 ceph-node-1 mon[7420]: INFO: OSD rebalancing completed
2025-02-24 10:50:46 ceph-node-5 radosgw[5660]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:36 ceph-node-4 radosgw[6119]: INFO: PG 1.2 replicated successfully
2025-02-24 10:50:38 ceph-node-2 client[7996]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:23 ceph-node-4 mgr[6386]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:53 ceph-node-5 client[8618]: DEBUG: OSD rebalancing completed
2025-02-24 10:51:01 ceph-node-4 client[5460]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:22 ceph-node-1 client[2130]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:50:33 ceph-node-5 radosgw[8445]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:30 ceph-node-1 radosgw[5836]: NOTICE: OSD rebalancing completed
2025-02-24 10:50:55 ceph-node-4 mds[9745]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:45 ceph-node-5 mgr[4715]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:50:49 ceph-node-1 mgr[1083]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:06 ceph-node-4 osd[1967]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:32 ceph-node-2 mgr[9853]: NOTICE: OSD rebalancing completed
2025-02-24 10:50:44 ceph-node-2 radosgw[8854]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:17 ceph-node-2 mgr[8317]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:50:42 ceph-node-1 radosgw[7054]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:50:50 ceph-node-2 mon[1257]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:17 ceph-node-2 mgr[2227]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:50:48 ceph-node-2 client[5289]: ERROR: Uncorrectable ECC error detected in MDS cache
2025-02-24 10:50:34 ceph-node-2 radosgw[6920]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:20 ceph-node-3 mds[6004]: NOTICE: Data replication completed for object pool
2025-02-24 10:51:17 ceph-node-5 mds[9228]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:14 ceph-node-5 client[8103]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:13 ceph-node-2 osd[1926]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:51:17 ceph-node-1 mds[9844]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:50:43 ceph-node-1 osd[8044]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:51:31 ceph-node-2 osd[9653]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:20 ceph-node-1 mds[5546]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:50:49 ceph-node-4 mgr[6946]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:32 ceph-node-2 client[7844]: INFO: OSD rebalancing completed
2025-02-24 10:51:15 ceph-node-3 mgr[2044]: INFO: Monitor map has been updated
2025-02-24 10:50:59 ceph-node-5 mds[7605]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:08 ceph-node-1 osd[1227]: INFO: Monitor map has been updated
2025-02-24 10:51:27 ceph-node-4 radosgw[2185]: DEBUG: Monitor map has been updated
2025-02-24 10:50:46 ceph-node-3 mon[2506]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:51:04 ceph-node-3 osd[7810]: CRITICAL: Authentication failure in RADOS Gateway
2025-02-24 10:51:01 ceph-node-4 mon[4467]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:50:58 ceph-node-2 radosgw[9173]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:51:28 ceph-node-4 mgr[1534]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:50:41 ceph-node-5 mds[9431]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:50:51 ceph-node-4 radosgw[7484]: CRITICAL: CRITICAL: Data loss detected on pool 1
2025-02-24 10:51:30 ceph-node-1 client[8307]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:51:05 ceph-node-2 mds[3357]: NOTICE: Monitor map has been updated
2025-02-24 10:50:49 ceph-node-5 mgr[8719]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:26 ceph-node-2 osd[2513]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:55 ceph-node-2 mds[2743]: NOTICE: OSD rebalancing completed
2025-02-24 10:50:38 ceph-node-5 client[5948]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:56 ceph-node-1 radosgw[1965]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:50:42 ceph-node-5 mgr[4413]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:59 ceph-node-3 mgr[8465]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:50:34 ceph-node-5 osd[8226]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:51:20 ceph-node-3 mgr[6112]: INFO: PG 1.2 replicated successfully
2025-02-24 10:51:31 ceph-node-4 mds[1312]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:50:52 ceph-node-1 client[7852]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:50:54 ceph-node-5 radosgw[4760]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:50:55 ceph-node-5 mon[4208]: NOTICE: Data replication completed for object pool
2025-02-24 10:50:33 ceph-node-4 mds[2114]: INFO: OSD rebalancing completed
2025-02-24 10:51:16 ceph-node-3 client[7405]: WARNING: Client connection timeout detected
2025-02-24 10:50:47 ceph-node-1 mgr[7619]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:50:58 ceph-node-4 radosgw[1210]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:50:35 ceph-node-5 mds[1205]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:51:09 ceph-node-5 client[2352]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:51:20 ceph-node-5 mds[7451]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:07 ceph-node-2 mon[8916]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:16 ceph-node-3 mds[1045]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:05 ceph-node-1 osd[5191]: INFO: Monitor map has been updated
2025-02-24 10:51:20 ceph-node-5 radosgw[1616]: CRITICAL: OSD disk space utilization exceeded 90%
2025-02-24 10:51:17 ceph-node-5 radosgw[5047]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:20 ceph-node-4 osd[7990]: NOTICE: OSD rebalancing completed
2025-02-24 10:50:36 ceph-node-1 osd[8004]: INFO: OSD disk space utilization: 70%
2025-02-24 10:51:22 ceph-node-5 mds[6495]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:50:42 ceph-node-5 client[5870]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:16 ceph-node-3 mds[5865]: INFO: Data replication completed for object pool
2025-02-24 10:51:14 ceph-node-4 mon[7388]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:45 ceph-node-3 mgr[1851]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:31 ceph-node-4 mgr[4621]: WARNING: Data corruption detected in object pool
2025-02-24 10:51:05 ceph-node-5 mon[1778]: INFO: MGR module loaded successfully
2025-02-24 10:50:58 ceph-node-3 radosgw[8581]: INFO: OSD disk space utilization: 70%
2025-02-24 10:51:15 ceph-node-3 mon[9178]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:51:12 ceph-node-3 mds[7519]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:07 ceph-node-5 mgr[3454]: ERROR: Monitor down, unable to connect to quorum
2025-02-24 10:51:15 ceph-node-2 radosgw[3967]: INFO: PG 1.2 replicated successfully
2025-02-24 10:51:20 ceph-node-5 mds[1564]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:24 ceph-node-5 mds[8308]: INFO: OSD disk space utilization: 70%
2025-02-24 10:50:45 ceph-node-5 radosgw[5306]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:27 ceph-node-1 osd[7386]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:57 ceph-node-5 osd[7309]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:51:25 ceph-node-1 mgr[7805]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:50:35 ceph-node-5 radosgw[1374]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:07 ceph-node-2 mds[8843]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:20 ceph-node-5 mgr[8664]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:51:30 ceph-node-4 client[4249]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:23 ceph-node-1 mgr[1046]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:50 ceph-node-4 mgr[4667]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:40 ceph-node-2 client[8756]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:51:07 ceph-node-4 mon[7985]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:51:02 ceph-node-1 mds[2210]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:50:55 ceph-node-2 mgr[4260]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:47 ceph-node-5 osd[7747]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:50:42 ceph-node-1 radosgw[2158]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:50:38 ceph-node-5 osd[1257]: NOTICE: OSD rebalancing completed
2025-02-24 10:50:46 ceph-node-5 mds[5503]: ERROR: High I/O latency detected in radosgw service
2025-02-24 10:50:48 ceph-node-5 osd[9812]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:50:58 ceph-node-3 client[7243]: CRITICAL: OSD disk space utilization exceeded 90%
2025-02-24 10:51:11 ceph-node-4 client[5832]: ERROR: CRITICAL: PG 2.1 stuck in inconsistent state
2025-02-24 10:50:49 ceph-node-4 mds[4027]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:40 ceph-node-4 osd[4270]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:05 ceph-node-4 osd[3677]: INFO: PG 1.2 replicated successfully
2025-02-24 10:51:09 ceph-node-3 client[7883]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:51:03 ceph-node-4 mds[8555]: WARNING: Client connection timeout detected
2025-02-24 10:51:08 ceph-node-4 mon[5155]: ERROR: CRITICAL: Data loss detected on pool 1
2025-02-24 10:50:42 ceph-node-3 radosgw[7517]: CRITICAL: Monitor mon1 failed to reach quorum
2025-02-24 10:51:10 ceph-node-4 mgr[2584]: INFO: MGR module loaded successfully
2025-02-24 10:50:46 ceph-node-4 client[1174]: INFO: OSD disk space utilization: 70%
2025-02-24 10:51:20 ceph-node-3 osd[6698]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:37 ceph-node-4 mgr[5374]: ERROR: Journal write failure detected on OSD node
2025-02-24 10:50:50 ceph-node-1 mon[1672]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:16 ceph-node-2 mon[3752]: DEBUG: Monitor map has been updated
2025-02-24 10:51:29 ceph-node-3 radosgw[5346]: INFO: Monitor map has been updated
2025-02-24 10:51:13 ceph-node-3 mon[1607]: ERROR: OSD disk space utilization exceeded 90%
2025-02-24 10:50:39 ceph-node-4 mon[2493]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:46 ceph-node-1 osd[5911]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:53 ceph-node-1 osd[9821]: DEBUG: OSD rebalancing completed
2025-02-24 10:50:37 ceph-node-1 mon[2948]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:03 ceph-node-3 mgr[8986]: DEBUG: OSD rebalancing completed
2025-02-24 10:50:53 ceph-node-2 mds[6189]: DEBUG: OSD rebalancing completed
2025-02-24 10:50:53 ceph-node-5 osd[6248]: WARNING: Monitor down, unable to connect to quorum
2025-02-24 10:51:31 ceph-node-5 mon[2598]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:21 ceph-node-5 osd[2809]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:21 ceph-node-5 mgr[8167]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:50:55 ceph-node-3 radosgw[6085]: INFO: OSD rebalancing completed
2025-02-24 10:51:21 ceph-node-4 mds[5428]: INFO: PG 1.2 replicated successfully
2025-02-24 10:50:57 ceph-node-4 mgr[7801]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:50:56 ceph-node-4 client[6773]: WARNING: Journal write failure detected on OSD node
2025-02-24 10:51:02 ceph-node-3 mgr[8949]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:43 ceph-node-4 mds[6601]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:51:21 ceph-node-4 mon[7913]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:23 ceph-node-5 mon[5845]: ERROR: CRITICAL: PG 2.1 stuck in inconsistent state
2025-02-24 10:50:48 ceph-node-3 mgr[4523]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:05 ceph-node-4 mds[8660]: NOTICE: Monitor map has been updated
2025-02-24 10:51:02 ceph-node-2 mon[2579]: CRITICAL: Authentication failure in RADOS Gateway
2025-02-24 10:51:13 ceph-node-2 osd[9618]: INFO: Monitor map has been updated
2025-02-24 10:51:05 ceph-node-3 mds[3677]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:35 ceph-node-4 mds[1054]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:08 ceph-node-5 mds[7155]: DEBUG: MGR module loaded successfully
2025-02-24 10:50:41 ceph-node-5 mon[7638]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:50:33 ceph-node-2 osd[8983]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:50 ceph-node-2 client[6573]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:50:34 ceph-node-3 mgr[7547]: INFO: OSD rebalancing completed
2025-02-24 10:51:28 ceph-node-2 radosgw[4728]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:50:58 ceph-node-1 mgr[3910]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:06 ceph-node-4 osd[3715]: NOTICE: OSD rebalancing completed
2025-02-24 10:50:59 ceph-node-2 mds[9757]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:26 ceph-node-4 osd[1880]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:14 ceph-node-2 client[6863]: DEBUG: Monitor map has been updated
2025-02-24 10:50:53 ceph-node-4 radosgw[4221]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:45 ceph-node-4 client[5886]: NOTICE: Data replication completed for object pool
2025-02-24 10:50:44 ceph-node-5 osd[7797]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:51:25 ceph-node-3 mon[9872]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:41 ceph-node-1 mds[9293]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:08 ceph-node-2 client[6958]: DEBUG: Monitor map has been updated
2025-02-24 10:50:51 ceph-node-5 client[6744]: DEBUG: OSD rebalancing completed
2025-02-24 10:51:15 ceph-node-1 client[8850]: NOTICE: Monitor map has been updated
2025-02-24 10:50:42 ceph-node-3 mon[3686]: INFO: Monitor map has been updated
2025-02-24 10:50:53 ceph-node-1 mon[6454]: ERROR: Journal write failure detected on OSD node
2025-02-24 10:51:25 ceph-node-4 radosgw[7337]: INFO: OSD rebalancing completed
2025-02-24 10:51:28 ceph-node-3 client[5009]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:19 ceph-node-5 client[3336]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:57 ceph-node-4 osd[3485]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:50:38 ceph-node-3 mgr[7295]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:50:32 ceph-node-1 mon[7240]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:06 ceph-node-1 mon[8225]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:38 ceph-node-1 client[2153]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:51:27 ceph-node-1 radosgw[8949]: INFO: PG 1.2 replicated successfully
2025-02-24 10:50:46 ceph-node-5 mds[6589]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:31 ceph-node-3 mds[2048]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:45 ceph-node-3 mon[1940]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:50:40 ceph-node-3 mon[5381]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:51:28 ceph-node-4 client[4177]: INFO: PG 1.2 replicated successfully
2025-02-24 10:51:30 ceph-node-5 osd[4404]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:50:58 ceph-node-5 client[9138]: NOTICE: MGR module loaded successfully
2025-02-24 10:51:17 ceph-node-2 radosgw[6665]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:50:32 ceph-node-2 radosgw[3796]: INFO: OSD disk space utilization: 70%
2025-02-24 10:51:21 ceph-node-3 mgr[4414]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:50:51 ceph-node-1 client[8760]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:50:59 ceph-node-1 mon[9880]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:50:41 ceph-node-4 mon[6493]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:50:42 ceph-node-5 mon[4188]: NOTICE: Data replication completed for object pool
2025-02-24 10:50:44 ceph-node-2 client[5358]: CRITICAL: Client connection timeout detected
2025-02-24 10:50:43 ceph-node-2 mgr[8067]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:50:58 ceph-node-5 mds[2149]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:51:22 ceph-node-4 osd[3256]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:15 ceph-node-3 mon[1433]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:50:38 ceph-node-4 radosgw[6384]: CRITICAL: Monitor mon1 failed to reach quorum
2025-02-24 10:50:46 ceph-node-4 mon[8079]: DEBUG: MGR module loaded successfully
2025-02-24 10:50:54 ceph-node-3 client[1380]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:50:55 ceph-node-1 osd[5797]: INFO: MGR module loaded successfully
2025-02-24 10:50:38 ceph-node-4 mon[1450]: DEBUG: Monitor map has been updated
2025-02-24 10:50:41 ceph-node-1 mds[3692]: ERROR: Slow request detected on OSD 4.3
2025-02-24 10:51:09 ceph-node-2 client[4379]: ERROR: Data corruption detected in object pool
2025-02-24 10:51:14 ceph-node-4 mgr[9874]: CRITICAL: OSD 3.1 marked down due to heartbeat failure
2025-02-24 10:51:15 ceph-node-5 mon[8542]: INFO: PG 1.2 replicated successfully
2025-02-24 10:51:09 ceph-node-1 client[2647]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:51:30 ceph-node-2 mds[3275]: DEBUG: OSD rebalancing completed
2025-02-24 10:51:30 ceph-node-1 mgr[1922]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:37 ceph-node-1 radosgw[8296]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:50:52 ceph-node-2 osd[6233]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:00 ceph-node-5 mds[2679]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:51:20 ceph-node-5 mds[4744]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:14 ceph-node-5 mgr[9233]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:17 ceph-node-4 client[5111]: ERROR: CRITICAL: Data loss detected on pool 1
2025-02-24 10:51:23 ceph-node-1 mgr[9930]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:50:40 ceph-node-5 radosgw[1614]: INFO: MGR module loaded successfully
2025-02-24 10:50:36 ceph-node-1 client[9096]: WARNING: High I/O latency detected in radosgw service
2025-02-24 10:51:12 ceph-node-5 mon[7118]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:50:39 ceph-node-4 osd[3794]: INFO: OSD disk space utilization: 70%
2025-02-24 10:50:46 ceph-node-5 osd[6978]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:15 ceph-node-3 mds[3406]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:38 ceph-node-3 mds[1172]: INFO: OSD rebalancing completed
2025-02-24 10:50:55 ceph-node-5 mds[9959]: DEBUG: OSD rebalancing completed
2025-02-24 10:50:51 ceph-node-3 client[2163]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:08 ceph-node-4 osd[8408]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:17 ceph-node-5 osd[1103]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:45 ceph-node-3 mgr[9809]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:59 ceph-node-2 mds[8575]: NOTICE: Monitor map has been updated
2025-02-24 10:51:27 ceph-node-5 radosgw[2760]: CRITICAL: Authentication failure in RADOS Gateway
2025-02-24 10:51:07 ceph-node-3 mds[6410]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:51:25 ceph-node-2 radosgw[9829]: NOTICE: Data replication completed for object pool
2025-02-24 10:50:36 ceph-node-5 mon[8313]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:27 ceph-node-3 radosgw[2960]: WARNING: Monitor mon1 failed to reach quorum
2025-02-24 10:50:53 ceph-node-5 radosgw[1316]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:01 ceph-node-2 osd[5449]: NOTICE: MGR module loaded successfully
2025-02-24 10:50:48 ceph-node-2 radosgw[5534]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:50:42 ceph-node-2 osd[2964]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:51:24 ceph-node-4 client[8487]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:39 ceph-node-4 mon[2478]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:33 ceph-node-5 client[2424]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:50:35 ceph-node-1 client[2061]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:50:37 ceph-node-3 client[2767]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:51:17 ceph-node-4 mds[6993]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:51:10 ceph-node-3 mgr[2879]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:51:08 ceph-node-5 mgr[3115]: NOTICE: Monitor map has been updated
2025-02-24 10:51:14 ceph-node-4 mon[2187]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:31 ceph-node-4 osd[5869]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:50:34 ceph-node-5 mon[5236]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:01 ceph-node-4 client[2156]: WARNING: CRITICAL: PG 2.1 stuck in inconsistent state
2025-02-24 10:51:10 ceph-node-3 client[1078]: NOTICE: Data replication completed for object pool
2025-02-24 10:50:37 ceph-node-2 radosgw[4292]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:11 ceph-node-5 radosgw[5883]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:28 ceph-node-3 osd[2676]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:24 ceph-node-3 radosgw[5018]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:50:43 ceph-node-2 mds[2657]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:56 ceph-node-3 mon[3759]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:27 ceph-node-4 radosgw[9810]: WARNING: Authentication failure in RADOS Gateway
2025-02-24 10:51:01 ceph-node-5 mgr[8700]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:46 ceph-node-3 mgr[7553]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:50:41 ceph-node-1 osd[1699]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:40 ceph-node-1 mon[4151]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:39 ceph-node-4 radosgw[3015]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:53 ceph-node-5 mds[7760]: NOTICE: OSD rebalancing completed
2025-02-24 10:50:52 ceph-node-2 osd[1583]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:51:16 ceph-node-4 mon[4495]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:51:01 ceph-node-5 mds[2761]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:49 ceph-node-2 client[3938]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:50:43 ceph-node-1 radosgw[5015]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:24 ceph-node-4 mgr[3093]: INFO: Monitor map has been updated
2025-02-24 10:50:52 ceph-node-2 osd[7544]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:08 ceph-node-3 osd[5791]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:31 ceph-node-3 mgr[4819]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:55 ceph-node-1 mgr[5402]: ERROR: Uncorrectable ECC error detected in MDS cache
2025-02-24 10:50:57 ceph-node-2 radosgw[6022]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:04 ceph-node-5 client[2023]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:02 ceph-node-5 mds[8319]: WARNING: CRITICAL: Data loss detected on pool 1
2025-02-24 10:51:05 ceph-node-1 mds[2174]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:30 ceph-node-1 mds[9538]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:51:07 ceph-node-1 radosgw[5353]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:33 ceph-node-3 radosgw[8626]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:50:53 ceph-node-4 mds[4956]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:31 ceph-node-4 mds[3218]: NOTICE: Data replication completed for object pool
2025-02-24 10:51:31 ceph-node-5 radosgw[3618]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:51:09 ceph-node-4 mon[5102]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:51:30 ceph-node-1 radosgw[1437]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:11 ceph-node-3 osd[3361]: CRITICAL: Journal write failure detected on OSD node
2025-02-24 10:51:23 ceph-node-5 client[3028]: INFO: OSD disk space utilization: 70%
2025-02-24 10:51:06 ceph-node-3 client[8858]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:48 ceph-node-2 mon[4845]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:50:53 ceph-node-3 osd[9848]: CRITICAL: MDS failure detected: metadata inconsistency
2025-02-24 10:50:32 ceph-node-1 osd[3395]: CRITICAL: Cluster status: HEALTH_WARN
2025-02-24 10:51:02 ceph-node-2 osd[1116]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:03 ceph-node-1 mgr[2018]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:30 ceph-node-2 client[2211]: CRITICAL: CRITICAL: Data loss detected on pool 1
2025-02-24 10:50:40 ceph-node-5 mgr[2625]: NOTICE: MGR module loaded successfully
2025-02-24 10:50:45 ceph-node-5 client[7906]: NOTICE: Monitor map has been updated
2025-02-24 10:50:49 ceph-node-3 mon[5628]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:51:14 ceph-node-5 client[9715]: WARNING: Monitor mon1 failed to reach quorum
2025-02-24 10:50:33 ceph-node-1 mon[7639]: INFO: PG 1.2 replicated successfully
2025-02-24 10:51:19 ceph-node-4 mds[8711]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:51:09 ceph-node-1 mon[7682]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:51:21 ceph-node-1 mgr[8699]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:51:29 ceph-node-5 osd[3011]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:19 ceph-node-1 mon[6392]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:51:01 ceph-node-3 client[3120]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:50:54 ceph-node-3 radosgw[3298]: DEBUG: OSD rebalancing completed
2025-02-24 10:51:30 ceph-node-4 osd[6184]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:50:42 ceph-node-4 mgr[3220]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:50:41 ceph-node-1 client[3860]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:51:08 ceph-node-5 radosgw[4849]: NOTICE: Data replication completed for object pool
2025-02-24 10:50:57 ceph-node-4 mon[2214]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:08 ceph-node-1 osd[8664]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:37 ceph-node-5 osd[3851]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:23 ceph-node-2 radosgw[6799]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:01 ceph-node-5 mgr[3834]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:51:25 ceph-node-2 mon[4956]: NOTICE: Data replication completed for object pool
2025-02-24 10:51:04 ceph-node-4 client[9634]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:51:17 ceph-node-2 radosgw[7989]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:19 ceph-node-2 radosgw[2527]: INFO: Data replication completed for object pool
2025-02-24 10:50:41 ceph-node-2 mds[5603]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:04 ceph-node-1 mds[9918]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:51:03 ceph-node-2 client[3943]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:50:57 ceph-node-2 mon[7280]: DEBUG: Monitor map has been updated
2025-02-24 10:51:03 ceph-node-2 mds[2694]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:51:19 ceph-node-3 mgr[5324]: NOTICE: Data replication completed for object pool
2025-02-24 10:50:40 ceph-node-5 mds[9362]: ERROR: MDS failure detected: metadata inconsistency
2025-02-24 10:51:00 ceph-node-5 mgr[7758]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:25 ceph-node-2 radosgw[2018]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:13 ceph-node-2 mon[1600]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:51:12 ceph-node-2 mon[5572]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:30 ceph-node-3 radosgw[4141]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:51:26 ceph-node-3 mon[6634]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:57 ceph-node-5 radosgw[3426]: NOTICE: Monitor map has been updated
2025-02-24 10:51:09 ceph-node-2 radosgw[1892]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:50:32 ceph-node-1 mgr[1034]: INFO: Data replication completed for object pool
2025-02-24 10:50:46 ceph-node-5 osd[4165]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:51:16 ceph-node-3 mon[5190]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:51:30 ceph-node-1 radosgw[2890]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:50:54 ceph-node-5 client[7126]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:05 ceph-node-5 client[2453]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:44 ceph-node-5 osd[1060]: NOTICE: Data replication completed for object pool
2025-02-24 10:51:19 ceph-node-5 radosgw[2904]: WARNING: Data corruption detected in object pool
2025-02-24 10:51:00 ceph-node-1 mgr[7519]: NOTICE: Data replication completed for object pool
2025-02-24 10:51:26 ceph-node-1 osd[2734]: NOTICE: Monitor map has been updated
2025-02-24 10:51:11 ceph-node-5 mon[9129]: INFO: Data replication completed for object pool
2025-02-24 10:51:07 ceph-node-4 mds[6627]: INFO: Data replication completed for object pool
2025-02-24 10:50:46 ceph-node-3 client[6915]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:44 ceph-node-3 mds[3440]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:50:43 ceph-node-4 mon[2902]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:18 ceph-node-3 radosgw[1432]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:50:57 ceph-node-3 radosgw[5420]: INFO: Data replication completed for object pool
2025-02-24 10:50:40 ceph-node-3 client[5287]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:17 ceph-node-3 mon[3539]: INFO: OSD disk space utilization: 70%
2025-02-24 10:51:08 ceph-node-2 radosgw[2198]: NOTICE: MGR module loaded successfully
2025-02-24 10:51:08 ceph-node-1 client[1246]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:02 ceph-node-5 mgr[5851]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:51:15 ceph-node-5 mon[3001]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:31 ceph-node-4 radosgw[5905]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:05 ceph-node-3 osd[9436]: INFO: OSD rebalancing completed
2025-02-24 10:51:22 ceph-node-5 client[7571]: INFO: OSD disk space utilization: 70%
2025-02-24 10:51:10 ceph-node-5 radosgw[1568]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:27 ceph-node-5 radosgw[1799]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:13 ceph-node-5 mon[9822]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:51:24 ceph-node-4 mon[4181]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:50:50 ceph-node-5 osd[6341]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:50:44 ceph-node-4 radosgw[7964]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:51:24 ceph-node-4 osd[4500]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:45 ceph-node-1 osd[2260]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:33 ceph-node-4 osd[6517]: INFO: PG 1.2 replicated successfully
2025-02-24 10:51:12 ceph-node-3 mds[9848]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:27 ceph-node-4 radosgw[4758]: DEBUG: Monitor map has been updated
2025-02-24 10:50:31 ceph-node-3 osd[3891]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:05 ceph-node-3 radosgw[1905]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:51:04 ceph-node-5 mon[1685]: NOTICE: Data replication completed for object pool
2025-02-24 10:51:25 ceph-node-3 client[1292]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:43 ceph-node-2 mds[2382]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:48 ceph-node-4 osd[6248]: DEBUG: MGR module loaded successfully
2025-02-24 10:50:35 ceph-node-2 client[3370]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:53 ceph-node-4 mgr[6677]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:50:53 ceph-node-1 client[9252]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:44 ceph-node-3 mgr[3931]: CRITICAL: CRITICAL: PG 2.1 stuck in inconsistent state
2025-02-24 10:50:42 ceph-node-3 mds[9327]: DEBUG: OSD rebalancing completed
2025-02-24 10:50:46 ceph-node-4 client[7870]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:01 ceph-node-5 radosgw[5138]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:29 ceph-node-2 osd[3306]: WARNING: Slow request detected on OSD 4.3
2025-02-24 10:50:39 ceph-node-5 mgr[5878]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:50:36 ceph-node-1 client[3809]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:50:38 ceph-node-2 mds[1478]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:50:31 ceph-node-4 mgr[3063]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:50:51 ceph-node-1 mon[7141]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:09 ceph-node-2 osd[7542]: DEBUG: Monitor map has been updated
2025-02-24 10:51:25 ceph-node-2 client[6847]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:51:22 ceph-node-4 osd[5099]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:50:55 ceph-node-1 mds[7824]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:30 ceph-node-5 radosgw[1559]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:51:10 ceph-node-3 osd[8598]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:50:44 ceph-node-3 mds[9605]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:51:07 ceph-node-4 radosgw[6827]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:53 ceph-node-3 mon[7542]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:51:23 ceph-node-3 mds[2835]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:50:53 ceph-node-2 radosgw[6334]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:51:21 ceph-node-3 mgr[5404]: INFO: OSD rebalancing completed
2025-02-24 10:50:40 ceph-node-2 osd[1549]: NOTICE: Data replication completed for object pool
2025-02-24 10:50:54 ceph-node-2 radosgw[8665]: NOTICE: Data replication completed for object pool
2025-02-24 10:51:11 ceph-node-1 mds[2428]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:22 ceph-node-2 osd[8274]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:51:30 ceph-node-3 mds[8976]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:23 ceph-node-4 client[5153]: INFO: Monitor map has been updated
2025-02-24 10:50:37 ceph-node-1 osd[7643]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:00 ceph-node-1 mds[8867]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:50:54 ceph-node-3 mon[4135]: ERROR: Network partition detected between OSD nodes
2025-02-24 10:50:54 ceph-node-4 osd[9468]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:59 ceph-node-1 client[1056]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:17 ceph-node-3 osd[9961]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:30 ceph-node-2 radosgw[4066]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:02 ceph-node-4 mon[7730]: INFO: Data replication completed for object pool
2025-02-24 10:50:38 ceph-node-2 mgr[2793]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:50:54 ceph-node-5 mgr[3962]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:51:30 ceph-node-5 radosgw[1588]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:04 ceph-node-2 mon[1876]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:50:38 ceph-node-3 client[8106]: DEBUG: OSD rebalancing completed
2025-02-24 10:51:05 ceph-node-1 mgr[1517]: NOTICE: MGR module loaded successfully
2025-02-24 10:51:23 ceph-node-3 mds[1941]: INFO: OSD disk space utilization: 70%
2025-02-24 10:51:25 ceph-node-5 osd[3817]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:12 ceph-node-5 mds[9156]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:50:39 ceph-node-1 radosgw[4989]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:51:28 ceph-node-2 mon[6912]: WARNING: Slow request detected on OSD 4.3
2025-02-24 10:50:43 ceph-node-3 mgr[8102]: NOTICE: Monitor map has been updated
2025-02-24 10:50:52 ceph-node-1 mds[3941]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:49 ceph-node-2 mgr[7859]: WARNING: MDS failure detected: metadata inconsistency
2025-02-24 10:51:24 ceph-node-4 mds[6157]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:51:14 ceph-node-5 osd[2594]: CRITICAL: Uncorrectable ECC error detected in MDS cache
2025-02-24 10:51:04 ceph-node-2 mon[7331]: NOTICE: MGR module loaded successfully
2025-02-24 10:50:55 ceph-node-1 mon[2477]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:12 ceph-node-2 radosgw[7371]: NOTICE: MGR module loaded successfully
2025-02-24 10:51:03 ceph-node-5 mon[6961]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:34 ceph-node-1 mds[1298]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:50:43 ceph-node-2 mgr[1744]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:48 ceph-node-2 mgr[3901]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:19 ceph-node-4 mgr[3022]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:23 ceph-node-4 mds[4354]: INFO: PG 1.2 replicated successfully
2025-02-24 10:51:24 ceph-node-1 mon[3468]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:30 ceph-node-5 osd[6659]: NOTICE: MGR module loaded successfully
2025-02-24 10:50:50 ceph-node-4 client[6695]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:08 ceph-node-4 mgr[7765]: DEBUG: MGR module loaded successfully
2025-02-24 10:50:32 ceph-node-5 client[6694]: NOTICE: OSD rebalancing completed
2025-02-24 10:50:57 ceph-node-3 osd[8197]: INFO: PG 1.2 replicated successfully
2025-02-24 10:51:17 ceph-node-2 mds[2005]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:03 ceph-node-2 mgr[2895]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:19 ceph-node-4 mgr[9035]: INFO: PG 1.2 replicated successfully
2025-02-24 10:51:31 ceph-node-2 client[4826]: INFO: OSD disk space utilization: 70%
2025-02-24 10:50:50 ceph-node-2 client[9585]: NOTICE: OSD rebalancing completed
2025-02-24 10:51:23 ceph-node-1 osd[5968]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:14 ceph-node-3 mon[6702]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:52 ceph-node-2 client[6430]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:13 ceph-node-2 osd[6213]: CRITICAL: CRITICAL: PG 2.1 stuck in inconsistent state
2025-02-24 10:51:06 ceph-node-1 client[4254]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:57 ceph-node-5 osd[3404]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:06 ceph-node-3 mds[9638]: NOTICE: MGR module loaded successfully
2025-02-24 10:50:50 ceph-node-4 osd[3150]: NOTICE: Data replication completed for object pool
2025-02-24 10:50:45 ceph-node-5 radosgw[2006]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:50:48 ceph-node-4 radosgw[3663]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:05 ceph-node-2 mon[2377]: DEBUG: OSD rebalancing completed
2025-02-24 10:50:40 ceph-node-3 mon[9034]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:52 ceph-node-4 mon[5772]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:19 ceph-node-4 client[4307]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:38 ceph-node-4 mon[1100]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:40 ceph-node-1 mgr[3939]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:04 ceph-node-2 osd[1985]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:06 ceph-node-3 mon[1060]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:38 ceph-node-5 osd[5597]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:51:26 ceph-node-1 mon[9986]: INFO: PG 1.2 replicated successfully
2025-02-24 10:51:19 ceph-node-3 mgr[2426]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:51:05 ceph-node-1 mgr[7942]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:50:40 ceph-node-3 mon[2888]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:51:20 ceph-node-4 mon[9251]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:51:02 ceph-node-3 client[1698]: INFO: MGR module loaded successfully
2025-02-24 10:51:17 ceph-node-2 client[3144]: ERROR: Monitor mon1 failed to reach quorum
2025-02-24 10:51:16 ceph-node-1 radosgw[4042]: DEBUG: OSD rebalancing completed
2025-02-24 10:51:12 ceph-node-5 radosgw[8071]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:36 ceph-node-1 mds[3066]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:50:35 ceph-node-3 mds[7981]: INFO: Data replication completed for object pool
2025-02-24 10:50:43 ceph-node-1 radosgw[2656]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:06 ceph-node-3 radosgw[4123]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:51:04 ceph-node-3 radosgw[4581]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:51:08 ceph-node-5 radosgw[1652]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:13 ceph-node-4 client[1499]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:37 ceph-node-2 osd[4428]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:01 ceph-node-2 osd[5669]: NOTICE: Monitor map has been updated
2025-02-24 10:51:27 ceph-node-4 mgr[9567]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:50:41 ceph-node-3 mon[7633]: CRITICAL: Disk failure reported on ceph-node-4
2025-02-24 10:51:13 ceph-node-2 mds[1026]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:51:06 ceph-node-2 osd[3895]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:51:17 ceph-node-1 radosgw[1300]: INFO: MGR module loaded successfully
2025-02-24 10:50:47 ceph-node-1 osd[8044]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:51:03 ceph-node-3 mds[4048]: NOTICE: OSD rebalancing completed
2025-02-24 10:50:34 ceph-node-4 radosgw[9641]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:51:16 ceph-node-1 mgr[5508]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:08 ceph-node-3 mds[5460]: INFO: PG 1.2 replicated successfully
2025-02-24 10:50:37 ceph-node-2 mon[4663]: INFO: OSD rebalancing completed
2025-02-24 10:51:10 ceph-node-1 osd[5981]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:50 ceph-node-3 radosgw[3019]: WARNING: OSD 3.1 marked down due to heartbeat failure
2025-02-24 10:51:30 ceph-node-3 radosgw[9114]: ERROR: Slow request detected on OSD 4.3
2025-02-24 10:50:34 ceph-node-2 mds[4270]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:52 ceph-node-5 osd[1274]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:57 ceph-node-2 mds[5657]: INFO: Data replication completed for object pool
2025-02-24 10:50:37 ceph-node-1 mds[6928]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:32 ceph-node-4 mgr[7422]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:50:34 ceph-node-4 radosgw[7071]: NOTICE: OSD rebalancing completed
2025-02-24 10:51:16 ceph-node-1 radosgw[3615]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:19 ceph-node-1 client[9785]: ERROR: CRITICAL: PG 2.1 stuck in inconsistent state
2025-02-24 10:50:50 ceph-node-2 mgr[3820]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:22 ceph-node-4 mgr[1310]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:09 ceph-node-3 mds[2401]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:16 ceph-node-3 mon[6478]: NOTICE: OSD rebalancing completed
2025-02-24 10:51:22 ceph-node-5 radosgw[6151]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:50:32 ceph-node-4 radosgw[6338]: NOTICE: Data replication completed for object pool
2025-02-24 10:50:32 ceph-node-5 client[1228]: NOTICE: OSD rebalancing completed
2025-02-24 10:50:56 ceph-node-4 mgr[5905]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:18 ceph-node-3 radosgw[7512]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:17 ceph-node-4 mds[1524]: INFO: OSD disk space utilization: 70%
2025-02-24 10:50:35 ceph-node-3 mon[6770]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:50:53 ceph-node-4 mgr[3891]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:51:28 ceph-node-3 mgr[2069]: DEBUG: OSD rebalancing completed
2025-02-24 10:51:30 ceph-node-1 mgr[1129]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:41 ceph-node-3 radosgw[3125]: DEBUG: OSD rebalancing completed
2025-02-24 10:51:17 ceph-node-4 osd[8660]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:50:38 ceph-node-3 mds[1331]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:50:34 ceph-node-5 mds[9938]: DEBUG: Monitor map has been updated
2025-02-24 10:50:54 ceph-node-1 osd[9905]: NOTICE: MGR module loaded successfully
2025-02-24 10:51:26 ceph-node-2 mds[1653]: INFO: Monitor map has been updated
2025-02-24 10:50:58 ceph-node-4 radosgw[5815]: NOTICE: MGR module loaded successfully
2025-02-24 10:51:02 ceph-node-4 client[7730]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:50:48 ceph-node-1 mon[6884]: NOTICE: OSD rebalancing completed
2025-02-24 10:50:37 ceph-node-5 client[8809]: DEBUG: Data replication completed for object pool
2025-02-24 10:50:44 ceph-node-2 client[5505]: WARNING: CRITICAL: Data loss detected on pool 1
2025-02-24 10:50:56 ceph-node-5 radosgw[3859]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:50:33 ceph-node-1 osd[7146]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:39 ceph-node-3 mgr[4318]: DEBUG: OSD rebalancing completed
2025-02-24 10:51:30 ceph-node-2 client[9267]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:51:26 ceph-node-2 mgr[9703]: DEBUG: MGR module loaded successfully
2025-02-24 10:50:57 ceph-node-5 client[5388]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:50:35 ceph-node-1 mds[9077]: INFO: Data replication completed for object pool
2025-02-24 10:50:41 ceph-node-1 osd[2771]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:51:08 ceph-node-1 mgr[9402]: WARNING: OSD 3.1 marked down due to heartbeat failure
2025-02-24 10:50:37 ceph-node-3 radosgw[7993]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:00 ceph-node-3 radosgw[9948]: DEBUG: OSD rebalancing completed
2025-02-24 10:51:03 ceph-node-2 mon[3394]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:55 ceph-node-5 radosgw[2360]: ERROR: Uncorrectable ECC error detected in MDS cache
2025-02-24 10:51:21 ceph-node-1 mon[3820]: CRITICAL: High I/O latency detected in radosgw service
2025-02-24 10:51:20 ceph-node-1 osd[1808]: ERROR: Disk failure reported on ceph-node-4
2025-02-24 10:51:28 ceph-node-3 mds[6822]: ERROR: OSD disk space utilization exceeded 90%
2025-02-24 10:50:40 ceph-node-4 radosgw[8541]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:52 ceph-node-1 osd[7196]: INFO: OSD rebalancing completed
2025-02-24 10:51:29 ceph-node-2 radosgw[7444]: INFO: Data replication completed for object pool
2025-02-24 10:50:41 ceph-node-2 mgr[6440]: DEBUG: MGR module loaded successfully
2025-02-24 10:50:44 ceph-node-2 mds[5523]: INFO: Monitor map has been updated
2025-02-24 10:50:56 ceph-node-2 radosgw[5500]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:06 ceph-node-2 client[6717]: NOTICE: Data replication completed for object pool
2025-02-24 10:51:16 ceph-node-3 radosgw[8156]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:51:20 ceph-node-3 osd[5513]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:50:44 ceph-node-1 osd[8110]: NOTICE: Data replication completed for object pool
2025-02-24 10:50:52 ceph-node-4 mon[2310]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:50:43 ceph-node-1 radosgw[3919]: ERROR: High I/O latency detected in radosgw service
2025-02-24 10:51:03 ceph-node-5 radosgw[5349]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:50 ceph-node-4 mgr[2312]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:27 ceph-node-1 radosgw[9602]: WARNING: MDS failure detected: metadata inconsistency
2025-02-24 10:51:11 ceph-node-5 client[5704]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:04 ceph-node-2 client[4221]: INFO: Monitor map has been updated
2025-02-24 10:51:02 ceph-node-4 mds[4050]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:50:36 ceph-node-4 radosgw[5607]: NOTICE: MGR module loaded successfully
2025-02-24 10:50:55 ceph-node-1 mds[5659]: WARNING: Disk failure reported on ceph-node-4
2025-02-24 10:50:52 ceph-node-2 mgr[3986]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:17 ceph-node-5 mgr[9384]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:50:37 ceph-node-1 mon[7086]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:00 ceph-node-3 mds[6334]: INFO: OSD disk space utilization: 70%
2025-02-24 10:51:00 ceph-node-3 osd[2996]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:51:07 ceph-node-5 mds[7160]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:54 ceph-node-3 mds[3379]: NOTICE: Data replication completed for object pool
2025-02-24 10:51:21 ceph-node-2 osd[5744]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:51:23 ceph-node-4 client[1594]: CRITICAL: Client connection timeout detected
2025-02-24 10:51:00 ceph-node-4 mds[5277]: INFO: OSD rebalancing completed
2025-02-24 10:50:43 ceph-node-1 mon[2338]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:44 ceph-node-5 client[2923]: ERROR: OSD 3.1 marked down due to heartbeat failure
2025-02-24 10:50:38 ceph-node-3 osd[8943]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:39 ceph-node-4 osd[6594]: INFO: MGR module loaded successfully
2025-02-24 10:51:26 ceph-node-5 mgr[4879]: INFO: PG 1.2 replicated successfully
2025-02-24 10:50:50 ceph-node-1 mon[7634]: DEBUG: Data replication completed for object pool
2025-02-24 10:50:31 ceph-node-4 mgr[8595]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:48 ceph-node-2 client[2367]: INFO: OSD rebalancing completed
2025-02-24 10:51:21 ceph-node-4 radosgw[3959]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:21 ceph-node-4 mds[1997]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:51:15 ceph-node-2 mds[5643]: DEBUG: OSD rebalancing completed
2025-02-24 10:51:19 ceph-node-5 mgr[9033]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:16 ceph-node-1 client[9154]: INFO: PG 1.2 replicated successfully
2025-02-24 10:51:02 ceph-node-5 radosgw[4276]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:50:38 ceph-node-1 mds[8174]: ERROR: High I/O latency detected in radosgw service
2025-02-24 10:51:14 ceph-node-2 client[4628]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:56 ceph-node-4 osd[4703]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:37 ceph-node-4 mon[2149]: INFO: OSD rebalancing completed
2025-02-24 10:50:34 ceph-node-1 mds[4218]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:42 ceph-node-2 osd[5407]: ERROR: CRITICAL: Data loss detected on pool 1
2025-02-24 10:51:27 ceph-node-3 mgr[6612]: WARNING: OSD disk space utilization exceeded 90%
2025-02-24 10:50:58 ceph-node-2 mgr[1356]: INFO: MGR module loaded successfully
2025-02-24 10:51:02 ceph-node-5 mon[4377]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:51:17 ceph-node-2 osd[1184]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:50:46 ceph-node-5 radosgw[2986]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:23 ceph-node-5 mgr[9488]: NOTICE: OSD rebalancing completed
2025-02-24 10:50:56 ceph-node-3 mon[6278]: DEBUG: OSD rebalancing completed
2025-02-24 10:50:48 ceph-node-2 mds[5779]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:50:44 ceph-node-3 mgr[7979]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:50:55 ceph-node-3 mds[1619]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:05 ceph-node-5 mon[4402]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:54 ceph-node-3 mgr[8269]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:51:01 ceph-node-5 mgr[9528]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:42 ceph-node-3 client[2384]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:18 ceph-node-4 radosgw[2180]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:56 ceph-node-3 mds[5051]: INFO: MGR module loaded successfully
2025-02-24 10:50:47 ceph-node-5 mds[9734]: WARNING: Monitor down, unable to connect to quorum
2025-02-24 10:51:24 ceph-node-2 client[3624]: NOTICE: Data replication completed for object pool
2025-02-24 10:51:14 ceph-node-1 osd[6325]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:51:16 ceph-node-1 osd[1193]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:31 ceph-node-3 osd[6704]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:50:38 ceph-node-4 radosgw[8048]: ERROR: Client connection timeout detected
2025-02-24 10:50:56 ceph-node-1 osd[9221]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:50:57 ceph-node-1 radosgw[2195]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:32 ceph-node-5 client[6713]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:51:23 ceph-node-2 mon[9179]: CRITICAL: Disk failure reported on ceph-node-4
2025-02-24 10:51:16 ceph-node-2 mgr[7368]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:50:38 ceph-node-1 mds[9251]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:50:37 ceph-node-5 mds[1697]: INFO: OSD rebalancing completed
2025-02-24 10:50:51 ceph-node-4 mds[1427]: DEBUG: Monitor map has been updated
2025-02-24 10:51:02 ceph-node-2 mgr[6733]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:11 ceph-node-5 radosgw[8118]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:59 ceph-node-3 osd[7552]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:59 ceph-node-3 mgr[3350]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:29 ceph-node-4 mgr[9491]: DEBUG: OSD rebalancing completed
2025-02-24 10:50:56 ceph-node-4 client[4272]: INFO: MGR module loaded successfully
2025-02-24 10:51:16 ceph-node-4 mgr[1716]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:24 ceph-node-1 mds[6988]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:04 ceph-node-5 client[5384]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:04 ceph-node-4 mds[7733]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:18 ceph-node-4 osd[9407]: ERROR: OSD 3.1 marked down due to heartbeat failure
2025-02-24 10:51:13 ceph-node-4 radosgw[7397]: ERROR: Uncorrectable ECC error detected in MDS cache
2025-02-24 10:50:59 ceph-node-5 osd[1576]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:51:21 ceph-node-4 mon[2230]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:51 ceph-node-2 mon[6637]: WARNING: Journal write failure detected on OSD node
2025-02-24 10:51:08 ceph-node-3 osd[5720]: NOTICE: Data replication completed for object pool
2025-02-24 10:50:46 ceph-node-4 mds[3244]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:50:56 ceph-node-3 mds[1800]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:34 ceph-node-5 radosgw[1762]: INFO: OSD disk space utilization: 70%
2025-02-24 10:51:10 ceph-node-1 mds[8616]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:50:32 ceph-node-3 client[6807]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:51:10 ceph-node-1 mds[6063]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:52 ceph-node-4 mgr[7872]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:03 ceph-node-4 mgr[7248]: INFO: Monitor map has been updated
2025-02-24 10:50:54 ceph-node-5 radosgw[3189]: DEBUG: OSD rebalancing completed
2025-02-24 10:51:03 ceph-node-1 mon[7210]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:16 ceph-node-5 mds[5087]: INFO: OSD disk space utilization: 70%
2025-02-24 10:50:35 ceph-node-5 mon[6844]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:51:17 ceph-node-5 mgr[4386]: INFO: Monitor map has been updated
2025-02-24 10:51:09 ceph-node-2 osd[7919]: DEBUG: OSD rebalancing completed
2025-02-24 10:50:48 ceph-node-4 osd[5506]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:26 ceph-node-1 mgr[5526]: WARNING: Disk failure reported on ceph-node-4
2025-02-24 10:50:39 ceph-node-5 radosgw[1425]: DEBUG: Data replication completed for object pool
2025-02-24 10:50:37 ceph-node-1 mon[7575]: INFO: MGR module loaded successfully
2025-02-24 10:50:56 ceph-node-3 osd[7738]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:09 ceph-node-1 mon[8297]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:50:54 ceph-node-2 mds[2404]: INFO: Monitor map has been updated
2025-02-24 10:51:10 ceph-node-4 mgr[3969]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:09 ceph-node-4 mgr[1314]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:51:23 ceph-node-5 mon[3816]: DEBUG: OSD rebalancing completed
2025-02-24 10:50:37 ceph-node-3 mon[9025]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:19 ceph-node-2 mgr[9845]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:10 ceph-node-3 client[1523]: NOTICE: Monitor map has been updated
2025-02-24 10:51:26 ceph-node-1 radosgw[3428]: CRITICAL: Data corruption detected in object pool
2025-02-24 10:51:02 ceph-node-2 mgr[6494]: NOTICE: Data replication completed for object pool
2025-02-24 10:50:45 ceph-node-5 mds[8299]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:43 ceph-node-3 mgr[9698]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:50:46 ceph-node-4 mon[3762]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:50:49 ceph-node-1 mds[5366]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:51:24 ceph-node-5 client[1287]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:20 ceph-node-3 client[2844]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:50:45 ceph-node-3 radosgw[7727]: DEBUG: Monitor map has been updated
2025-02-24 10:51:30 ceph-node-2 mon[6795]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:01 ceph-node-4 client[6704]: CRITICAL: CRITICAL: Data loss detected on pool 1
2025-02-24 10:50:56 ceph-node-3 radosgw[3718]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:51:22 ceph-node-3 mgr[7835]: ERROR: OSD 3.1 marked down due to heartbeat failure
2025-02-24 10:51:22 ceph-node-5 radosgw[4933]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:27 ceph-node-2 mgr[1455]: CRITICAL: CRITICAL: Data loss detected on pool 1
2025-02-24 10:50:51 ceph-node-5 client[7123]: INFO: OSD rebalancing completed
2025-02-24 10:50:44 ceph-node-4 osd[5277]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:50:52 ceph-node-2 mgr[4798]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:50:59 ceph-node-1 client[5502]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:51:12 ceph-node-3 radosgw[6314]: WARNING: OSD disk space utilization exceeded 90%
2025-02-24 10:51:04 ceph-node-2 mon[3506]: NOTICE: MGR module loaded successfully
2025-02-24 10:51:27 ceph-node-1 osd[8060]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:34 ceph-node-3 mgr[3450]: NOTICE: Data replication completed for object pool
2025-02-24 10:50:57 ceph-node-5 client[4255]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:51:24 ceph-node-4 mgr[1813]: INFO: Data replication completed for object pool
2025-02-24 10:51:06 ceph-node-3 mgr[7106]: INFO: Monitor map has been updated
2025-02-24 10:50:58 ceph-node-3 radosgw[6482]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:06 ceph-node-5 osd[9478]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:50:42 ceph-node-3 mds[5842]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:55 ceph-node-3 osd[8430]: DEBUG: Data replication completed for object pool
2025-02-24 10:50:44 ceph-node-5 radosgw[9750]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:26 ceph-node-4 radosgw[1026]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:51:05 ceph-node-5 client[2445]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:16 ceph-node-3 mgr[4958]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:54 ceph-node-5 mgr[3953]: WARNING: Monitor mon1 failed to reach quorum
2025-02-24 10:51:10 ceph-node-1 osd[3714]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:25 ceph-node-4 client[4139]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:50:46 ceph-node-3 radosgw[2003]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:51:02 ceph-node-2 mgr[1412]: DEBUG: Monitor map has been updated
2025-02-24 10:50:47 ceph-node-2 radosgw[3371]: NOTICE: OSD rebalancing completed
2025-02-24 10:50:53 ceph-node-4 radosgw[3644]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:50:56 ceph-node-3 client[5960]: INFO: PG 1.2 replicated successfully
2025-02-24 10:51:08 ceph-node-3 osd[2523]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:50:53 ceph-node-1 client[5739]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:57 ceph-node-4 mgr[7165]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:36 ceph-node-1 osd[3588]: NOTICE: Monitor map has been updated
2025-02-24 10:51:23 ceph-node-3 mgr[5990]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:51:04 ceph-node-3 radosgw[2118]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:31 ceph-node-5 client[8922]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:04 ceph-node-2 osd[4621]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:50:50 ceph-node-4 osd[7282]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:50:52 ceph-node-1 osd[8372]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:19 ceph-node-4 radosgw[3068]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:51:18 ceph-node-4 mon[5756]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:42 ceph-node-1 radosgw[4982]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:50:34 ceph-node-1 client[1246]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:50:38 ceph-node-5 mgr[6699]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:50:58 ceph-node-4 mds[4177]: INFO: Data replication completed for object pool
2025-02-24 10:51:03 ceph-node-1 mds[2238]: NOTICE: OSD rebalancing completed
2025-02-24 10:51:16 ceph-node-2 client[6990]: WARNING: Slow request detected on OSD 4.3
2025-02-24 10:51:11 ceph-node-5 mgr[6854]: DEBUG: OSD rebalancing completed
2025-02-24 10:51:26 ceph-node-5 osd[9535]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:31 ceph-node-4 mds[9227]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:50:54 ceph-node-2 mds[3484]: DEBUG: Data replication completed for object pool
2025-02-24 10:50:35 ceph-node-2 mds[3807]: ERROR: Client connection timeout detected
2025-02-24 10:50:32 ceph-node-3 osd[6047]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:50:44 ceph-node-4 mon[6894]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:46 ceph-node-3 mds[2145]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:25 ceph-node-1 mon[1653]: NOTICE: MGR module loaded successfully
2025-02-24 10:51:20 ceph-node-5 mon[1488]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:57 ceph-node-1 mgr[6346]: DEBUG: OSD rebalancing completed
2025-02-24 10:51:21 ceph-node-2 mon[8948]: DEBUG: MGR module loaded successfully
2025-02-24 10:50:39 ceph-node-1 radosgw[7001]: CRITICAL: CRITICAL: Data loss detected on pool 1
2025-02-24 10:51:10 ceph-node-1 mgr[6352]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:50:49 ceph-node-2 mds[6488]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:50:53 ceph-node-1 mds[9646]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:51:14 ceph-node-2 mds[5535]: NOTICE: OSD rebalancing completed
2025-02-24 10:51:26 ceph-node-2 mon[3848]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:31 ceph-node-4 client[4399]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:41 ceph-node-2 mon[7592]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:50:45 ceph-node-5 radosgw[2210]: DEBUG: OSD rebalancing completed
2025-02-24 10:51:24 ceph-node-2 client[5513]: INFO: Data replication completed for object pool
2025-02-24 10:51:18 ceph-node-3 mgr[5927]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:50:52 ceph-node-3 mon[1060]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:02 ceph-node-5 mgr[1614]: INFO: MGR module loaded successfully
2025-02-24 10:50:50 ceph-node-1 osd[8656]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:49 ceph-node-2 mon[2953]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:12 ceph-node-3 mds[6358]: CRITICAL: Uncorrectable ECC error detected in MDS cache
2025-02-24 10:50:48 ceph-node-1 client[6718]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:15 ceph-node-2 osd[7861]: DEBUG: MGR module loaded successfully
2025-02-24 10:50:37 ceph-node-2 mds[9979]: INFO: Data replication completed for object pool
2025-02-24 10:50:35 ceph-node-3 mds[8312]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:50:50 ceph-node-3 osd[9619]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:31 ceph-node-4 client[7516]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:51:16 ceph-node-4 mon[7265]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:50:56 ceph-node-1 mon[6502]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:50:58 ceph-node-1 client[4272]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:50:51 ceph-node-2 client[9712]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:51:17 ceph-node-3 mgr[1031]: NOTICE: Monitor map has been updated
2025-02-24 10:51:04 ceph-node-1 mgr[5145]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:31 ceph-node-4 osd[5111]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:32 ceph-node-3 client[8292]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:50:39 ceph-node-5 client[9052]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:51:07 ceph-node-5 radosgw[7361]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:51:21 ceph-node-3 mon[7364]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:50:35 ceph-node-1 radosgw[3545]: INFO: OSD rebalancing completed
2025-02-24 10:50:42 ceph-node-5 mgr[9504]: CRITICAL: Authentication failure in RADOS Gateway
2025-02-24 10:50:42 ceph-node-5 mon[7001]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:38 ceph-node-5 mds[4127]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:28 ceph-node-4 radosgw[4022]: INFO: Monitor map has been updated
2025-02-24 10:51:17 ceph-node-2 mgr[6737]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:16 ceph-node-2 radosgw[3902]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:29 ceph-node-2 mgr[1998]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:51:00 ceph-node-1 osd[8946]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:59 ceph-node-5 mds[7359]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:19 ceph-node-2 mon[3008]: NOTICE: Data replication completed for object pool
2025-02-24 10:50:44 ceph-node-1 client[2350]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:50:34 ceph-node-1 mds[3842]: CRITICAL: Disk failure reported on ceph-node-4
2025-02-24 10:50:41 ceph-node-3 mon[6578]: DEBUG: Monitor map has been updated
2025-02-24 10:51:17 ceph-node-2 client[4360]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:28 ceph-node-2 osd[2967]: DEBUG: MGR module loaded successfully
2025-02-24 10:50:44 ceph-node-1 radosgw[8520]: DEBUG: MGR module loaded successfully
2025-02-24 10:50:43 ceph-node-1 mds[9411]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:50:48 ceph-node-4 radosgw[9847]: WARNING: CRITICAL: Data loss detected on pool 1
2025-02-24 10:51:09 ceph-node-3 mon[1908]: CRITICAL: Monitor mon1 failed to reach quorum
2025-02-24 10:51:10 ceph-node-4 client[9312]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:51:09 ceph-node-4 client[3356]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:10 ceph-node-1 client[4884]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:51:11 ceph-node-1 mon[6517]: WARNING: Cluster status: HEALTH_WARN
2025-02-24 10:51:13 ceph-node-5 mon[9682]: INFO: OSD rebalancing completed
2025-02-24 10:51:31 ceph-node-2 mon[6759]: WARNING: OSD disk space utilization exceeded 90%
2025-02-24 10:50:46 ceph-node-2 mds[7456]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:10 ceph-node-1 mds[1017]: INFO: PG 1.2 replicated successfully
2025-02-24 10:50:54 ceph-node-2 mgr[5893]: INFO: OSD rebalancing completed
2025-02-24 10:51:11 ceph-node-3 radosgw[8613]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:43 ceph-node-4 radosgw[9421]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:08 ceph-node-3 mds[8347]: NOTICE: OSD rebalancing completed
2025-02-24 10:51:19 ceph-node-2 mds[2588]: INFO: OSD rebalancing completed
2025-02-24 10:50:40 ceph-node-2 client[7720]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:51 ceph-node-1 radosgw[4443]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:50:33 ceph-node-1 mds[1577]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:50:48 ceph-node-5 mgr[5181]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:49 ceph-node-4 mds[8903]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:20 ceph-node-4 client[4118]: INFO: OSD rebalancing completed
2025-02-24 10:50:54 ceph-node-3 radosgw[9260]: CRITICAL: Journal write failure detected on OSD node
2025-02-24 10:51:00 ceph-node-4 radosgw[3303]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:47 ceph-node-2 osd[9629]: DEBUG: OSD rebalancing completed
2025-02-24 10:51:24 ceph-node-4 mds[5397]: INFO: Data replication completed for object pool
2025-02-24 10:51:27 ceph-node-1 client[3081]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:51:28 ceph-node-4 mon[4159]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:11 ceph-node-5 osd[5636]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:20 ceph-node-2 mon[7739]: INFO: PG 1.2 replicated successfully
2025-02-24 10:50:37 ceph-node-5 mds[4217]: NOTICE: MGR module loaded successfully
2025-02-24 10:50:52 ceph-node-5 client[4391]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:51:04 ceph-node-5 radosgw[8911]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:51:09 ceph-node-4 client[7375]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:51:12 ceph-node-1 client[3923]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:50:32 ceph-node-1 osd[4806]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:50:32 ceph-node-1 client[5080]: INFO: MGR module loaded successfully
2025-02-24 10:51:07 ceph-node-1 osd[8786]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:50:34 ceph-node-5 radosgw[7932]: NOTICE: MGR module loaded successfully
2025-02-24 10:50:33 ceph-node-2 client[1514]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:35 ceph-node-1 mgr[3416]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:07 ceph-node-2 mds[9283]: DEBUG: Monitor map has been updated
2025-02-24 10:51:29 ceph-node-4 mgr[4295]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:08 ceph-node-4 mds[3468]: INFO: Data replication completed for object pool
2025-02-24 10:51:01 ceph-node-5 osd[2493]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:24 ceph-node-4 client[6430]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:42 ceph-node-2 mon[4837]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:39 ceph-node-1 radosgw[6931]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:50:47 ceph-node-5 client[1692]: ERROR: MDS failure detected: metadata inconsistency
2025-02-24 10:50:43 ceph-node-1 radosgw[3657]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:18 ceph-node-4 mds[7209]: DEBUG: Monitor map has been updated
2025-02-24 10:50:35 ceph-node-1 osd[1230]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:50:45 ceph-node-1 client[1436]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:08 ceph-node-4 osd[1973]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:45 ceph-node-2 radosgw[6046]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:29 ceph-node-2 osd[9420]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:51:17 ceph-node-5 client[9845]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:50:59 ceph-node-4 radosgw[6558]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:50:48 ceph-node-4 mgr[9121]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:50:41 ceph-node-2 radosgw[1210]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:50 ceph-node-1 osd[1632]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:51:23 ceph-node-5 osd[2372]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:01 ceph-node-1 osd[5381]: INFO: Monitor map has been updated
2025-02-24 10:51:11 ceph-node-1 mds[5541]: CRITICAL: Disk failure reported on ceph-node-4
2025-02-24 10:50:34 ceph-node-2 osd[1540]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:04 ceph-node-2 osd[6239]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:51:15 ceph-node-4 mds[8071]: ERROR: High I/O latency detected in radosgw service
2025-02-24 10:50:45 ceph-node-1 mds[5418]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:50:42 ceph-node-2 client[6090]: CRITICAL: Authentication failure in RADOS Gateway
2025-02-24 10:50:44 ceph-node-1 mds[3731]: NOTICE: OSD rebalancing completed
2025-02-24 10:50:55 ceph-node-4 radosgw[2428]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:34 ceph-node-4 mon[2533]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:22 ceph-node-1 mgr[5573]: NOTICE: MGR module loaded successfully
2025-02-24 10:51:13 ceph-node-5 mgr[2154]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:50 ceph-node-3 mds[7233]: INFO: OSD disk space utilization: 70%
2025-02-24 10:51:29 ceph-node-3 mds[8109]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:50:56 ceph-node-2 radosgw[9061]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:50:58 ceph-node-2 radosgw[9603]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:50:32 ceph-node-1 mds[2605]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:58 ceph-node-3 mon[2006]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:50:43 ceph-node-4 mon[5933]: DEBUG: OSD rebalancing completed
2025-02-24 10:50:34 ceph-node-2 mon[7222]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:07 ceph-node-1 mds[6546]: ERROR: Data corruption detected in object pool
2025-02-24 10:51:28 ceph-node-3 osd[9048]: INFO: Data replication completed for object pool
2025-02-24 10:51:01 ceph-node-5 radosgw[5322]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:51:10 ceph-node-5 mon[6044]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:51:24 ceph-node-2 mon[4647]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:32 ceph-node-5 mon[9872]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:51:06 ceph-node-3 client[9587]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:07 ceph-node-3 mon[7851]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:50:58 ceph-node-4 mds[6388]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:16 ceph-node-2 mds[3687]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:25 ceph-node-5 mgr[5569]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:59 ceph-node-1 mds[9733]: INFO: MGR module loaded successfully
2025-02-24 10:50:58 ceph-node-5 mds[2494]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:31 ceph-node-4 radosgw[2648]: NOTICE: Monitor map has been updated
2025-02-24 10:51:01 ceph-node-5 osd[6698]: WARNING: MDS failure detected: metadata inconsistency
2025-02-24 10:51:09 ceph-node-5 mon[9085]: NOTICE: MGR module loaded successfully
2025-02-24 10:51:20 ceph-node-5 client[8971]: DEBUG: Monitor map has been updated
2025-02-24 10:50:45 ceph-node-2 osd[6672]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:23 ceph-node-3 mon[8752]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:50:43 ceph-node-4 mon[9848]: NOTICE: Monitor map has been updated
2025-02-24 10:51:28 ceph-node-4 radosgw[8128]: INFO: OSD rebalancing completed
2025-02-24 10:51:17 ceph-node-2 radosgw[6100]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:51:03 ceph-node-3 radosgw[8391]: ERROR: High I/O latency detected in radosgw service
2025-02-24 10:51:25 ceph-node-3 client[8507]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:50:53 ceph-node-4 client[3338]: NOTICE: MGR module loaded successfully
2025-02-24 10:51:13 ceph-node-5 client[9936]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:50:58 ceph-node-5 mgr[2825]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:31 ceph-node-2 osd[4154]: WARNING: Cluster status: HEALTH_WARN
2025-02-24 10:50:38 ceph-node-1 client[4375]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:27 ceph-node-1 radosgw[4552]: CRITICAL: Client connection timeout detected
2025-02-24 10:51:02 ceph-node-1 mds[7320]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:51:07 ceph-node-4 mgr[8176]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:50:56 ceph-node-2 osd[5332]: INFO: MGR module loaded successfully
2025-02-24 10:50:43 ceph-node-4 osd[5250]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:50:47 ceph-node-2 client[5170]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:50 ceph-node-4 mds[5118]: DEBUG: Monitor map has been updated
2025-02-24 10:50:35 ceph-node-1 client[7934]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:50:40 ceph-node-3 mgr[2709]: INFO: PG 1.2 replicated successfully
2025-02-24 10:51:01 ceph-node-3 mds[4383]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:02 ceph-node-5 client[4190]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:01 ceph-node-4 mgr[7419]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:44 ceph-node-4 mgr[2385]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:36 ceph-node-2 radosgw[2467]: WARNING: OSD disk space utilization exceeded 90%
2025-02-24 10:51:05 ceph-node-4 osd[6710]: NOTICE: MGR module loaded successfully
2025-02-24 10:51:18 ceph-node-4 mds[7674]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:50:46 ceph-node-3 osd[7040]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:51:22 ceph-node-1 radosgw[8614]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:50:50 ceph-node-1 mon[8329]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:50:43 ceph-node-2 osd[6926]: DEBUG: Data replication completed for object pool
2025-02-24 10:50:46 ceph-node-3 osd[7299]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:50:43 ceph-node-3 mds[4990]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:15 ceph-node-2 client[5429]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:49 ceph-node-1 mgr[3032]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:50:53 ceph-node-4 mgr[2428]: INFO: MGR module loaded successfully
2025-02-24 10:50:44 ceph-node-3 client[3008]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:51:28 ceph-node-3 radosgw[9761]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:29 ceph-node-1 osd[1306]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:51:24 ceph-node-1 mon[4110]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:27 ceph-node-3 mgr[3029]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:50:49 ceph-node-1 osd[7567]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:51:27 ceph-node-3 radosgw[3049]: INFO: OSD rebalancing completed
2025-02-24 10:51:22 ceph-node-2 mds[6203]: NOTICE: MGR module loaded successfully
2025-02-24 10:50:50 ceph-node-1 mgr[5660]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:51:17 ceph-node-1 radosgw[5407]: NOTICE: Data replication completed for object pool
2025-02-24 10:51:23 ceph-node-4 mon[4085]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:50:50 ceph-node-3 client[9708]: ERROR: Monitor mon1 failed to reach quorum
2025-02-24 10:51:07 ceph-node-2 mon[2491]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:22 ceph-node-3 mgr[1350]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:50:37 ceph-node-1 osd[2750]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:50:55 ceph-node-5 radosgw[9137]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:02 ceph-node-5 osd[4106]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:50:34 ceph-node-5 osd[8056]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:50:46 ceph-node-4 mon[2825]: NOTICE: Data replication completed for object pool
2025-02-24 10:50:55 ceph-node-4 osd[6338]: WARNING: Data corruption detected in object pool
2025-02-24 10:51:19 ceph-node-4 osd[8496]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:50:43 ceph-node-4 radosgw[7135]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:51:05 ceph-node-1 radosgw[4886]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:18 ceph-node-2 mgr[6370]: NOTICE: MGR module loaded successfully
2025-02-24 10:50:55 ceph-node-5 mon[7288]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:51:01 ceph-node-4 client[5410]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:08 ceph-node-1 osd[2592]: ERROR: Monitor down, unable to connect to quorum
2025-02-24 10:51:11 ceph-node-2 mgr[6356]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:43 ceph-node-1 mon[8638]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:31 ceph-node-4 client[1438]: CRITICAL: CRITICAL: Data loss detected on pool 1
2025-02-24 10:50:32 ceph-node-4 mgr[8785]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:51:17 ceph-node-2 mgr[5892]: DEBUG: OSD rebalancing completed
2025-02-24 10:50:40 ceph-node-4 client[2990]: INFO: OSD rebalancing completed
2025-02-24 10:51:06 ceph-node-3 osd[3140]: ERROR: MDS failure detected: metadata inconsistency
2025-02-24 10:50:42 ceph-node-4 mgr[4046]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:50 ceph-node-3 mgr[3569]: NOTICE: Monitor map has been updated
2025-02-24 10:51:05 ceph-node-4 mgr[8841]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:50:41 ceph-node-2 client[9835]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:51:30 ceph-node-2 radosgw[3970]: INFO: OSD rebalancing completed
2025-02-24 10:50:46 ceph-node-4 mgr[6885]: ERROR: Network partition detected between OSD nodes
2025-02-24 10:51:28 ceph-node-5 client[8021]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:00 ceph-node-4 mgr[5577]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:23 ceph-node-1 mon[5363]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:43 ceph-node-3 mds[4969]: DEBUG: OSD rebalancing completed
2025-02-24 10:51:08 ceph-node-5 radosgw[2408]: DEBUG: MGR module loaded successfully
2025-02-24 10:50:46 ceph-node-3 client[5683]: ERROR: Network partition detected between OSD nodes
2025-02-24 10:50:34 ceph-node-5 radosgw[7888]: ERROR: Monitor mon1 failed to reach quorum
2025-02-24 10:51:14 ceph-node-1 osd[7373]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:55 ceph-node-5 mds[3582]: ERROR: High I/O latency detected in radosgw service
2025-02-24 10:51:31 ceph-node-3 client[5037]: INFO: Data replication completed for object pool
2025-02-24 10:50:58 ceph-node-1 radosgw[8023]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:24 ceph-node-4 osd[1060]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:20 ceph-node-1 mds[5414]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:13 ceph-node-5 mgr[4375]: ERROR: MDS failure detected: metadata inconsistency
2025-02-24 10:50:55 ceph-node-1 mgr[1133]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:42 ceph-node-2 client[1504]: INFO: OSD rebalancing completed
2025-02-24 10:50:42 ceph-node-2 osd[4229]: INFO: MGR module loaded successfully
2025-02-24 10:50:40 ceph-node-3 mgr[6229]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:50:32 ceph-node-4 mon[3405]: INFO: OSD rebalancing completed
2025-02-24 10:50:31 ceph-node-4 mon[5777]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:50:40 ceph-node-1 radosgw[7159]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:51:25 ceph-node-4 mds[6522]: INFO: MGR module loaded successfully
2025-02-24 10:50:59 ceph-node-1 mds[5374]: CRITICAL: MDS failure detected: metadata inconsistency
2025-02-24 10:51:04 ceph-node-4 mon[3567]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:16 ceph-node-2 mgr[8153]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:50:37 ceph-node-3 osd[3365]: NOTICE: MGR module loaded successfully
2025-02-24 10:50:47 ceph-node-5 client[9572]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:11 ceph-node-1 client[4636]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:47 ceph-node-2 mds[7277]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:02 ceph-node-3 mgr[4397]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:50:47 ceph-node-4 radosgw[6665]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:14 ceph-node-1 client[4287]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:56 ceph-node-4 mon[8874]: INFO: OSD disk space utilization: 70%
2025-02-24 10:50:48 ceph-node-4 mgr[9487]: INFO: OSD disk space utilization: 70%
2025-02-24 10:50:59 ceph-node-4 radosgw[2546]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:51:10 ceph-node-5 mds[6763]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:37 ceph-node-5 mgr[6914]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:50:51 ceph-node-5 osd[2122]: NOTICE: Monitor map has been updated
2025-02-24 10:50:43 ceph-node-4 mds[5128]: INFO: PG 1.2 replicated successfully
2025-02-24 10:50:53 ceph-node-3 client[9667]: DEBUG: Data replication completed for object pool
2025-02-24 10:50:31 ceph-node-1 radosgw[3227]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:51:14 ceph-node-1 client[6141]: NOTICE: Data replication completed for object pool
2025-02-24 10:50:43 ceph-node-4 mon[5956]: NOTICE: Data replication completed for object pool
2025-02-24 10:51:19 ceph-node-3 client[9726]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:51:04 ceph-node-3 client[1498]: NOTICE: MGR module loaded successfully
2025-02-24 10:51:26 ceph-node-5 radosgw[9616]: INFO: MGR module loaded successfully
2025-02-24 10:51:28 ceph-node-1 mon[1534]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:51:21 ceph-node-3 client[9136]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:33 ceph-node-5 mgr[7078]: DEBUG: MGR module loaded successfully
2025-02-24 10:50:54 ceph-node-2 mds[8725]: DEBUG: OSD rebalancing completed
2025-02-24 10:51:09 ceph-node-5 osd[9457]: NOTICE: MGR module loaded successfully
2025-02-24 10:50:53 ceph-node-1 mgr[2178]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:50:55 ceph-node-2 radosgw[1957]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:50:36 ceph-node-3 mon[1314]: ERROR: Data corruption detected in object pool
2025-02-24 10:50:56 ceph-node-4 mgr[9100]: NOTICE: Monitor map has been updated
2025-02-24 10:50:39 ceph-node-1 client[9818]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:50:57 ceph-node-4 radosgw[5412]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:29 ceph-node-3 mgr[1716]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:25 ceph-node-4 mds[4599]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:25 ceph-node-3 client[8430]: NOTICE: Data replication completed for object pool
2025-02-24 10:50:59 ceph-node-1 osd[3114]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:14 ceph-node-1 osd[4764]: INFO: OSD disk space utilization: 70%
2025-02-24 10:50:46 ceph-node-4 radosgw[5253]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:06 ceph-node-4 osd[5280]: NOTICE: MGR module loaded successfully
2025-02-24 10:50:59 ceph-node-5 mds[3100]: ERROR: Monitor mon1 failed to reach quorum
2025-02-24 10:51:15 ceph-node-5 radosgw[2852]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:10 ceph-node-1 client[7922]: NOTICE: Monitor map has been updated
2025-02-24 10:51:29 ceph-node-4 mds[6013]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:02 ceph-node-4 mgr[5936]: INFO: PG 1.2 replicated successfully
2025-02-24 10:50:32 ceph-node-1 mds[2044]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:50:44 ceph-node-3 mds[5721]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:03 ceph-node-3 client[7404]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:26 ceph-node-5 radosgw[6886]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:50:31 ceph-node-1 mgr[5223]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:51:10 ceph-node-4 client[9195]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:50:47 ceph-node-1 client[7603]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:51:26 ceph-node-5 mon[5389]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:50:46 ceph-node-2 radosgw[8570]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:00 ceph-node-2 client[3483]: DEBUG: Monitor map has been updated
2025-02-24 10:50:37 ceph-node-4 mon[8105]: NOTICE: OSD rebalancing completed
2025-02-24 10:51:15 ceph-node-4 client[5591]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:50:46 ceph-node-1 radosgw[6652]: CRITICAL: Monitor down, unable to connect to quorum
2025-02-24 10:51:25 ceph-node-3 mgr[7780]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:03 ceph-node-3 mon[3701]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:30 ceph-node-4 osd[5671]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:10 ceph-node-3 mds[3289]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:42 ceph-node-3 osd[1517]: NOTICE: Data replication completed for object pool
2025-02-24 10:50:59 ceph-node-2 osd[6130]: NOTICE: OSD rebalancing completed
2025-02-24 10:51:03 ceph-node-5 mds[8761]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:10 ceph-node-1 radosgw[8556]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:11 ceph-node-3 osd[7272]: INFO: OSD disk space utilization: 70%
2025-02-24 10:51:18 ceph-node-4 mgr[9509]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:22 ceph-node-3 osd[8558]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:30 ceph-node-4 mgr[2151]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:50:36 ceph-node-4 radosgw[2132]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:51:24 ceph-node-3 mds[8853]: INFO: PG 1.2 replicated successfully
2025-02-24 10:50:45 ceph-node-5 radosgw[3952]: NOTICE: MGR module loaded successfully
2025-02-24 10:51:30 ceph-node-3 osd[4249]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:50:36 ceph-node-2 osd[1795]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:29 ceph-node-2 mgr[7356]: DEBUG: MGR module loaded successfully
2025-02-24 10:50:59 ceph-node-1 radosgw[5270]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:16 ceph-node-2 osd[9949]: INFO: MGR module loaded successfully
2025-02-24 10:51:06 ceph-node-3 osd[3886]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:51:21 ceph-node-2 mon[5330]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:31 ceph-node-2 mds[8316]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:50:46 ceph-node-2 mds[6072]: WARNING: Journal write failure detected on OSD node
2025-02-24 10:51:26 ceph-node-2 mds[5422]: INFO: OSD rebalancing completed
2025-02-24 10:51:13 ceph-node-3 radosgw[4337]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:54 ceph-node-3 osd[5430]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:51:10 ceph-node-4 mon[4113]: NOTICE: Data replication completed for object pool
2025-02-24 10:50:46 ceph-node-3 mon[5841]: NOTICE: MGR module loaded successfully
2025-02-24 10:50:53 ceph-node-4 mon[1228]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:29 ceph-node-3 mon[7454]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:45 ceph-node-3 client[7436]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:31 ceph-node-1 client[7257]: CRITICAL: OSD 3.1 marked down due to heartbeat failure
2025-02-24 10:51:13 ceph-node-3 radosgw[2647]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:50:51 ceph-node-5 mon[4994]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:45 ceph-node-3 mon[4839]: INFO: Data replication completed for object pool
2025-02-24 10:51:26 ceph-node-1 client[9405]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:19 ceph-node-1 mgr[3976]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:26 ceph-node-1 mds[9876]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:02 ceph-node-2 client[2667]: CRITICAL: Disk failure reported on ceph-node-4
2025-02-24 10:50:44 ceph-node-5 mon[2396]: INFO: PG 1.2 replicated successfully
2025-02-24 10:51:19 ceph-node-4 mon[7979]: DEBUG: Monitor map has been updated
2025-02-24 10:50:37 ceph-node-4 osd[8951]: ERROR: Authentication failure in RADOS Gateway
2025-02-24 10:50:48 ceph-node-3 client[6130]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:50:48 ceph-node-3 client[7663]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:35 ceph-node-2 mon[8281]: DEBUG: Monitor map has been updated
2025-02-24 10:50:42 ceph-node-4 mds[8264]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:59 ceph-node-4 osd[7147]: INFO: OSD disk space utilization: 70%
2025-02-24 10:51:12 ceph-node-1 mon[1058]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:50:38 ceph-node-2 radosgw[1626]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:05 ceph-node-2 mds[4268]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:02 ceph-node-3 osd[2401]: NOTICE: MGR module loaded successfully
2025-02-24 10:51:09 ceph-node-1 client[8217]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:50:57 ceph-node-3 radosgw[5383]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:40 ceph-node-1 client[4739]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:50:53 ceph-node-2 mds[6993]: INFO: Data replication completed for object pool
2025-02-24 10:50:48 ceph-node-3 mon[9940]: CRITICAL: CRITICAL: PG 2.1 stuck in inconsistent state
2025-02-24 10:51:12 ceph-node-4 osd[1391]: INFO: OSD disk space utilization: 70%
2025-02-24 10:51:01 ceph-node-4 mgr[8789]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:46 ceph-node-1 radosgw[9569]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:50:38 ceph-node-3 client[8776]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:45 ceph-node-5 radosgw[8407]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:56 ceph-node-4 mds[2499]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:32 ceph-node-2 mds[4767]: WARNING: MDS failure detected: metadata inconsistency
2025-02-24 10:50:35 ceph-node-5 radosgw[4879]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:20 ceph-node-3 mds[6221]: INFO: Data replication completed for object pool
2025-02-24 10:51:15 ceph-node-1 mon[6469]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:51:10 ceph-node-1 client[6754]: NOTICE: MGR module loaded successfully
2025-02-24 10:50:53 ceph-node-4 client[6050]: NOTICE: OSD rebalancing completed
2025-02-24 10:51:19 ceph-node-2 mgr[6587]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:51:07 ceph-node-4 client[1384]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:56 ceph-node-2 mds[7450]: INFO: OSD rebalancing completed
2025-02-24 10:51:14 ceph-node-4 mgr[4606]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:50:43 ceph-node-3 radosgw[8998]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:10 ceph-node-5 mds[3455]: DEBUG: MGR module loaded successfully
2025-02-24 10:50:54 ceph-node-4 mds[6804]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:51:29 ceph-node-3 osd[7210]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:50:46 ceph-node-5 client[8369]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:53 ceph-node-4 client[2633]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:51:10 ceph-node-1 radosgw[9192]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:51:25 ceph-node-5 mon[6629]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:15 ceph-node-4 osd[9572]: CRITICAL: Authentication failure in RADOS Gateway
2025-02-24 10:51:17 ceph-node-4 mgr[3524]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:15 ceph-node-4 osd[2440]: NOTICE: Data replication completed for object pool
2025-02-24 10:50:36 ceph-node-4 mgr[1886]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:50:47 ceph-node-4 mgr[8492]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:26 ceph-node-5 client[1500]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:50:52 ceph-node-2 client[6498]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:35 ceph-node-3 mds[4498]: WARNING: MDS failure detected: metadata inconsistency
2025-02-24 10:51:25 ceph-node-1 mon[7515]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:50:47 ceph-node-5 mgr[1930]: NOTICE: Data replication completed for object pool
2025-02-24 10:51:22 ceph-node-2 mgr[9654]: INFO: PG 1.2 replicated successfully
2025-02-24 10:51:05 ceph-node-1 mon[5443]: DEBUG: Monitor map has been updated
2025-02-24 10:50:50 ceph-node-1 mgr[3040]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:50:45 ceph-node-3 mgr[3327]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:05 ceph-node-1 osd[1294]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:52 ceph-node-3 client[6480]: NOTICE: MGR module loaded successfully
2025-02-24 10:51:21 ceph-node-2 radosgw[5901]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:50:50 ceph-node-2 mds[9297]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:51:28 ceph-node-2 mgr[1891]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:51:03 ceph-node-5 mon[6894]: CRITICAL: OSD disk space utilization exceeded 90%
2025-02-24 10:51:12 ceph-node-1 radosgw[2233]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:24 ceph-node-4 radosgw[5364]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:50:34 ceph-node-4 radosgw[5998]: INFO: OSD disk space utilization: 70%
2025-02-24 10:50:43 ceph-node-4 radosgw[7524]: INFO: Data replication completed for object pool
2025-02-24 10:50:57 ceph-node-2 osd[5173]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:10 ceph-node-4 radosgw[8902]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:51:19 ceph-node-4 client[7895]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:10 ceph-node-5 radosgw[4616]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:50:31 ceph-node-4 client[4849]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:16 ceph-node-5 mon[3965]: INFO: OSD rebalancing completed
2025-02-24 10:50:48 ceph-node-5 osd[7509]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:51:02 ceph-node-5 mds[9652]: ERROR: Authentication failure in RADOS Gateway
2025-02-24 10:50:32 ceph-node-4 osd[7593]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:51:09 ceph-node-4 radosgw[5259]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:39 ceph-node-3 mgr[8414]: ERROR: CRITICAL: PG 2.1 stuck in inconsistent state
2025-02-24 10:51:17 ceph-node-4 mon[4657]: INFO: MGR module loaded successfully
2025-02-24 10:50:40 ceph-node-4 mon[3080]: DEBUG: OSD rebalancing completed
2025-02-24 10:50:49 ceph-node-5 mon[8498]: INFO: Data replication completed for object pool
2025-02-24 10:50:34 ceph-node-3 mds[6567]: ERROR: CRITICAL: PG 2.1 stuck in inconsistent state
2025-02-24 10:50:41 ceph-node-1 osd[8653]: INFO: PG 1.2 replicated successfully
2025-02-24 10:51:29 ceph-node-5 mgr[9523]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:17 ceph-node-5 osd[5870]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:22 ceph-node-2 client[4161]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:50:57 ceph-node-4 osd[7339]: CRITICAL: Monitor mon1 failed to reach quorum
2025-02-24 10:50:59 ceph-node-1 mon[4684]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:43 ceph-node-2 mgr[9502]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:43 ceph-node-4 client[1797]: NOTICE: Monitor map has been updated
2025-02-24 10:51:17 ceph-node-2 osd[6483]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:50:40 ceph-node-3 mgr[8168]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:30 ceph-node-3 osd[7829]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:50:39 ceph-node-2 radosgw[3939]: INFO: Monitor map has been updated
2025-02-24 10:51:31 ceph-node-5 client[9554]: WARNING: CRITICAL: Data loss detected on pool 1
2025-02-24 10:50:52 ceph-node-4 mon[9417]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:36 ceph-node-1 radosgw[8948]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:50:45 ceph-node-4 client[5946]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:26 ceph-node-2 mon[5969]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:52 ceph-node-2 osd[9662]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:18 ceph-node-2 osd[9974]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:51:04 ceph-node-4 radosgw[2804]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:14 ceph-node-2 osd[6554]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:25 ceph-node-1 mgr[3565]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:33 ceph-node-3 client[4073]: INFO: MGR module loaded successfully
2025-02-24 10:50:44 ceph-node-1 mgr[2912]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:50 ceph-node-4 mon[5710]: INFO: MGR module loaded successfully
2025-02-24 10:51:14 ceph-node-5 mon[6612]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:25 ceph-node-4 mgr[9003]: NOTICE: MGR module loaded successfully
2025-02-24 10:50:55 ceph-node-1 mon[6293]: DEBUG: Monitor map has been updated
2025-02-24 10:51:15 ceph-node-5 radosgw[9447]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:43 ceph-node-1 radosgw[3825]: INFO: OSD disk space utilization: 70%
2025-02-24 10:51:19 ceph-node-3 client[3584]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:27 ceph-node-2 radosgw[9408]: INFO: PG 1.2 replicated successfully
2025-02-24 10:50:42 ceph-node-3 mds[9979]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:51:10 ceph-node-5 radosgw[9765]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:50:50 ceph-node-2 client[7523]: INFO: PG 1.2 replicated successfully
2025-02-24 10:51:12 ceph-node-5 mgr[8392]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:51:20 ceph-node-3 mon[4047]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:51:14 ceph-node-3 mds[8636]: INFO: Data replication completed for object pool
2025-02-24 10:50:40 ceph-node-4 mds[5439]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:22 ceph-node-1 osd[3738]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:50:51 ceph-node-2 mds[3960]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:49 ceph-node-4 mon[8562]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:50:39 ceph-node-5 client[5156]: DEBUG: Monitor map has been updated
2025-02-24 10:51:30 ceph-node-3 mon[6986]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:31 ceph-node-5 radosgw[7922]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:51:04 ceph-node-4 mds[2366]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:51:25 ceph-node-3 mgr[7998]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:16 ceph-node-5 osd[3073]: DEBUG: Monitor map has been updated
2025-02-24 10:51:04 ceph-node-4 osd[3895]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:50:34 ceph-node-3 mds[7361]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:27 ceph-node-5 mgr[5175]: NOTICE: Monitor map has been updated
2025-02-24 10:50:34 ceph-node-4 mon[5570]: DEBUG: OSD rebalancing completed
2025-02-24 10:51:22 ceph-node-4 mds[4935]: INFO: Monitor map has been updated
2025-02-24 10:50:50 ceph-node-2 mon[4836]: INFO: MGR module loaded successfully
2025-02-24 10:51:11 ceph-node-3 mds[5386]: INFO: MGR module loaded successfully
2025-02-24 10:50:42 ceph-node-3 mgr[8641]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:51:07 ceph-node-3 mgr[1006]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:50:37 ceph-node-2 mgr[1536]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:10 ceph-node-1 mds[6349]: NOTICE: MGR module loaded successfully
2025-02-24 10:50:51 ceph-node-3 mds[7818]: NOTICE: Data replication completed for object pool
2025-02-24 10:50:36 ceph-node-4 mon[6935]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:51:12 ceph-node-2 radosgw[9230]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:51:06 ceph-node-5 osd[8167]: NOTICE: OSD rebalancing completed
2025-02-24 10:50:55 ceph-node-4 mds[9005]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:04 ceph-node-1 mgr[6803]: DEBUG: Data replication completed for object pool
2025-02-24 10:50:47 ceph-node-5 osd[1024]: NOTICE: OSD rebalancing completed
2025-02-24 10:51:10 ceph-node-4 mon[6467]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:08 ceph-node-1 mds[5396]: NOTICE: Data replication completed for object pool
2025-02-24 10:51:26 ceph-node-5 client[1693]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:41 ceph-node-1 osd[8317]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:10 ceph-node-5 radosgw[2227]: DEBUG: Data replication completed for object pool
2025-02-24 10:50:50 ceph-node-1 osd[2971]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:08 ceph-node-5 radosgw[3611]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:51:18 ceph-node-2 client[7791]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:51:09 ceph-node-4 osd[1151]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:51:30 ceph-node-1 radosgw[6150]: NOTICE: MGR module loaded successfully
2025-02-24 10:51:19 ceph-node-2 mds[2437]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:50:42 ceph-node-1 osd[4878]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:51:30 ceph-node-2 client[5124]: ERROR: Slow request detected on OSD 4.3
2025-02-24 10:50:59 ceph-node-3 radosgw[2516]: CRITICAL: Monitor mon1 failed to reach quorum
2025-02-24 10:51:18 ceph-node-1 client[6932]: CRITICAL: Monitor down, unable to connect to quorum
2025-02-24 10:50:45 ceph-node-2 client[9723]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:50:59 ceph-node-1 radosgw[7860]: ERROR: Disk failure reported on ceph-node-4
2025-02-24 10:51:06 ceph-node-1 mon[6985]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:32 ceph-node-4 mon[9161]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:51:11 ceph-node-5 radosgw[4406]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:09 ceph-node-4 client[4956]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:58 ceph-node-4 osd[6772]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:50:53 ceph-node-2 mon[7398]: CRITICAL: OSD disk space utilization exceeded 90%
2025-02-24 10:51:12 ceph-node-1 radosgw[5541]: NOTICE: Monitor map has been updated
2025-02-24 10:50:57 ceph-node-2 mon[6176]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:24 ceph-node-2 mgr[4626]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:13 ceph-node-2 client[8299]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:05 ceph-node-1 mds[2668]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:50:57 ceph-node-2 mds[3457]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:30 ceph-node-5 radosgw[5508]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:05 ceph-node-4 osd[2752]: DEBUG: OSD rebalancing completed
2025-02-24 10:51:13 ceph-node-1 client[8476]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:50:39 ceph-node-2 client[5361]: NOTICE: MGR module loaded successfully
2025-02-24 10:51:08 ceph-node-1 mds[8731]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:51:20 ceph-node-1 mgr[2154]: INFO: MGR module loaded successfully
2025-02-24 10:50:35 ceph-node-1 radosgw[3564]: WARNING: Network partition detected between OSD nodes
2025-02-24 10:50:38 ceph-node-3 radosgw[5250]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:50:32 ceph-node-5 mds[2977]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:50:51 ceph-node-5 client[8387]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:12 ceph-node-5 mon[5386]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:56 ceph-node-2 osd[5361]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:31 ceph-node-5 radosgw[6458]: DEBUG: OSD rebalancing completed
2025-02-24 10:50:45 ceph-node-1 mds[9159]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:00 ceph-node-5 mds[5747]: ERROR: Slow request detected on OSD 4.3
2025-02-24 10:50:47 ceph-node-2 client[6557]: DEBUG: OSD rebalancing completed
2025-02-24 10:51:24 ceph-node-5 osd[2183]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:30 ceph-node-1 mgr[4207]: NOTICE: OSD rebalancing completed
2025-02-24 10:50:41 ceph-node-3 mds[4075]: INFO: OSD disk space utilization: 70%
2025-02-24 10:51:29 ceph-node-5 client[4621]: CRITICAL: Monitor down, unable to connect to quorum
2025-02-24 10:50:40 ceph-node-4 mgr[4689]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:51:29 ceph-node-2 mgr[3331]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:11 ceph-node-1 client[6345]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:13 ceph-node-3 osd[4527]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:21 ceph-node-4 client[5144]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:34 ceph-node-2 mgr[3810]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:51:07 ceph-node-1 client[3051]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:40 ceph-node-5 mds[8605]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:50:31 ceph-node-1 radosgw[9857]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:23 ceph-node-1 mon[5808]: INFO: PG 1.2 replicated successfully
2025-02-24 10:50:33 ceph-node-4 radosgw[7634]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:19 ceph-node-4 osd[5500]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:54 ceph-node-5 mon[9810]: CRITICAL: OSD 3.1 marked down due to heartbeat failure
2025-02-24 10:51:28 ceph-node-1 mds[3759]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:50 ceph-node-5 mds[8798]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:26 ceph-node-4 osd[9563]: CRITICAL: Monitor mon1 failed to reach quorum
2025-02-24 10:51:09 ceph-node-5 client[2179]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:47 ceph-node-1 radosgw[6705]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:52 ceph-node-5 mds[4154]: NOTICE: Monitor map has been updated
2025-02-24 10:50:31 ceph-node-3 mds[6375]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:20 ceph-node-4 radosgw[3478]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:50:38 ceph-node-2 mon[3490]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:51:16 ceph-node-1 mon[6006]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:47 ceph-node-4 mon[1920]: INFO: PG 1.2 replicated successfully
2025-02-24 10:51:03 ceph-node-5 mds[9512]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:54 ceph-node-5 osd[3149]: DEBUG: Monitor map has been updated
2025-02-24 10:50:37 ceph-node-1 mon[2367]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:51:23 ceph-node-3 mgr[8381]: CRITICAL: Uncorrectable ECC error detected in MDS cache
2025-02-24 10:50:37 ceph-node-3 radosgw[1226]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:50:56 ceph-node-4 client[3240]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:51:25 ceph-node-4 osd[5265]: CRITICAL: Journal write failure detected on OSD node
2025-02-24 10:50:34 ceph-node-2 mon[8068]: NOTICE: OSD rebalancing completed
2025-02-24 10:51:30 ceph-node-3 client[7742]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:24 ceph-node-2 client[4662]: CRITICAL: Monitor down, unable to connect to quorum
2025-02-24 10:50:46 ceph-node-1 mgr[7992]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:51:31 ceph-node-5 mds[8989]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:50:39 ceph-node-3 mgr[3059]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:50:47 ceph-node-3 radosgw[5634]: NOTICE: MGR module loaded successfully
2025-02-24 10:51:28 ceph-node-3 mds[1333]: NOTICE: OSD rebalancing completed
2025-02-24 10:50:33 ceph-node-5 osd[5133]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:50:56 ceph-node-1 osd[5367]: INFO: Monitor map has been updated
2025-02-24 10:50:48 ceph-node-1 mon[3495]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:50:40 ceph-node-3 mgr[7702]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:05 ceph-node-1 mon[5936]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:32 ceph-node-3 mgr[7468]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:50:41 ceph-node-3 osd[6936]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:39 ceph-node-1 mgr[2532]: NOTICE: MGR module loaded successfully
2025-02-24 10:51:24 ceph-node-1 radosgw[6665]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:04 ceph-node-3 osd[1694]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:50:31 ceph-node-2 mds[5865]: INFO: OSD rebalancing completed
2025-02-24 10:50:39 ceph-node-4 osd[7128]: DEBUG: OSD rebalancing completed
2025-02-24 10:50:54 ceph-node-4 osd[1805]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:50:51 ceph-node-2 mon[4000]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:01 ceph-node-2 mds[2733]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:49 ceph-node-1 mds[6254]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:51:28 ceph-node-1 client[7356]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:39 ceph-node-5 client[4417]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:31 ceph-node-3 osd[1104]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:10 ceph-node-5 client[6517]: NOTICE: MGR module loaded successfully
2025-02-24 10:51:04 ceph-node-3 mon[7930]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:41 ceph-node-2 mds[8314]: INFO: Monitor map has been updated
2025-02-24 10:51:01 ceph-node-2 client[1134]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:12 ceph-node-1 client[7833]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:50:44 ceph-node-2 mon[4336]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:30 ceph-node-3 radosgw[9263]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:51:29 ceph-node-1 radosgw[6313]: INFO: OSD disk space utilization: 70%
2025-02-24 10:51:13 ceph-node-1 radosgw[7964]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:50:41 ceph-node-3 mon[2336]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:03 ceph-node-5 mon[2057]: NOTICE: Data replication completed for object pool
2025-02-24 10:50:45 ceph-node-1 mon[3784]: DEBUG: Monitor map has been updated
2025-02-24 10:51:07 ceph-node-4 radosgw[4876]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:50:56 ceph-node-3 client[5598]: CRITICAL: Slow request detected on OSD 4.3
2025-02-24 10:50:56 ceph-node-3 radosgw[5292]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:50:47 ceph-node-1 mon[1324]: ERROR: MDS failure detected: metadata inconsistency
2025-02-24 10:51:05 ceph-node-4 mgr[6739]: CRITICAL: Disk failure reported on ceph-node-4
2025-02-24 10:51:12 ceph-node-5 radosgw[5903]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:05 ceph-node-4 mon[2257]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:51:15 ceph-node-1 mds[5149]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:50:32 ceph-node-3 radosgw[7078]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:23 ceph-node-1 mon[8647]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:48 ceph-node-3 radosgw[3872]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:50:58 ceph-node-5 radosgw[2048]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:51:14 ceph-node-2 mds[1784]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:50:45 ceph-node-3 mon[6203]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:50:31 ceph-node-4 client[5391]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:51:05 ceph-node-4 radosgw[5472]: CRITICAL: Network partition detected between OSD nodes
2025-02-24 10:51:25 ceph-node-1 mds[2195]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:50:50 ceph-node-2 mon[9703]: INFO: OSD rebalancing completed
2025-02-24 10:51:28 ceph-node-2 radosgw[8233]: NOTICE: OSD rebalancing completed
2025-02-24 10:51:27 ceph-node-5 mgr[1759]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:56 ceph-node-2 radosgw[8263]: DEBUG: MGR module loaded successfully
2025-02-24 10:50:50 ceph-node-2 mgr[1793]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:04 ceph-node-2 mon[1303]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:51:23 ceph-node-2 radosgw[3245]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:37 ceph-node-4 mgr[3199]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:30 ceph-node-4 mon[1941]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:08 ceph-node-5 osd[3867]: NOTICE: Monitor map has been updated
2025-02-24 10:51:08 ceph-node-4 mds[5163]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:48 ceph-node-1 mgr[7330]: INFO: OSD rebalancing completed
2025-02-24 10:51:08 ceph-node-5 mon[2120]: INFO: Monitor map has been updated
2025-02-24 10:51:12 ceph-node-4 mon[6129]: WARNING: Slow request detected on OSD 4.3
2025-02-24 10:51:22 ceph-node-4 mgr[6940]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:51:24 ceph-node-4 mds[9638]: NOTICE: Monitor map has been updated
2025-02-24 10:50:45 ceph-node-2 radosgw[3194]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:05 ceph-node-5 mds[1619]: INFO: Monitor map has been updated
2025-02-24 10:50:49 ceph-node-5 client[7483]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:50:52 ceph-node-2 mon[5063]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:50:40 ceph-node-2 mds[4791]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:26 ceph-node-2 client[6330]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:02 ceph-node-1 radosgw[2647]: CRITICAL: Uncorrectable ECC error detected in MDS cache
2025-02-24 10:51:30 ceph-node-5 mds[9144]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:25 ceph-node-1 mds[6457]: INFO: Monitor map has been updated
2025-02-24 10:50:52 ceph-node-4 mon[2091]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:39 ceph-node-1 radosgw[8950]: CRITICAL: Authentication failure in RADOS Gateway
2025-02-24 10:50:40 ceph-node-4 radosgw[5886]: NOTICE: Monitor map has been updated
2025-02-24 10:50:47 ceph-node-4 osd[9351]: INFO: PG 1.2 replicated successfully
2025-02-24 10:51:00 ceph-node-1 mds[7143]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:32 ceph-node-4 client[7987]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:23 ceph-node-5 mon[6839]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:35 ceph-node-1 radosgw[9475]: INFO: Data replication completed for object pool
2025-02-24 10:50:59 ceph-node-2 osd[7225]: DEBUG: Data replication completed for object pool
2025-02-24 10:50:48 ceph-node-1 mon[2116]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:50:48 ceph-node-2 mon[1980]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:26 ceph-node-4 mgr[9458]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:50:49 ceph-node-4 osd[3770]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:51:06 ceph-node-2 radosgw[3845]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:57 ceph-node-5 osd[7795]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:50:42 ceph-node-4 mgr[8943]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:16 ceph-node-5 radosgw[9742]: INFO: PG 1.2 replicated successfully
2025-02-24 10:51:24 ceph-node-5 mds[5596]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:51:20 ceph-node-3 radosgw[2560]: NOTICE: Data replication completed for object pool
2025-02-24 10:51:24 ceph-node-1 radosgw[1862]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:51:02 ceph-node-1 radosgw[6631]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:30 ceph-node-1 mgr[8290]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:51:28 ceph-node-3 mds[6009]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:28 ceph-node-2 osd[6262]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:50:40 ceph-node-3 mgr[8217]: INFO: Monitor map has been updated
2025-02-24 10:51:10 ceph-node-3 client[1932]: NOTICE: OSD rebalancing completed
2025-02-24 10:50:42 ceph-node-5 mon[1638]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:20 ceph-node-1 radosgw[6150]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:51:29 ceph-node-1 osd[6201]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:51:16 ceph-node-1 mds[9382]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:51:11 ceph-node-3 radosgw[1306]: CRITICAL: CRITICAL: PG 2.1 stuck in inconsistent state
2025-02-24 10:50:58 ceph-node-3 mon[3508]: INFO: OSD disk space utilization: 70%
2025-02-24 10:50:50 ceph-node-2 mgr[1264]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:45 ceph-node-1 mon[7553]: DEBUG: OSD rebalancing completed
2025-02-24 10:51:31 ceph-node-5 mon[3360]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:50:31 ceph-node-5 client[8567]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:51:03 ceph-node-4 client[2494]: INFO: Data replication completed for object pool
2025-02-24 10:51:16 ceph-node-5 mds[9357]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:37 ceph-node-5 mds[8099]: NOTICE: MGR module loaded successfully
2025-02-24 10:50:54 ceph-node-3 client[9500]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:26 ceph-node-2 osd[5551]: NOTICE: Data replication completed for object pool
2025-02-24 10:51:12 ceph-node-5 osd[8607]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:51:26 ceph-node-4 radosgw[9502]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:33 ceph-node-4 client[3559]: DEBUG: OSD rebalancing completed
2025-02-24 10:51:28 ceph-node-1 osd[4428]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:10 ceph-node-3 radosgw[3797]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:50:40 ceph-node-1 radosgw[5300]: NOTICE: OSD rebalancing completed
2025-02-24 10:51:15 ceph-node-1 mds[8439]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:30 ceph-node-3 mds[8348]: WARNING: Cluster status: HEALTH_WARN
2025-02-24 10:51:00 ceph-node-2 mon[7514]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:50:52 ceph-node-4 client[2359]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:10 ceph-node-1 radosgw[8346]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:50:51 ceph-node-4 osd[2524]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:51:27 ceph-node-1 mon[8508]: CRITICAL: Disk failure reported on ceph-node-4
2025-02-24 10:51:27 ceph-node-2 mon[6351]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:35 ceph-node-1 mon[7874]: INFO: OSD rebalancing completed
2025-02-24 10:51:25 ceph-node-5 osd[5207]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:50:49 ceph-node-1 mgr[7869]: NOTICE: Monitor map has been updated
2025-02-24 10:51:31 ceph-node-2 osd[9022]: INFO: OSD rebalancing completed
2025-02-24 10:51:04 ceph-node-5 mds[1094]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:43 ceph-node-2 radosgw[3746]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:35 ceph-node-1 radosgw[7533]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:50:31 ceph-node-2 mgr[3554]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:33 ceph-node-1 mon[4678]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:03 ceph-node-5 mon[4133]: WARNING: Journal write failure detected on OSD node
2025-02-24 10:51:26 ceph-node-1 client[9048]: WARNING: Uncorrectable ECC error detected in MDS cache
2025-02-24 10:51:09 ceph-node-1 radosgw[5617]: WARNING: OSD 3.1 marked down due to heartbeat failure
2025-02-24 10:50:33 ceph-node-4 client[8262]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:07 ceph-node-4 mgr[1497]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:51:08 ceph-node-1 client[2587]: DEBUG: Monitor map has been updated
2025-02-24 10:50:56 ceph-node-5 client[1300]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:15 ceph-node-4 osd[8400]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:51:11 ceph-node-3 osd[6622]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:49 ceph-node-2 mgr[8514]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:51:22 ceph-node-2 mon[5423]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:50:55 ceph-node-1 osd[5237]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:50:55 ceph-node-3 mgr[4531]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:51:31 ceph-node-5 mgr[7396]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:50:32 ceph-node-5 radosgw[6905]: NOTICE: OSD rebalancing completed
2025-02-24 10:50:39 ceph-node-1 osd[3685]: DEBUG: OSD rebalancing completed
2025-02-24 10:51:03 ceph-node-5 mon[4476]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:51:15 ceph-node-1 mon[2686]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:27 ceph-node-1 mgr[3339]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:50:35 ceph-node-4 client[4574]: INFO: OSD disk space utilization: 70%
2025-02-24 10:50:57 ceph-node-5 client[4925]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:57 ceph-node-1 client[1522]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:58 ceph-node-5 mgr[7455]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:30 ceph-node-1 client[1903]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:50 ceph-node-3 radosgw[8243]: NOTICE: OSD rebalancing completed
2025-02-24 10:50:46 ceph-node-1 mds[4185]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:51:26 ceph-node-5 osd[7002]: NOTICE: MGR module loaded successfully
2025-02-24 10:51:20 ceph-node-3 mgr[8674]: WARNING: CRITICAL: Data loss detected on pool 1
2025-02-24 10:50:47 ceph-node-2 mgr[4788]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:50:43 ceph-node-1 osd[6670]: INFO: PG 1.2 replicated successfully
2025-02-24 10:50:40 ceph-node-2 radosgw[1614]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:51:19 ceph-node-3 mon[5573]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:40 ceph-node-1 client[7436]: NOTICE: MGR module loaded successfully
2025-02-24 10:50:49 ceph-node-5 mon[3820]: NOTICE: OSD rebalancing completed
2025-02-24 10:50:37 ceph-node-2 mon[6088]: INFO: PG 1.2 replicated successfully
2025-02-24 10:51:06 ceph-node-4 client[8266]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:24 ceph-node-2 radosgw[9107]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:31 ceph-node-5 mgr[4113]: WARNING: Monitor down, unable to connect to quorum
2025-02-24 10:50:47 ceph-node-4 mon[1126]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:50:52 ceph-node-4 osd[9131]: NOTICE: MGR module loaded successfully
2025-02-24 10:50:45 ceph-node-5 client[3072]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:51:11 ceph-node-2 client[4865]: INFO: Data replication completed for object pool
2025-02-24 10:51:04 ceph-node-2 client[9195]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:51:20 ceph-node-3 mon[9418]: DEBUG: OSD rebalancing completed
2025-02-24 10:51:22 ceph-node-3 radosgw[5161]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:23 ceph-node-1 client[2562]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:47 ceph-node-3 mds[5966]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:14 ceph-node-2 client[2317]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:51 ceph-node-4 mds[1948]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:19 ceph-node-4 mgr[8222]: INFO: OSD rebalancing completed
2025-02-24 10:50:34 ceph-node-3 mds[7168]: INFO: Data replication completed for object pool
2025-02-24 10:51:16 ceph-node-4 osd[1504]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:51:07 ceph-node-1 mds[5181]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:51:08 ceph-node-4 radosgw[7846]: INFO: PG 1.2 replicated successfully
2025-02-24 10:50:58 ceph-node-3 mgr[3916]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:50:53 ceph-node-5 mon[5816]: WARNING: MDS failure detected: metadata inconsistency
2025-02-24 10:51:08 ceph-node-2 client[1315]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:18 ceph-node-4 mds[8090]: INFO: Monitor map has been updated
2025-02-24 10:50:43 ceph-node-5 osd[1807]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:24 ceph-node-2 mgr[2236]: NOTICE: Monitor map has been updated
2025-02-24 10:50:32 ceph-node-2 client[3365]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:51:11 ceph-node-3 mon[8365]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:54 ceph-node-5 mds[4633]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:51:05 ceph-node-5 mgr[9525]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:04 ceph-node-5 mgr[6368]: DEBUG: Monitor map has been updated
2025-02-24 10:51:22 ceph-node-3 mds[9986]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:29 ceph-node-3 mon[8049]: DEBUG: OSD rebalancing completed
2025-02-24 10:51:13 ceph-node-4 mds[2299]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:51:12 ceph-node-1 mgr[4564]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:49 ceph-node-1 radosgw[1719]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:40 ceph-node-3 mon[6879]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:48 ceph-node-1 mds[2079]: INFO: OSD disk space utilization: 70%
2025-02-24 10:50:58 ceph-node-4 mds[4756]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:50:51 ceph-node-5 client[4814]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:50:51 ceph-node-1 radosgw[2993]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:51:05 ceph-node-1 client[6417]: INFO: MGR module loaded successfully
2025-02-24 10:51:11 ceph-node-3 radosgw[6456]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:51:23 ceph-node-2 client[7527]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:50:44 ceph-node-1 mon[2370]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:51:17 ceph-node-1 mds[5973]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:51:06 ceph-node-4 radosgw[9342]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:48 ceph-node-4 radosgw[3960]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:12 ceph-node-2 mon[8523]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:50:36 ceph-node-4 mgr[6695]: DEBUG: OSD rebalancing completed
2025-02-24 10:50:53 ceph-node-5 mgr[3118]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:10 ceph-node-1 radosgw[5487]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:44 ceph-node-5 mon[5720]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:51:27 ceph-node-4 mgr[7611]: DEBUG: MGR module loaded successfully
2025-02-24 10:50:56 ceph-node-3 mds[7816]: CRITICAL: Cluster status: HEALTH_WARN
2025-02-24 10:50:32 ceph-node-2 mon[2940]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:31 ceph-node-1 osd[1396]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:09 ceph-node-5 osd[4427]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:10 ceph-node-3 radosgw[4873]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:51:31 ceph-node-4 mon[9560]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:50:34 ceph-node-4 mgr[3879]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:26 ceph-node-4 osd[6500]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:50:51 ceph-node-3 mon[4398]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:50:54 ceph-node-2 osd[8202]: CRITICAL: Data corruption detected in object pool
2025-02-24 10:51:00 ceph-node-1 mgr[2612]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:19 ceph-node-5 mgr[6143]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:51:16 ceph-node-4 mds[8179]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:50:52 ceph-node-1 mgr[9361]: WARNING: CRITICAL: Data loss detected on pool 1
2025-02-24 10:51:29 ceph-node-5 mon[1336]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:22 ceph-node-4 osd[8362]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:19 ceph-node-2 mon[6911]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:34 ceph-node-5 mds[8588]: INFO: Data replication completed for object pool
2025-02-24 10:50:42 ceph-node-1 radosgw[4358]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:50:42 ceph-node-2 osd[8106]: NOTICE: Data replication completed for object pool
2025-02-24 10:51:18 ceph-node-1 mds[3081]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:31 ceph-node-1 mds[7054]: ERROR: Client connection timeout detected
2025-02-24 10:51:02 ceph-node-5 osd[9368]: ERROR: MDS failure detected: metadata inconsistency
2025-02-24 10:50:40 ceph-node-2 radosgw[2657]: DEBUG: Monitor map has been updated
2025-02-24 10:50:43 ceph-node-2 mds[7019]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:50:39 ceph-node-2 mds[9311]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:50:45 ceph-node-3 radosgw[4838]: WARNING: Slow request detected on OSD 4.3
2025-02-24 10:51:01 ceph-node-5 mon[8361]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:51:12 ceph-node-4 mon[4746]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:51:27 ceph-node-2 mon[4531]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:50:59 ceph-node-5 mds[4303]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:02 ceph-node-4 client[1634]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:50:40 ceph-node-2 osd[2784]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:51:21 ceph-node-2 radosgw[7305]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:51:07 ceph-node-4 client[8964]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:50:59 ceph-node-2 osd[5542]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:51:24 ceph-node-3 mds[5424]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:50:37 ceph-node-4 radosgw[8245]: ERROR: Monitor mon1 failed to reach quorum
2025-02-24 10:51:14 ceph-node-1 osd[4885]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:23 ceph-node-5 client[1589]: CRITICAL: Authentication failure in RADOS Gateway
2025-02-24 10:51:13 ceph-node-2 mgr[9532]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:51:06 ceph-node-3 mds[4062]: WARNING: Disk failure reported on ceph-node-4
2025-02-24 10:50:37 ceph-node-2 mgr[5821]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:50:54 ceph-node-1 osd[6581]: INFO: PG 1.2 replicated successfully
2025-02-24 10:51:20 ceph-node-3 client[2800]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:50 ceph-node-1 osd[3689]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:50:38 ceph-node-4 mds[6818]: DEBUG: Data replication completed for object pool
2025-02-24 10:50:52 ceph-node-5 client[8251]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:00 ceph-node-2 mgr[1831]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:51:00 ceph-node-2 radosgw[5680]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:50:38 ceph-node-3 mon[6989]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:17 ceph-node-2 mon[6385]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:51:17 ceph-node-1 osd[8235]: CRITICAL: CRITICAL: PG 2.1 stuck in inconsistent state
2025-02-24 10:51:21 ceph-node-1 client[2349]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:51:30 ceph-node-2 radosgw[5999]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:50:44 ceph-node-5 radosgw[4159]: DEBUG: OSD rebalancing completed
2025-02-24 10:51:01 ceph-node-5 client[8764]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:36 ceph-node-4 mgr[2734]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:41 ceph-node-1 radosgw[5952]: INFO: Data replication completed for object pool
2025-02-24 10:50:46 ceph-node-1 mds[7254]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:50:55 ceph-node-5 mds[7716]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:52 ceph-node-1 mds[9360]: DEBUG: Data replication completed for object pool
2025-02-24 10:50:54 ceph-node-4 mgr[9531]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:23 ceph-node-1 mgr[6139]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:56 ceph-node-4 osd[4239]: CRITICAL: MDS failure detected: metadata inconsistency
2025-02-24 10:51:06 ceph-node-1 mgr[4779]: CRITICAL: CRITICAL: PG 2.1 stuck in inconsistent state
2025-02-24 10:51:29 ceph-node-5 mgr[4980]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:51:26 ceph-node-2 mgr[4972]: ERROR: Uncorrectable ECC error detected in MDS cache
2025-02-24 10:50:55 ceph-node-3 mon[2722]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:51:25 ceph-node-4 osd[8396]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:03 ceph-node-1 mgr[7525]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:01 ceph-node-1 mds[4080]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:24 ceph-node-5 mgr[1337]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:51:26 ceph-node-4 client[6750]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:50:32 ceph-node-4 client[1549]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:24 ceph-node-2 mds[2003]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:51:19 ceph-node-5 radosgw[5696]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:51:18 ceph-node-2 mds[7106]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:51:26 ceph-node-2 radosgw[9448]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:52 ceph-node-3 mgr[8509]: ERROR: MDS failure detected: metadata inconsistency
2025-02-24 10:50:41 ceph-node-1 client[8320]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:17 ceph-node-4 client[6363]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:06 ceph-node-5 mon[5650]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:14 ceph-node-3 mgr[8430]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:50:42 ceph-node-3 mon[6336]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:37 ceph-node-5 osd[5838]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:08 ceph-node-4 mon[9395]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:46 ceph-node-1 osd[1147]: ERROR: OSD 3.1 marked down due to heartbeat failure
2025-02-24 10:50:55 ceph-node-1 radosgw[2285]: ERROR: MDS failure detected: metadata inconsistency
2025-02-24 10:51:29 ceph-node-4 radosgw[4862]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:50:45 ceph-node-1 client[8170]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:51 ceph-node-4 radosgw[3058]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:50:47 ceph-node-5 osd[2613]: INFO: MGR module loaded successfully
2025-02-24 10:51:01 ceph-node-2 mgr[7585]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:31 ceph-node-1 radosgw[8276]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:51:20 ceph-node-5 mon[5068]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:50:41 ceph-node-4 client[2149]: CRITICAL: Client connection timeout detected
2025-02-24 10:51:04 ceph-node-1 client[8869]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:50:43 ceph-node-1 mds[4708]: WARNING: Authentication failure in RADOS Gateway
2025-02-24 10:50:49 ceph-node-3 mds[1953]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:50:49 ceph-node-2 mon[3655]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:50:41 ceph-node-1 radosgw[9276]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:09 ceph-node-3 client[2980]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:51:09 ceph-node-1 radosgw[7510]: CRITICAL: Monitor mon1 failed to reach quorum
2025-02-24 10:51:13 ceph-node-4 radosgw[8739]: INFO: OSD rebalancing completed
2025-02-24 10:51:21 ceph-node-4 radosgw[7093]: NOTICE: Monitor map has been updated
2025-02-24 10:50:57 ceph-node-5 mds[4862]: DEBUG: Data replication completed for object pool
2025-02-24 10:50:32 ceph-node-4 mon[3406]: INFO: MGR module loaded successfully
2025-02-24 10:50:55 ceph-node-1 mgr[2351]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:13 ceph-node-1 osd[8530]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:02 ceph-node-5 mon[3570]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:39 ceph-node-5 mon[8945]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:51:17 ceph-node-2 client[1489]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:51:23 ceph-node-5 mgr[8877]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:50:39 ceph-node-2 osd[2187]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:03 ceph-node-2 mon[7876]: NOTICE: Data replication completed for object pool
2025-02-24 10:51:23 ceph-node-2 client[3548]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:25 ceph-node-4 radosgw[9556]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:50:47 ceph-node-3 client[8737]: INFO: OSD rebalancing completed
2025-02-24 10:50:40 ceph-node-1 mgr[6204]: INFO: OSD disk space utilization: 70%
2025-02-24 10:51:25 ceph-node-4 mon[7945]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:50:40 ceph-node-5 radosgw[4890]: INFO: MGR module loaded successfully
2025-02-24 10:50:44 ceph-node-2 radosgw[9449]: ERROR: Uncorrectable ECC error detected in MDS cache
2025-02-24 10:50:55 ceph-node-2 client[8585]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:04 ceph-node-2 mgr[9197]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:44 ceph-node-3 radosgw[6374]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:51:02 ceph-node-5 mds[1663]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:27 ceph-node-4 mds[9048]: INFO: PG 1.2 replicated successfully
2025-02-24 10:50:59 ceph-node-4 client[5781]: NOTICE: Data replication completed for object pool
2025-02-24 10:50:46 ceph-node-4 client[7616]: INFO: Data replication completed for object pool
2025-02-24 10:50:44 ceph-node-1 client[9915]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:57 ceph-node-1 radosgw[6946]: ERROR: Journal write failure detected on OSD node
2025-02-24 10:51:09 ceph-node-3 mon[1131]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:31 ceph-node-3 osd[2051]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:12 ceph-node-2 client[7128]: CRITICAL: Journal write failure detected on OSD node
2025-02-24 10:50:32 ceph-node-1 mon[5468]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:23 ceph-node-2 osd[5777]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:51:18 ceph-node-4 mds[2376]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:50:48 ceph-node-5 mds[7587]: NOTICE: OSD rebalancing completed
2025-02-24 10:50:44 ceph-node-4 client[1428]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:51:16 ceph-node-3 mds[1466]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:59 ceph-node-2 mon[9177]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:50:46 ceph-node-4 mds[4736]: INFO: PG 1.2 replicated successfully
2025-02-24 10:50:44 ceph-node-4 mgr[8687]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:02 ceph-node-2 client[8609]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:26 ceph-node-4 client[5853]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:50:52 ceph-node-2 osd[8866]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:50:55 ceph-node-1 mds[8269]: INFO: Monitor map has been updated
2025-02-24 10:50:41 ceph-node-2 client[3263]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:50:54 ceph-node-1 mds[4291]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:50:34 ceph-node-4 client[6840]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:50:36 ceph-node-3 client[6923]: NOTICE: MGR module loaded successfully
2025-02-24 10:50:43 ceph-node-4 radosgw[8949]: NOTICE: Monitor map has been updated
2025-02-24 10:50:55 ceph-node-5 mgr[1187]: INFO: MGR module loaded successfully
2025-02-24 10:50:32 ceph-node-5 osd[4035]: WARNING: CRITICAL: PG 2.1 stuck in inconsistent state
2025-02-24 10:51:09 ceph-node-3 client[4938]: CRITICAL: OSD 3.1 marked down due to heartbeat failure
2025-02-24 10:51:16 ceph-node-3 client[9962]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:50:59 ceph-node-1 client[9124]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:50:43 ceph-node-5 mgr[2563]: INFO: OSD rebalancing completed
2025-02-24 10:50:44 ceph-node-2 mon[6697]: INFO: OSD rebalancing completed
2025-02-24 10:50:45 ceph-node-2 mds[5254]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:11 ceph-node-4 client[3081]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:51:21 ceph-node-3 client[7515]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:51:27 ceph-node-1 osd[7264]: WARNING: Disk failure reported on ceph-node-4
2025-02-24 10:50:46 ceph-node-2 client[4449]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:50:46 ceph-node-4 mon[3281]: DEBUG: Monitor map has been updated
2025-02-24 10:50:54 ceph-node-4 radosgw[6651]: DEBUG: OSD rebalancing completed
2025-02-24 10:50:48 ceph-node-2 osd[4421]: INFO: Monitor map has been updated
2025-02-24 10:50:51 ceph-node-5 mon[4767]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:45 ceph-node-3 mds[8129]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:13 ceph-node-4 mds[6757]: INFO: OSD disk space utilization: 70%
2025-02-24 10:51:07 ceph-node-2 mgr[2619]: ERROR: Monitor down, unable to connect to quorum
2025-02-24 10:50:31 ceph-node-4 mgr[1025]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:50:36 ceph-node-3 mds[2964]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:50:53 ceph-node-2 osd[5534]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:49 ceph-node-1 mgr[3501]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:17 ceph-node-1 mon[1710]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:50:41 ceph-node-4 mds[5753]: INFO: OSD rebalancing completed
2025-02-24 10:51:08 ceph-node-2 mgr[7158]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:29 ceph-node-3 mds[3629]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:22 ceph-node-1 client[7323]: NOTICE: OSD rebalancing completed
2025-02-24 10:51:25 ceph-node-4 mgr[3392]: WARNING: CRITICAL: PG 2.1 stuck in inconsistent state
2025-02-24 10:51:12 ceph-node-5 radosgw[2588]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:35 ceph-node-5 osd[4848]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:50:59 ceph-node-2 client[8397]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:50:56 ceph-node-2 mds[9677]: NOTICE: OSD rebalancing completed
2025-02-24 10:51:23 ceph-node-3 radosgw[4395]: INFO: PG 1.2 replicated successfully
2025-02-24 10:50:49 ceph-node-2 mgr[3279]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:38 ceph-node-4 osd[2048]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:55 ceph-node-3 mds[8545]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:46 ceph-node-4 mds[6224]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:04 ceph-node-1 client[2608]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:31 ceph-node-4 mgr[8638]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:50:45 ceph-node-4 mgr[1738]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:50:54 ceph-node-4 radosgw[4736]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:51:26 ceph-node-1 osd[5859]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:50:52 ceph-node-4 mds[5947]: DEBUG: OSD rebalancing completed
2025-02-24 10:51:01 ceph-node-3 mds[8245]: INFO: PG 1.2 replicated successfully
2025-02-24 10:51:09 ceph-node-5 mds[1241]: CRITICAL: Slow request detected on OSD 4.3
2025-02-24 10:51:14 ceph-node-4 mon[5108]: NOTICE: Monitor map has been updated
2025-02-24 10:51:03 ceph-node-2 radosgw[7461]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:50:48 ceph-node-2 osd[8153]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:50:41 ceph-node-2 osd[6793]: CRITICAL: CRITICAL: Data loss detected on pool 1
2025-02-24 10:51:14 ceph-node-2 radosgw[7212]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:03 ceph-node-1 osd[8537]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:51:13 ceph-node-4 client[5942]: CRITICAL: Journal write failure detected on OSD node
2025-02-24 10:51:26 ceph-node-3 osd[5137]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:50:58 ceph-node-4 osd[9931]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:14 ceph-node-3 osd[1767]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:08 ceph-node-4 client[3788]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:29 ceph-node-1 mds[7558]: WARNING: High I/O latency detected in radosgw service
2025-02-24 10:50:49 ceph-node-5 mgr[5857]: INFO: OSD rebalancing completed
2025-02-24 10:51:09 ceph-node-2 mgr[9495]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:15 ceph-node-3 radosgw[4997]: INFO: OSD disk space utilization: 70%
2025-02-24 10:51:31 ceph-node-5 osd[9413]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:51:02 ceph-node-4 mon[4899]: INFO: PG 1.2 replicated successfully
2025-02-24 10:51:22 ceph-node-3 client[6023]: INFO: OSD disk space utilization: 70%
2025-02-24 10:50:34 ceph-node-1 radosgw[3171]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:01 ceph-node-1 osd[8770]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:51:23 ceph-node-4 mds[9096]: DEBUG: MGR module loaded successfully
2025-02-24 10:50:53 ceph-node-2 client[9252]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:37 ceph-node-4 client[9372]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:56 ceph-node-1 radosgw[3330]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:10 ceph-node-2 mds[4630]: DEBUG: Monitor map has been updated
2025-02-24 10:51:11 ceph-node-3 radosgw[6163]: ERROR: Uncorrectable ECC error detected in MDS cache
2025-02-24 10:51:18 ceph-node-2 mds[3888]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:51:19 ceph-node-4 mon[6071]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:14 ceph-node-1 client[7479]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:50:46 ceph-node-1 osd[3080]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:51:13 ceph-node-5 client[9127]: NOTICE: Data replication completed for object pool
2025-02-24 10:51:25 ceph-node-3 mon[8599]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:43 ceph-node-1 mgr[2302]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:50:33 ceph-node-1 osd[9399]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:55 ceph-node-2 mon[5942]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:50:55 ceph-node-2 osd[8685]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:09 ceph-node-5 mds[8313]: INFO: OSD disk space utilization: 70%
2025-02-24 10:51:31 ceph-node-4 mds[9828]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:56 ceph-node-5 mon[3041]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:22 ceph-node-5 osd[7551]: CRITICAL: CRITICAL: Data loss detected on pool 1
2025-02-24 10:50:51 ceph-node-1 osd[3708]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:18 ceph-node-2 radosgw[7276]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:00 ceph-node-4 client[1949]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:13 ceph-node-4 mds[2336]: NOTICE: OSD rebalancing completed
2025-02-24 10:50:43 ceph-node-3 mon[7343]: WARNING: Monitor mon1 failed to reach quorum
2025-02-24 10:51:05 ceph-node-1 client[4229]: CRITICAL: Authentication failure in RADOS Gateway
2025-02-24 10:51:13 ceph-node-5 mds[9219]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:50:42 ceph-node-4 osd[5261]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:51:02 ceph-node-2 mgr[9223]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:51:02 ceph-node-3 mgr[9302]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:22 ceph-node-1 mgr[1734]: NOTICE: Monitor map has been updated
2025-02-24 10:51:09 ceph-node-1 mgr[7452]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:50:56 ceph-node-2 osd[3664]: NOTICE: MGR module loaded successfully
2025-02-24 10:50:52 ceph-node-2 mds[2515]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:51 ceph-node-4 mon[6544]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:51:05 ceph-node-4 mds[9614]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:51:23 ceph-node-5 client[6399]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:50:48 ceph-node-5 radosgw[9707]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:50:37 ceph-node-1 mds[9389]: INFO: Monitor map has been updated
2025-02-24 10:50:42 ceph-node-4 mds[7264]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:15 ceph-node-1 client[4518]: INFO: OSD disk space utilization: 70%
2025-02-24 10:51:11 ceph-node-5 mon[4738]: INFO: PG 1.2 replicated successfully
2025-02-24 10:51:13 ceph-node-3 radosgw[3687]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:50:42 ceph-node-1 radosgw[6176]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:45 ceph-node-5 osd[7350]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:50:57 ceph-node-3 radosgw[6417]: CRITICAL: CRITICAL: PG 2.1 stuck in inconsistent state
2025-02-24 10:50:55 ceph-node-5 mgr[1763]: NOTICE: MGR module loaded successfully
2025-02-24 10:51:05 ceph-node-5 mon[3561]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:13 ceph-node-1 mgr[6630]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:51:25 ceph-node-5 client[4211]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:33 ceph-node-4 client[6259]: INFO: MGR module loaded successfully
2025-02-24 10:50:48 ceph-node-4 mgr[1751]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:51 ceph-node-2 mon[7125]: ERROR: Client connection timeout detected
2025-02-24 10:51:21 ceph-node-2 mgr[6432]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:17 ceph-node-4 radosgw[2653]: NOTICE: Monitor map has been updated
2025-02-24 10:51:02 ceph-node-2 osd[3931]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:11 ceph-node-2 mgr[7886]: ERROR: Disk failure reported on ceph-node-4
2025-02-24 10:50:33 ceph-node-4 radosgw[7226]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:29 ceph-node-3 mds[8255]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:51:05 ceph-node-4 radosgw[5857]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:50:48 ceph-node-4 mds[4109]: INFO: OSD rebalancing completed
2025-02-24 10:50:55 ceph-node-5 mon[3980]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:51:05 ceph-node-2 mgr[3016]: NOTICE: MGR module loaded successfully
2025-02-24 10:50:41 ceph-node-4 client[6638]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:51:29 ceph-node-5 mon[8613]: INFO: OSD rebalancing completed
2025-02-24 10:51:01 ceph-node-3 osd[4025]: INFO: Monitor map has been updated
2025-02-24 10:51:18 ceph-node-1 radosgw[2203]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:51:19 ceph-node-4 client[1385]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:41 ceph-node-1 mgr[7345]: ERROR: High I/O latency detected in radosgw service
2025-02-24 10:51:06 ceph-node-2 mds[8288]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:18 ceph-node-4 osd[6075]: INFO: OSD rebalancing completed
2025-02-24 10:50:52 ceph-node-2 client[5588]: WARNING: Data corruption detected in object pool
2025-02-24 10:51:10 ceph-node-3 mgr[5671]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:28 ceph-node-2 radosgw[2030]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:50:33 ceph-node-1 osd[1325]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:48 ceph-node-4 client[7382]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:50:50 ceph-node-1 mon[4207]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:51:12 ceph-node-2 radosgw[8034]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:51:02 ceph-node-4 mgr[1249]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:58 ceph-node-1 mds[9249]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:29 ceph-node-3 radosgw[1503]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:51:04 ceph-node-5 mds[3833]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:51:15 ceph-node-3 client[1577]: INFO: Data replication completed for object pool
2025-02-24 10:50:46 ceph-node-1 mds[5231]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:50:56 ceph-node-5 radosgw[8410]: CRITICAL: Monitor down, unable to connect to quorum
2025-02-24 10:50:36 ceph-node-4 mds[9537]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:41 ceph-node-4 mgr[7894]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:17 ceph-node-3 mgr[1593]: DEBUG: MGR module loaded successfully
2025-02-24 10:50:51 ceph-node-3 mds[7367]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:25 ceph-node-2 client[6420]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:50:35 ceph-node-4 mon[9812]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:50:58 ceph-node-2 client[6404]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:51:23 ceph-node-3 mds[8833]: WARNING: Disk failure reported on ceph-node-4
2025-02-24 10:50:58 ceph-node-1 mds[8202]: INFO: MGR module loaded successfully
2025-02-24 10:51:07 ceph-node-5 mon[6813]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:02 ceph-node-3 osd[2658]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:50:40 ceph-node-4 mgr[4368]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:26 ceph-node-4 osd[8048]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:50 ceph-node-3 mgr[7830]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:51:07 ceph-node-4 mgr[2982]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:31 ceph-node-1 mgr[3257]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:50:33 ceph-node-4 radosgw[3730]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:50:55 ceph-node-1 mon[7753]: NOTICE: Data replication completed for object pool
2025-02-24 10:51:10 ceph-node-2 mgr[4267]: INFO: Data replication completed for object pool
2025-02-24 10:50:41 ceph-node-1 client[6400]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:03 ceph-node-4 mgr[9766]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:19 ceph-node-1 mgr[9051]: NOTICE: MGR module loaded successfully
2025-02-24 10:50:47 ceph-node-4 osd[3156]: WARNING: Cluster status: HEALTH_WARN
2025-02-24 10:51:00 ceph-node-4 mds[7784]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:51 ceph-node-4 osd[8596]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:38 ceph-node-4 mds[7814]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:00 ceph-node-3 mon[8608]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:33 ceph-node-2 radosgw[3832]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:30 ceph-node-4 mds[5583]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:50:49 ceph-node-3 mds[9965]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:45 ceph-node-5 client[4964]: NOTICE: OSD rebalancing completed
2025-02-24 10:50:40 ceph-node-1 mgr[5023]: CRITICAL: Network partition detected between OSD nodes
2025-02-24 10:50:35 ceph-node-1 mds[7618]: WARNING: MDS failure detected: metadata inconsistency
2025-02-24 10:51:14 ceph-node-4 mon[8559]: NOTICE: Monitor map has been updated
2025-02-24 10:51:23 ceph-node-3 mds[2493]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:51:15 ceph-node-5 client[8605]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:59 ceph-node-2 radosgw[7510]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:35 ceph-node-2 radosgw[5431]: NOTICE: MGR module loaded successfully
2025-02-24 10:51:28 ceph-node-5 mds[7739]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:44 ceph-node-2 radosgw[5569]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:27 ceph-node-5 mds[7308]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:31 ceph-node-2 mgr[9681]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:08 ceph-node-5 mgr[9692]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:04 ceph-node-4 mon[4977]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:50:56 ceph-node-2 mgr[7905]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:50:33 ceph-node-3 osd[5351]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:08 ceph-node-4 radosgw[5745]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:24 ceph-node-3 client[3047]: NOTICE: Data replication completed for object pool
2025-02-24 10:50:58 ceph-node-3 mds[9880]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:02 ceph-node-1 radosgw[1559]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:29 ceph-node-4 osd[7469]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:50:47 ceph-node-5 radosgw[9719]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:23 ceph-node-4 client[3278]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:50:48 ceph-node-3 mon[5612]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:38 ceph-node-5 mds[9343]: ERROR: CRITICAL: PG 2.1 stuck in inconsistent state
2025-02-24 10:51:00 ceph-node-5 osd[9542]: INFO: OSD disk space utilization: 70%
2025-02-24 10:50:33 ceph-node-5 mon[8565]: WARNING: Monitor mon1 failed to reach quorum
2025-02-24 10:51:25 ceph-node-1 mds[4087]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:31 ceph-node-4 mds[4983]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:28 ceph-node-5 mon[5594]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:12 ceph-node-3 mgr[7542]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:56 ceph-node-5 mgr[4201]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:57 ceph-node-1 mds[1295]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:51:31 ceph-node-3 osd[1165]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:51:15 ceph-node-1 mgr[4052]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:02 ceph-node-5 mds[8021]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:00 ceph-node-4 mgr[7488]: ERROR: Disk failure reported on ceph-node-4
2025-02-24 10:51:16 ceph-node-1 mgr[8177]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:50:31 ceph-node-3 mon[3370]: INFO: MGR module loaded successfully
2025-02-24 10:51:24 ceph-node-5 osd[8796]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:04 ceph-node-2 radosgw[6541]: NOTICE: Data replication completed for object pool
2025-02-24 10:50:37 ceph-node-3 mgr[3351]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:55 ceph-node-4 mgr[9877]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:40 ceph-node-3 radosgw[6970]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:50:34 ceph-node-2 osd[3084]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:50:43 ceph-node-4 mgr[4821]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:27 ceph-node-4 client[9240]: DEBUG: OSD rebalancing completed
2025-02-24 10:50:40 ceph-node-3 client[4369]: NOTICE: Data replication completed for object pool
2025-02-24 10:51:12 ceph-node-5 osd[4203]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:51:05 ceph-node-2 mon[4186]: INFO: MGR module loaded successfully
2025-02-24 10:50:44 ceph-node-2 client[7995]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:51:28 ceph-node-3 client[3103]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:50:33 ceph-node-5 mgr[4193]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:50:47 ceph-node-3 client[6335]: NOTICE: Data replication completed for object pool
2025-02-24 10:50:43 ceph-node-1 radosgw[5243]: DEBUG: Monitor map has been updated
2025-02-24 10:50:45 ceph-node-2 radosgw[3705]: CRITICAL: Monitor mon1 failed to reach quorum
2025-02-24 10:50:32 ceph-node-5 mgr[4199]: ERROR: CRITICAL: PG 2.1 stuck in inconsistent state
2025-02-24 10:50:53 ceph-node-4 mon[2814]: WARNING: CRITICAL: PG 2.1 stuck in inconsistent state
2025-02-24 10:51:22 ceph-node-3 client[4671]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:03 ceph-node-2 radosgw[7826]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:50:44 ceph-node-2 mgr[3344]: INFO: MGR module loaded successfully
2025-02-24 10:51:10 ceph-node-5 radosgw[4598]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:20 ceph-node-5 radosgw[4731]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:51:30 ceph-node-2 osd[3945]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:51:26 ceph-node-2 mon[3387]: DEBUG: Monitor map has been updated
2025-02-24 10:50:38 ceph-node-1 radosgw[1918]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:51:06 ceph-node-4 osd[2774]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:50:41 ceph-node-4 radosgw[8095]: INFO: OSD rebalancing completed
2025-02-24 10:51:08 ceph-node-4 mon[2237]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:17 ceph-node-4 client[2101]: INFO: OSD disk space utilization: 70%
2025-02-24 10:50:33 ceph-node-1 mgr[8469]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:42 ceph-node-2 client[2873]: ERROR: Journal write failure detected on OSD node
2025-02-24 10:51:01 ceph-node-1 client[7242]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:59 ceph-node-1 osd[6711]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:04 ceph-node-5 osd[4357]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:23 ceph-node-2 mds[9090]: ERROR: Monitor down, unable to connect to quorum
2025-02-24 10:51:07 ceph-node-4 mon[6498]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:43 ceph-node-1 osd[8820]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:50:33 ceph-node-3 mds[1993]: INFO: Monitor map has been updated
2025-02-24 10:51:20 ceph-node-2 mon[7383]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:12 ceph-node-2 mon[7647]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:50:44 ceph-node-5 radosgw[4492]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:04 ceph-node-3 client[9208]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:29 ceph-node-4 osd[4739]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:51:23 ceph-node-5 radosgw[8999]: ERROR: Network partition detected between OSD nodes
2025-02-24 10:50:53 ceph-node-2 mgr[3377]: CRITICAL: Slow request detected on OSD 4.3
2025-02-24 10:50:55 ceph-node-4 mon[7773]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:51:20 ceph-node-5 mgr[9311]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:51:22 ceph-node-5 osd[4510]: NOTICE: MGR module loaded successfully
2025-02-24 10:51:19 ceph-node-5 client[4576]: CRITICAL: Monitor down, unable to connect to quorum
2025-02-24 10:50:40 ceph-node-3 mon[7204]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:50:53 ceph-node-5 client[3669]: NOTICE: MGR module loaded successfully
2025-02-24 10:51:12 ceph-node-3 mds[6870]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:07 ceph-node-4 radosgw[2405]: WARNING: Authentication failure in RADOS Gateway
2025-02-24 10:51:02 ceph-node-2 radosgw[6896]: NOTICE: OSD rebalancing completed
2025-02-24 10:50:32 ceph-node-2 osd[6991]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:30 ceph-node-5 client[2277]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:03 ceph-node-2 radosgw[8256]: NOTICE: Data replication completed for object pool
2025-02-24 10:50:55 ceph-node-1 mon[2225]: WARNING: Network partition detected between OSD nodes
2025-02-24 10:50:43 ceph-node-3 mds[8348]: NOTICE: OSD rebalancing completed
2025-02-24 10:51:02 ceph-node-2 mon[2973]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:51:06 ceph-node-2 osd[5259]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:54 ceph-node-4 client[5126]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:04 ceph-node-4 mon[3727]: CRITICAL: Client connection timeout detected
2025-02-24 10:50:56 ceph-node-3 osd[4369]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:07 ceph-node-3 mgr[5327]: NOTICE: Monitor map has been updated
2025-02-24 10:51:18 ceph-node-3 mon[1480]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:00 ceph-node-4 radosgw[1654]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:51 ceph-node-3 osd[6290]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:50:33 ceph-node-5 client[5179]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:51:03 ceph-node-2 mgr[9356]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:16 ceph-node-4 mon[4060]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:03 ceph-node-2 mon[2814]: INFO: OSD disk space utilization: 70%
2025-02-24 10:51:13 ceph-node-1 client[3396]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:40 ceph-node-1 radosgw[3538]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:50:49 ceph-node-3 mon[3436]: CRITICAL: OSD 3.1 marked down due to heartbeat failure
2025-02-24 10:50:52 ceph-node-1 mds[9916]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:50:39 ceph-node-5 osd[6965]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:50:57 ceph-node-2 mgr[2566]: INFO: Data replication completed for object pool
2025-02-24 10:50:33 ceph-node-1 osd[6084]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:24 ceph-node-4 mgr[4634]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:50:41 ceph-node-2 mon[1694]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:50:34 ceph-node-3 radosgw[6091]: INFO: PG 1.2 replicated successfully
2025-02-24 10:51:07 ceph-node-1 mgr[3796]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:52 ceph-node-3 osd[6469]: WARNING: Disk failure reported on ceph-node-4
2025-02-24 10:51:05 ceph-node-4 mon[4003]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:50:35 ceph-node-5 mds[1849]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:29 ceph-node-1 radosgw[9118]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:43 ceph-node-2 osd[4162]: INFO: PG 1.2 replicated successfully
2025-02-24 10:51:13 ceph-node-4 mds[5402]: DEBUG: Monitor map has been updated
2025-02-24 10:50:41 ceph-node-5 radosgw[3102]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:50:47 ceph-node-3 client[6981]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:00 ceph-node-4 osd[7760]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:51:06 ceph-node-3 radosgw[8504]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:16 ceph-node-3 osd[2511]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:45 ceph-node-5 mgr[4595]: INFO: OSD rebalancing completed
2025-02-24 10:50:34 ceph-node-4 mds[3943]: DEBUG: Monitor map has been updated
2025-02-24 10:51:11 ceph-node-2 mds[2558]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:50:36 ceph-node-2 radosgw[6186]: DEBUG: Monitor map has been updated
2025-02-24 10:51:03 ceph-node-2 radosgw[8908]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:51:25 ceph-node-1 radosgw[6666]: INFO: MGR module loaded successfully
2025-02-24 10:50:50 ceph-node-4 mds[7003]: ERROR: Client connection timeout detected
2025-02-24 10:50:58 ceph-node-2 mgr[2173]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:50:40 ceph-node-4 mgr[4357]: INFO: MGR module loaded successfully
2025-02-24 10:50:38 ceph-node-2 radosgw[3945]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:22 ceph-node-3 mgr[5293]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:51:01 ceph-node-4 radosgw[1302]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:51:28 ceph-node-4 client[8926]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:51:18 ceph-node-3 mds[2054]: DEBUG: Monitor map has been updated
2025-02-24 10:51:30 ceph-node-1 radosgw[7949]: ERROR: Cluster status: HEALTH_WARN
2025-02-24 10:50:53 ceph-node-4 client[5091]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:05 ceph-node-2 mgr[1278]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:11 ceph-node-1 osd[2638]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:50:56 ceph-node-5 mon[4741]: NOTICE: Data replication completed for object pool
2025-02-24 10:51:01 ceph-node-4 mgr[1705]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:17 ceph-node-2 client[3461]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:50:35 ceph-node-2 mon[9643]: INFO: OSD disk space utilization: 70%
2025-02-24 10:51:00 ceph-node-5 mds[6548]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:51:03 ceph-node-4 osd[3175]: WARNING: Uncorrectable ECC error detected in MDS cache
2025-02-24 10:51:22 ceph-node-3 client[1524]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:58 ceph-node-1 mds[6935]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:47 ceph-node-2 mon[3118]: NOTICE: Monitor map has been updated
2025-02-24 10:50:55 ceph-node-4 mds[6814]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:50:37 ceph-node-5 radosgw[7320]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:51:24 ceph-node-4 client[7335]: NOTICE: OSD rebalancing completed
2025-02-24 10:50:47 ceph-node-5 mgr[9882]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:57 ceph-node-1 client[9225]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:31 ceph-node-1 mds[1817]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:11 ceph-node-5 mon[5063]: WARNING: CRITICAL: Data loss detected on pool 1
2025-02-24 10:51:10 ceph-node-3 client[3628]: INFO: Data replication completed for object pool
2025-02-24 10:50:42 ceph-node-3 mgr[1674]: NOTICE: Data replication completed for object pool
2025-02-24 10:50:54 ceph-node-2 mon[6964]: INFO: OSD rebalancing completed
2025-02-24 10:50:59 ceph-node-4 client[5919]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:14 ceph-node-4 mgr[4622]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:50:34 ceph-node-1 radosgw[1775]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:19 ceph-node-2 client[6889]: WARNING: Monitor down, unable to connect to quorum
2025-02-24 10:51:00 ceph-node-2 mon[6594]: NOTICE: OSD rebalancing completed
2025-02-24 10:51:12 ceph-node-4 mgr[8560]: DEBUG: MGR module loaded successfully
2025-02-24 10:50:38 ceph-node-2 mds[7140]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:51:05 ceph-node-4 osd[2129]: INFO: OSD rebalancing completed
2025-02-24 10:51:06 ceph-node-1 osd[5295]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:50:53 ceph-node-1 radosgw[7750]: ERROR: Monitor down, unable to connect to quorum
2025-02-24 10:51:01 ceph-node-5 mds[5094]: INFO: PG 1.2 replicated successfully
2025-02-24 10:50:35 ceph-node-1 osd[3300]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:55 ceph-node-5 osd[4915]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:51:22 ceph-node-3 client[3002]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:50:59 ceph-node-3 client[6227]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:07 ceph-node-1 mgr[8455]: WARNING: CRITICAL: PG 2.1 stuck in inconsistent state
2025-02-24 10:50:35 ceph-node-4 mgr[7637]: INFO: Data replication completed for object pool
2025-02-24 10:51:02 ceph-node-2 radosgw[4660]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:51:23 ceph-node-3 osd[4465]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:15 ceph-node-3 mgr[2901]: NOTICE: Data replication completed for object pool
2025-02-24 10:50:37 ceph-node-5 osd[1188]: INFO: Monitor map has been updated
2025-02-24 10:50:33 ceph-node-1 osd[7589]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:20 ceph-node-1 osd[1871]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:50:59 ceph-node-5 mon[2044]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:50 ceph-node-5 client[7464]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:21 ceph-node-3 mon[9842]: NOTICE: MGR module loaded successfully
2025-02-24 10:51:21 ceph-node-3 radosgw[8844]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:30 ceph-node-2 mds[1847]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:51:14 ceph-node-1 osd[3026]: NOTICE: OSD rebalancing completed
2025-02-24 10:51:14 ceph-node-4 mds[1343]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:09 ceph-node-2 osd[5568]: NOTICE: OSD rebalancing completed
2025-02-24 10:50:33 ceph-node-5 radosgw[8195]: ERROR: Monitor mon1 failed to reach quorum
2025-02-24 10:51:19 ceph-node-3 mds[3372]: NOTICE: Monitor map has been updated
2025-02-24 10:50:52 ceph-node-5 radosgw[4417]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:31 ceph-node-3 mon[9056]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:37 ceph-node-5 osd[2974]: WARNING: Journal write failure detected on OSD node
2025-02-24 10:51:31 ceph-node-3 mon[7721]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:51 ceph-node-4 osd[8818]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:49 ceph-node-5 radosgw[6943]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:22 ceph-node-2 client[2046]: WARNING: High I/O latency detected in radosgw service
2025-02-24 10:50:48 ceph-node-4 mon[5801]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:10 ceph-node-1 mds[6625]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:52 ceph-node-4 mgr[3404]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:51:17 ceph-node-2 client[8830]: DEBUG: MGR module loaded successfully
2025-02-24 10:50:37 ceph-node-3 mds[5040]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:51:28 ceph-node-4 mgr[3760]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:14 ceph-node-3 mds[8267]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:11 ceph-node-1 mds[9521]: INFO: Monitor map has been updated
2025-02-24 10:51:08 ceph-node-5 client[1413]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:50:31 ceph-node-5 mon[9365]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:51:29 ceph-node-3 mon[3259]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:25 ceph-node-1 osd[5064]: CRITICAL: OSD 3.1 marked down due to heartbeat failure
2025-02-24 10:51:02 ceph-node-2 client[9480]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:51:26 ceph-node-2 radosgw[2237]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:39 ceph-node-3 mon[2839]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:50:40 ceph-node-4 mds[6996]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:33 ceph-node-3 mgr[3361]: INFO: Data replication completed for object pool
2025-02-24 10:51:17 ceph-node-5 client[5404]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:14 ceph-node-1 radosgw[8859]: INFO: Data replication completed for object pool
2025-02-24 10:51:10 ceph-node-4 mon[5736]: DEBUG: MGR module loaded successfully
2025-02-24 10:50:54 ceph-node-4 mgr[8134]: DEBUG: Monitor map has been updated
2025-02-24 10:51:19 ceph-node-3 osd[6683]: INFO: MGR module loaded successfully
2025-02-24 10:51:13 ceph-node-3 client[7095]: DEBUG: Data replication completed for object pool
2025-02-24 10:50:47 ceph-node-1 mgr[7847]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:35 ceph-node-4 mgr[9106]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:29 ceph-node-5 osd[5529]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:51:21 ceph-node-5 mon[1051]: ERROR: Cluster status: HEALTH_WARN
2025-02-24 10:50:42 ceph-node-4 osd[1854]: CRITICAL: Data corruption detected in object pool
2025-02-24 10:50:43 ceph-node-2 mon[2095]: WARNING: Cluster status: HEALTH_WARN
2025-02-24 10:50:45 ceph-node-1 osd[3962]: NOTICE: Monitor map has been updated
2025-02-24 10:51:20 ceph-node-2 osd[2736]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:42 ceph-node-3 radosgw[1718]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:50:32 ceph-node-3 mon[2552]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:02 ceph-node-4 osd[4247]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:50:56 ceph-node-3 radosgw[7880]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:53 ceph-node-5 radosgw[1057]: DEBUG: Data replication completed for object pool
2025-02-24 10:50:50 ceph-node-4 client[9220]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:25 ceph-node-4 mgr[3171]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:18 ceph-node-2 mon[9327]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:50:46 ceph-node-3 mon[4692]: INFO: PG 1.2 replicated successfully
2025-02-24 10:50:52 ceph-node-2 radosgw[6263]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:10 ceph-node-2 client[7660]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:50:46 ceph-node-5 osd[7445]: NOTICE: Monitor map has been updated
2025-02-24 10:51:12 ceph-node-2 mgr[6546]: DEBUG: OSD rebalancing completed
2025-02-24 10:51:20 ceph-node-4 mgr[3457]: INFO: OSD disk space utilization: 70%
2025-02-24 10:50:57 ceph-node-2 mon[2416]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:10 ceph-node-5 mon[3379]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:50:50 ceph-node-2 mgr[9409]: ERROR: CRITICAL: PG 2.1 stuck in inconsistent state
2025-02-24 10:50:38 ceph-node-2 osd[3633]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:51:02 ceph-node-5 mgr[2993]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:32 ceph-node-1 radosgw[2981]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:51:28 ceph-node-2 mgr[2345]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:45 ceph-node-1 mgr[9916]: WARNING: OSD 3.1 marked down due to heartbeat failure
2025-02-24 10:50:56 ceph-node-5 mon[7896]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:50:47 ceph-node-1 mds[3049]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:50:44 ceph-node-3 mgr[2763]: INFO: OSD rebalancing completed
2025-02-24 10:50:58 ceph-node-2 osd[5598]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:50:51 ceph-node-5 client[7628]: NOTICE: MGR module loaded successfully
2025-02-24 10:51:30 ceph-node-2 mds[8844]: INFO: MGR module loaded successfully
2025-02-24 10:50:51 ceph-node-5 mds[2711]: NOTICE: OSD rebalancing completed
2025-02-24 10:51:16 ceph-node-5 mon[2099]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:49 ceph-node-3 mds[1212]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:06 ceph-node-4 radosgw[9267]: INFO: PG 1.2 replicated successfully
2025-02-24 10:51:02 ceph-node-3 mon[8760]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:29 ceph-node-5 client[8021]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:50:38 ceph-node-5 mgr[3495]: INFO: MGR module loaded successfully
2025-02-24 10:51:02 ceph-node-1 radosgw[2486]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:48 ceph-node-3 mon[7156]: ERROR: OSD 3.1 marked down due to heartbeat failure
2025-02-24 10:51:16 ceph-node-4 mgr[7956]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:49 ceph-node-5 radosgw[2080]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:51:14 ceph-node-3 client[3059]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:54 ceph-node-2 mgr[7436]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:31 ceph-node-4 mds[5368]: NOTICE: Data replication completed for object pool
2025-02-24 10:51:22 ceph-node-2 client[8756]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:10 ceph-node-5 mgr[3516]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:50:51 ceph-node-2 mon[7110]: CRITICAL: Slow request detected on OSD 4.3
2025-02-24 10:51:22 ceph-node-2 client[8062]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:50:53 ceph-node-3 mgr[3462]: DEBUG: Monitor map has been updated
2025-02-24 10:50:51 ceph-node-1 mon[9114]: NOTICE: MGR module loaded successfully
2025-02-24 10:50:52 ceph-node-3 mgr[6123]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:50:39 ceph-node-1 mgr[1579]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:40 ceph-node-4 client[4473]: NOTICE: MGR module loaded successfully
2025-02-24 10:50:46 ceph-node-3 osd[3815]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:12 ceph-node-5 osd[2532]: NOTICE: Cluster status: HEALTH_OK
2025-02-24 10:51:07 ceph-node-1 osd[8226]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:31 ceph-node-5 mds[6013]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:50:37 ceph-node-4 radosgw[3882]: CRITICAL: Monitor down, unable to connect to quorum
2025-02-24 10:50:46 ceph-node-2 mds[8201]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:51 ceph-node-3 mgr[1718]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:51:14 ceph-node-3 mon[1404]: INFO: OSD disk space utilization: 70%
2025-02-24 10:50:32 ceph-node-3 osd[3039]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:51:00 ceph-node-1 mon[4713]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:50:41 ceph-node-5 client[8968]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:51:24 ceph-node-5 client[4941]: DEBUG: OSD rebalancing completed
2025-02-24 10:50:47 ceph-node-2 mds[4481]: WARNING: Network partition detected between OSD nodes
2025-02-24 10:51:11 ceph-node-5 osd[5831]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:10 ceph-node-1 mgr[7180]: NOTICE: Data replication completed for object pool
2025-02-24 10:51:13 ceph-node-4 mgr[7423]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:51:13 ceph-node-1 mds[8900]: DEBUG: OSD rebalancing completed
2025-02-24 10:50:47 ceph-node-5 mds[6589]: DEBUG: Monitor map has been updated
2025-02-24 10:50:59 ceph-node-5 radosgw[8534]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:51:20 ceph-node-3 radosgw[4145]: INFO: MGR module loaded successfully
2025-02-24 10:50:42 ceph-node-1 mgr[4822]: NOTICE: Monitor map has been updated
2025-02-24 10:51:09 ceph-node-1 mon[2032]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:50:38 ceph-node-3 mon[6016]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:50:59 ceph-node-3 osd[8855]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:51:25 ceph-node-1 client[3924]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:30 ceph-node-2 client[7774]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:59 ceph-node-5 mgr[9447]: INFO: MGR module loaded successfully
2025-02-24 10:51:24 ceph-node-4 osd[6530]: INFO: PG 1.2 replicated successfully
2025-02-24 10:50:32 ceph-node-5 mon[1875]: NOTICE: OSD rebalancing completed
2025-02-24 10:51:22 ceph-node-1 mgr[4371]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:50:34 ceph-node-2 client[7584]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:43 ceph-node-5 mds[9094]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:22 ceph-node-2 client[8956]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:24 ceph-node-5 mds[9050]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:38 ceph-node-5 mgr[7613]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:50:59 ceph-node-1 mon[8696]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:48 ceph-node-2 mds[2215]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:14 ceph-node-5 radosgw[8023]: NOTICE: Monitor map has been updated
2025-02-24 10:51:01 ceph-node-1 mgr[8457]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:44 ceph-node-1 osd[2495]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:46 ceph-node-5 mgr[2510]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:15 ceph-node-3 mds[8452]: NOTICE: Data replication completed for object pool
2025-02-24 10:51:31 ceph-node-3 client[5308]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:51:23 ceph-node-5 radosgw[2231]: CRITICAL: High I/O latency detected in radosgw service
2025-02-24 10:51:22 ceph-node-1 mgr[8788]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:50:34 ceph-node-2 client[7734]: INFO: PG 1.2 replicated successfully
2025-02-24 10:50:48 ceph-node-3 radosgw[7730]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:50:52 ceph-node-4 client[3117]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:50:56 ceph-node-4 osd[3459]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:49 ceph-node-3 mgr[7232]: DEBUG: Data replication completed for object pool
2025-02-24 10:50:37 ceph-node-1 osd[7576]: DEBUG: MGR module loaded successfully
2025-02-24 10:50:59 ceph-node-2 radosgw[2700]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:50:43 ceph-node-2 mds[1690]: NOTICE: Data replication completed for object pool
2025-02-24 10:51:18 ceph-node-5 mon[9519]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:50:37 ceph-node-3 radosgw[9813]: ERROR: Uncorrectable ECC error detected in MDS cache
2025-02-24 10:50:54 ceph-node-5 radosgw[7265]: INFO: Data replication completed for object pool
2025-02-24 10:51:23 ceph-node-3 osd[5204]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:15 ceph-node-5 osd[1310]: INFO: OSD disk space utilization: 70%
2025-02-24 10:51:12 ceph-node-5 mon[8685]: INFO: Monitor map has been updated
2025-02-24 10:50:44 ceph-node-2 client[2996]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:51:06 ceph-node-3 client[5595]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:50:54 ceph-node-3 radosgw[9594]: INFO: MGR module loaded successfully
2025-02-24 10:50:44 ceph-node-4 osd[2871]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:51:27 ceph-node-1 client[6426]: ERROR: Cluster status: HEALTH_WARN
2025-02-24 10:50:49 ceph-node-3 mgr[7619]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:50:54 ceph-node-5 client[3373]: DEBUG: Data replication completed for object pool
2025-02-24 10:51:14 ceph-node-3 mds[2738]: DEBUG: OSD rebalancing completed
2025-02-24 10:50:49 ceph-node-3 client[1381]: NOTICE: Monitor map has been updated
2025-02-24 10:51:10 ceph-node-3 osd[2418]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:31 ceph-node-4 mds[3790]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:10 ceph-node-5 osd[7259]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:43 ceph-node-1 mgr[8130]: NOTICE: Data replication completed for object pool
2025-02-24 10:50:42 ceph-node-3 radosgw[9474]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:50:49 ceph-node-1 mgr[5768]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:31 ceph-node-1 mds[1280]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:50:56 ceph-node-3 mon[6872]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:50:56 ceph-node-2 mgr[2535]: DEBUG: Monitor map has been updated
2025-02-24 10:50:56 ceph-node-3 mon[8513]: INFO: MGR module loaded successfully
2025-02-24 10:51:07 ceph-node-5 osd[9531]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:13 ceph-node-5 client[3753]: ERROR: OSD 3.1 marked down due to heartbeat failure
2025-02-24 10:50:57 ceph-node-5 radosgw[5277]: INFO: OSD disk space utilization: 70%
2025-02-24 10:50:49 ceph-node-3 client[9305]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:50:59 ceph-node-5 osd[2467]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:51:17 ceph-node-3 client[8031]: DEBUG: Monitor map has been updated
2025-02-24 10:50:46 ceph-node-2 mon[6728]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:05 ceph-node-5 mgr[9939]: INFO: Monitor map has been updated
2025-02-24 10:51:11 ceph-node-5 radosgw[3806]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:50:44 ceph-node-1 osd[2349]: NOTICE: Data replication completed for object pool
2025-02-24 10:50:37 ceph-node-2 mon[5312]: WARNING: Journal write failure detected on OSD node
2025-02-24 10:51:25 ceph-node-5 radosgw[3846]: INFO: Monitor map has been updated
2025-02-24 10:50:59 ceph-node-3 mgr[2455]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:51:21 ceph-node-2 radosgw[1710]: INFO: Monitor map has been updated
2025-02-24 10:50:43 ceph-node-1 mds[5644]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:56 ceph-node-3 mon[8490]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:50:39 ceph-node-5 mgr[7911]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:50:37 ceph-node-5 osd[9019]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:51:03 ceph-node-1 mds[8711]: INFO: PG 1.2 replicated successfully
2025-02-24 10:51:14 ceph-node-1 osd[8887]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:53 ceph-node-1 mds[1577]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:05 ceph-node-5 mgr[2299]: ERROR: Monitor mon1 failed to reach quorum
2025-02-24 10:50:46 ceph-node-4 mgr[6973]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:51:18 ceph-node-2 mgr[3445]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:35 ceph-node-5 mds[4200]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:50:57 ceph-node-4 client[2893]: DEBUG: OSD rebalancing completed
2025-02-24 10:50:55 ceph-node-3 osd[5027]: NOTICE: Data replication completed for object pool
2025-02-24 10:50:36 ceph-node-3 mds[9896]: INFO: OSD rebalancing completed
2025-02-24 10:51:10 ceph-node-3 mon[3307]: WARNING: Monitor down, unable to connect to quorum
2025-02-24 10:51:19 ceph-node-2 mgr[9343]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:47 ceph-node-5 mon[5532]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:14 ceph-node-5 mgr[2018]: DEBUG: Monitor map has been updated
2025-02-24 10:51:03 ceph-node-3 radosgw[4548]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:50:51 ceph-node-4 mds[7224]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:50:59 ceph-node-2 osd[8471]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:51:26 ceph-node-1 radosgw[7271]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:50:52 ceph-node-5 mon[8178]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:50:41 ceph-node-3 osd[2326]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:27 ceph-node-2 osd[4825]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:26 ceph-node-4 osd[2339]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:08 ceph-node-1 client[7935]: INFO: Scrubbing completed for PG 1.5
2025-02-24 10:51:14 ceph-node-3 mgr[3452]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:21 ceph-node-4 mgr[4724]: NOTICE: Monitor map has been updated
2025-02-24 10:51:28 ceph-node-2 radosgw[7414]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:51:14 ceph-node-4 client[2522]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:50:35 ceph-node-3 mds[4324]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:51:09 ceph-node-5 radosgw[3611]: ERROR: MDS failure detected: metadata inconsistency
2025-02-24 10:50:49 ceph-node-3 radosgw[8708]: NOTICE: MGR module loaded successfully
2025-02-24 10:51:24 ceph-node-5 mon[8326]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:50:58 ceph-node-5 mgr[5084]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:51:08 ceph-node-3 client[6613]: NOTICE: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:53 ceph-node-2 mon[3979]: WARNING: OSD 3.1 marked down due to heartbeat failure
2025-02-24 10:51:26 ceph-node-5 mon[8783]: DEBUG: Monitor map has been updated
2025-02-24 10:51:06 ceph-node-1 mon[4947]: CRITICAL: Client connection timeout detected
2025-02-24 10:50:58 ceph-node-5 mon[8426]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:51 ceph-node-5 mgr[9967]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:03 ceph-node-2 mon[6144]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:51:01 ceph-node-4 mon[2814]: INFO: OSD disk space utilization: 70%
2025-02-24 10:51:19 ceph-node-2 mon[7870]: INFO: PG 1.2 replicated successfully
2025-02-24 10:50:31 ceph-node-4 mon[5464]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:51:08 ceph-node-3 mgr[9998]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:51:11 ceph-node-2 osd[6864]: NOTICE: OSD rebalancing completed
2025-02-24 10:50:31 ceph-node-4 client[4540]: DEBUG: Backfilling completed for PG 1.9
2025-02-24 10:51:09 ceph-node-4 osd[7143]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:51:07 ceph-node-3 mon[2240]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:50:39 ceph-node-5 client[4146]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:51:31 ceph-node-3 mgr[8818]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:50:53 ceph-node-1 radosgw[6348]: ERROR: CRITICAL: PG 2.1 stuck in inconsistent state
2025-02-24 10:51:17 ceph-node-4 mds[4108]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:51:03 ceph-node-2 osd[3010]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:50:48 ceph-node-1 radosgw[6278]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:03 ceph-node-4 mds[9991]: NOTICE: OSD rebalancing completed
2025-02-24 10:50:49 ceph-node-5 osd[5894]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:41 ceph-node-5 client[7050]: DEBUG: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:03 ceph-node-2 client[2475]: INFO: OSD disk space utilization: 70%
2025-02-24 10:51:05 ceph-node-5 osd[2437]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:51:07 ceph-node-3 mon[6089]: INFO: MGR module loaded successfully
2025-02-24 10:51:06 ceph-node-5 mds[2319]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:40 ceph-node-1 mon[5241]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:01 ceph-node-5 mon[8775]: NOTICE: MGR module loaded successfully
2025-02-24 10:51:07 ceph-node-2 mds[2335]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:57 ceph-node-2 osd[2719]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:50:37 ceph-node-2 mds[2656]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:50:54 ceph-node-5 mon[4144]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:51:21 ceph-node-5 mds[4730]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:14 ceph-node-2 client[2739]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:01 ceph-node-5 client[3783]: DEBUG: OSD rebalancing completed
2025-02-24 10:51:07 ceph-node-4 radosgw[7974]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:51:28 ceph-node-1 osd[7813]: DEBUG: Cluster status: HEALTH_OK
2025-02-24 10:50:46 ceph-node-4 client[2614]: CRITICAL: Data corruption detected in object pool
2025-02-24 10:51:28 ceph-node-2 mon[5352]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:39 ceph-node-2 client[4328]: WARNING: Monitor down, unable to connect to quorum
2025-02-24 10:50:50 ceph-node-1 client[8795]: INFO: OSD rebalancing completed
2025-02-24 10:51:21 ceph-node-5 mgr[3784]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:09 ceph-node-5 client[8688]: WARNING: Data corruption detected in object pool
2025-02-24 10:50:50 ceph-node-1 mds[1750]: NOTICE: Scrubbing completed for PG 1.5
2025-02-24 10:50:39 ceph-node-5 mds[4617]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:07 ceph-node-4 mds[9495]: INFO: Monitor map has been updated
2025-02-24 10:51:22 ceph-node-3 mgr[6429]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:51:12 ceph-node-5 mon[6130]: WARNING: Cluster status: HEALTH_WARN
2025-02-24 10:51:20 ceph-node-2 mds[2079]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:25 ceph-node-5 mon[8426]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:50:47 ceph-node-3 mgr[6848]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:51:16 ceph-node-2 mds[6942]: DEBUG: OSD rebalancing completed
2025-02-24 10:51:14 ceph-node-2 client[7811]: DEBUG: File write operation completed by client 10.0.0.2
2025-02-24 10:50:39 ceph-node-3 client[8926]: INFO: PG 1.2 replicated successfully
2025-02-24 10:50:36 ceph-node-1 mds[7596]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:50:48 ceph-node-1 mgr[1391]: DEBUG: Monitor map has been updated
2025-02-24 10:51:25 ceph-node-3 mgr[2314]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:50:53 ceph-node-2 osd[5737]: NOTICE: OSD rebalancing completed
2025-02-24 10:50:34 ceph-node-3 osd[4117]: NOTICE: OSD rebalancing completed
2025-02-24 10:50:41 ceph-node-1 client[1167]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:50:38 ceph-node-4 osd[7777]: INFO: OSD rebalancing completed
2025-02-24 10:51:04 ceph-node-4 mon[6429]: ERROR: Disk failure reported on ceph-node-4
2025-02-24 10:51:16 ceph-node-2 mgr[1835]: NOTICE: Data replication completed for object pool
2025-02-24 10:51:20 ceph-node-2 radosgw[8718]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:51:04 ceph-node-1 mon[3739]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:50:51 ceph-node-1 mds[6435]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:50:46 ceph-node-1 osd[3784]: INFO: OSD disk space utilization: 70%
2025-02-24 10:51:22 ceph-node-1 mgr[4347]: CRITICAL: CRITICAL: Data loss detected on pool 1
2025-02-24 10:51:08 ceph-node-4 mon[9453]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:50:48 ceph-node-2 osd[8070]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:55 ceph-node-4 client[2647]: INFO: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:30 ceph-node-2 mgr[1548]: INFO: Monitor map has been updated
2025-02-24 10:51:00 ceph-node-4 mon[1053]: INFO: OSD rebalancing completed
2025-02-24 10:50:44 ceph-node-5 radosgw[7925]: INFO: PG 1.2 replicated successfully
2025-02-24 10:51:30 ceph-node-4 radosgw[1899]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:21 ceph-node-4 mgr[7547]: CRITICAL: Network partition detected between OSD nodes
2025-02-24 10:51:28 ceph-node-3 osd[2077]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:50:59 ceph-node-2 mgr[4440]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:15 ceph-node-1 radosgw[3948]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:51:03 ceph-node-3 mon[2788]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:57 ceph-node-4 mon[6457]: INFO: Data replication completed for object pool
2025-02-24 10:50:56 ceph-node-4 radosgw[1172]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:51:29 ceph-node-4 mgr[1525]: INFO: OSD disk space utilization: 70%
2025-02-24 10:51:16 ceph-node-2 mds[3957]: INFO: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:02 ceph-node-5 osd[5982]: DEBUG: RADOS Gateway bucket created successfully
2025-02-24 10:51:26 ceph-node-1 client[2366]: ERROR: Slow request detected on OSD 4.3
2025-02-24 10:50:31 ceph-node-5 radosgw[5871]: NOTICE: Backfilling completed for PG 1.9
2025-02-24 10:50:42 ceph-node-4 osd[2049]: CRITICAL: CRITICAL: PG 2.1 stuck in inconsistent state
2025-02-24 10:50:51 ceph-node-2 mds[4488]: DEBUG: Monitor map has been updated
2025-02-24 10:50:58 ceph-node-4 mgr[2340]: DEBUG: PG 1.2 replicated successfully
2025-02-24 10:50:35 ceph-node-1 mgr[4952]: NOTICE: OSD rebalancing completed
2025-02-24 10:51:19 ceph-node-5 radosgw[8389]: CRITICAL: OSD disk space utilization exceeded 90%
2025-02-24 10:50:50 ceph-node-1 mon[4471]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:50:40 ceph-node-4 mon[6993]: DEBUG: MGR module loaded successfully
2025-02-24 10:51:16 ceph-node-4 mgr[9496]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:51:30 ceph-node-5 osd[3139]: CRITICAL: CRITICAL: Data loss detected on pool 1
2025-02-24 10:51:30 ceph-node-5 mgr[4816]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:51:28 ceph-node-3 mon[5216]: DEBUG: OSD 1.2 heartbeat sent to OSD 1.1
2025-02-24 10:50:55 ceph-node-3 mds[1339]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:51:13 ceph-node-1 mon[6619]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:10 ceph-node-2 client[2920]: WARNING: High I/O latency detected in radosgw service
2025-02-24 10:51:29 ceph-node-3 osd[6072]: INFO: RADOS Gateway bucket created successfully
2025-02-24 10:50:57 ceph-node-5 mgr[8523]: INFO: File write operation completed by client 10.0.0.2
2025-02-24 10:51:26 ceph-node-4 osd[1314]: DEBUG: Metadata operation completed successfully on MDS
2025-02-24 10:51:19 ceph-node-5 mgr[7428]: DEBUG: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:39 ceph-node-3 osd[7354]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:18 ceph-node-4 radosgw[3741]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:50:59 ceph-node-4 osd[6749]: INFO: PG 1.2 replicated successfully
2025-02-24 10:50:50 ceph-node-5 mgr[6559]: NOTICE: Client 10.0.0.1 connected to monitor
2025-02-24 10:50:51 ceph-node-4 client[7950]: INFO: Client 10.0.0.1 connected to monitor
2025-02-24 10:51:16 ceph-node-4 mon[7460]: NOTICE: PG 1.2 replicated successfully
2025-02-24 10:50:53 ceph-node-1 mon[6081]: DEBUG: OSD disk space utilization: 70%
2025-02-24 10:50:35 ceph-node-2 client[4015]: NOTICE: RADOS Gateway bucket created successfully
2025-02-24 10:50:59 ceph-node-2 radosgw[6689]: NOTICE: File write operation completed by client 10.0.0.2
2025-02-24 10:50:31 ceph-node-2 mgr[7044]: NOTICE: Monitor quorum established: mon1, mon2, mon3
2025-02-24 10:51:11 ceph-node-1 radosgw[5072]: NOTICE: OSD disk space utilization: 70%
2025-02-24 10:50:34 ceph-node-2 mgr[8728]: INFO: Cluster status: HEALTH_OK
2025-02-24 10:51:24 ceph-node-4 radosgw[8222]: NOTICE: Metadata operation completed successfully on MDS
2025-02-24 10:50:36 ceph-node-1 mon[9691]: DEBUG: Scrubbing completed for PG 1.5
2025-02-24 10:51:06 ceph-node-1 mgr[5171]: INFO: Monitor map has been updated
2025-02-24 10:50:50 ceph-node-2 radosgw[5616]: INFO: Metadata operation completed successfully on MDS
2025-02-24 10:50:55 ceph-node-1 radosgw[8101]: INFO: PG 1.2 replicated successfully
2025-02-24 10:50:46 ceph-node-1 mon[4017]: INFO: OSD disk space utilization: 70%
2025-02-24 10:50:59 ceph-node-1 radosgw[1855]: ERROR: Network partition detected between OSD nodes
2025-02-24 10:50:47 ceph-node-5 radosgw[4964]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:50:55 ceph-node-5 osd[2951]: INFO: PG 1.2 replicated successfully
2025-02-24 10:51:08 ceph-node-2 client[4970]: INFO: Backfilling completed for PG 1.9
2025-02-24 10:51:05 ceph-node-2 mgr[4145]: DEBUG: Backfilling completed for PG 1.9
